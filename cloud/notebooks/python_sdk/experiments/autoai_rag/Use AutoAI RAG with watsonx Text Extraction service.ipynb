{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2373ada319e4fda5",
            "metadata": {},
            "source": [
                "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
                "# Use AutoAI RAG with watsonx Text Extraction service"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24aa5fa58bb5d946",
            "metadata": {},
            "source": [
                "#### Disclaimers\n",
                "\n",
                "- Use only Projects and Spaces that are available in the watsonx context.\n",
                "\n",
                "\n",
                "## Notebook content\n",
                "\n",
                "This notebook demonstrates how to process data using the IBM watsonx.ai Text Extraction service and use the result in an AutoAI RAG experiment.\n",
                "The data used in this notebook is from the [Granite Code Models paper](https://arxiv.org/pdf/2405.04324)\n",
                "\n",
                "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
                "\n",
                "\n",
                "## Learning goal\n",
                "\n",
                "The learning goals of this notebook are:\n",
                "\n",
                "- Process data using the IBM watsonx.ai Text Extraction service\n",
                "- Create an AutoAI RAG job that will find the best RAG pattern based on processed data\n",
                "\n",
                "\n",
                "## Contents\n",
                "\n",
                "This notebook contains the following parts:\n",
                "- [Set up the environment](#setup)\n",
                "- [Prepare data and connections for the Text Extraction service](#prepare-te)\n",
                "- [Process data using the Text Extraction service](#run-te)\n",
                "- [Prepare data and connections for the AutoAI RAG experiment](#prepare-autorag)\n",
                "- [Run the AutoAI RAG experiment](#run-autorag)\n",
                "- [Compare and test RAG Patterns](#comparision)\n",
                "- [Summary](#summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "797be889df452a0c",
            "metadata": {},
            "source": [
                "<a id=\"setup\"></a>\n",
                "# Set up the environment\n",
                "\n",
                "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
                "\n",
                "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">watsonx.ai Runtime Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77e9a79e53ca99e9",
            "metadata": {},
            "source": [
                "### Install and import the required modules and dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd46247ca5490688",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -U 'ibm-watsonx-ai[rag]>=1.3.26' | tail -n 1\n",
                "!pip install -U \"langchain_community>=0.3,<0.4\" | tail -n 1"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5a79fd5b0a8d2e6",
            "metadata": {},
            "source": [
                "### Define the watsonx.ai credentials\n",
                "This cell defines the credentials required to work with the watsonx.ai service.\n",
                "\n",
                "**Action:** Provide your IBM Cloud API key and the platform URL. For more information, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">Managing user API keys</a>."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "871e4d4831dfff81",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:17:03.432461Z",
                    "start_time": "2025-01-09T15:17:03.430046Z"
                }
            },
            "outputs": [],
            "source": [
                "import getpass\n",
                "\n",
                "from ibm_watsonx_ai import Credentials\n",
                "\n",
                "credentials = Credentials(\n",
                "    url=\"https://yp-qa.ml.cloud.ibm.com\",\n",
                "    api_key=getpass.getpass(\"Please enter your watsonx.ai api key (hit enter): \")\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a7d9204ae2b4503d",
            "metadata": {},
            "source": [
                "### Work with spaces\n",
                "\n",
                "You need to create a space that will be used for your work. If you do not have a space, you can use [Deployment Spaces Dashboard](https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=wx) to create one.\n",
                "\n",
                "- Click **New Deployment Space**\n",
                "- Create an empty space\n",
                "- Select Cloud Object Storage\n",
                "- Select watsonx.ai Runtime instance and press **Create**\n",
                "- Go to **Manage** tab\n",
                "- Copy `Space GUID` and paste it below\n",
                "\n",
                "**Tip**: You can also use SDK to prepare the space for your work. For more information, see [the Space management notebook](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n",
                "\n",
                "**Action**: Assign space ID below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "edfc7a406063d875",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:17:23.482166Z",
                    "start_time": "2025-01-09T15:17:23.480080Z"
                }
            },
            "outputs": [],
            "source": [
                "SPACE_ID = 'PUT YOUR SPACE ID HERE'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0c9b2035ae5e023",
            "metadata": {},
            "source": [
                "### Create an instance of APIClient with authentication details"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "6191de9d36b0ea37",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:17:34.654925Z",
                    "start_time": "2025-01-09T15:17:32.854580Z"
                }
            },
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai import APIClient\n",
                "\n",
                "client = APIClient(credentials=credentials, space_id=SPACE_ID)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1a538feb8038c56b",
            "metadata": {},
            "source": [
                "### Create an instance of COS client"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b292e1bcfbd5849a",
            "metadata": {},
            "source": [
                "Connect to the default COS instance for the provided space by using the `ibm_boto3` package."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "1df2d93b579cdd34",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:17:50.947335Z",
                    "start_time": "2025-01-09T15:17:49.986337Z"
                },
                "id": "3f8b9e40-985e-44cf-a1d6-509493ea85bc"
            },
            "outputs": [],
            "source": [
                "import ibm_boto3\n",
                "\n",
                "cos_credentials = client.spaces.get_details(space_id=SPACE_ID)['entity']['storage']['properties']\n",
                "\n",
                "cos_client = ibm_boto3.client(\n",
                "    service_name=\"s3\",\n",
                "    endpoint_url=cos_credentials[\"endpoint_url\"],\n",
                "    aws_access_key_id=cos_credentials[\"credentials\"][\"editor\"][\"access_key_id\"],\n",
                "    aws_secret_access_key=cos_credentials[\"credentials\"][\"editor\"][\"secret_access_key\"],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb7353b94e404e69",
            "metadata": {},
            "source": [
                "Create a new bucket."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "705245bc2f2785d6",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:17:53.625465Z",
                    "start_time": "2025-01-09T15:17:52.147218Z"
                },
                "id": "0b34af2a-04ab-497f-ab91-ee764a0df804"
            },
            "outputs": [],
            "source": [
                "cos_bucket_name = \"autoai-rag-with-extraction-experiment\"\n",
                "\n",
                "buckets_names = [bucket[\"Name\"] for bucket in cos_client.list_buckets()[\"Buckets\"]]\n",
                "if not cos_bucket_name in buckets_names:\n",
                "    cos_client.create_bucket(Bucket=cos_bucket_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "afa211d65c3f216",
            "metadata": {},
            "source": [
                "Initialize the client connection to the created bucket and get the connection ID."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "17b0d8d77c8a04f8",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:01.575234Z",
                    "start_time": "2025-01-09T15:17:53.631336Z"
                },
                "id": "2d2233f0-4717-4aef-b2b0-ed63afdc19c1"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating connections...\n",
                        "SUCCESS\n"
                    ]
                }
            ],
            "source": [
                "connection_details = client.connections.create(\n",
                "    {\n",
                "        \"datasource_type\": client.connections.get_datasource_type_uid_by_name(\n",
                "            \"bluemixcloudobjectstorage\"\n",
                "        ),\n",
                "        \"name\": \"Connection to COS for tests\",\n",
                "        \"properties\": {\n",
                "            \"bucket\": cos_bucket_name,\n",
                "            \"access_key\": cos_credentials[\"credentials\"][\"editor\"][\"access_key_id\"],\n",
                "            \"secret_key\": cos_credentials[\"credentials\"][\"editor\"][\"secret_access_key\"],\n",
                "            \"iam_url\": client.service_instance._href_definitions.get_iam_token_url(),\n",
                "            \"url\": cos_credentials[\"endpoint_url\"],\n",
                "        },\n",
                "    }\n",
                ")\n",
                "\n",
                "cos_connection_id = client.connections.get_id(\n",
                "    connection_details\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff0a7b3c2e49edb0",
            "metadata": {},
            "source": [
                "<a id=\"prepare-te\"></a>\n",
                "## Prepare data and connections for the Text Extraction service\n",
                "\n",
                "The document, from which we are going to extract text, is located in the IBM Cloud Object Storage (COS). In this notebook, we will use the [Granite Code Models paper](https://arxiv.org/pdf/2405.04324) as a source text document. The final results file, which will contain extracted text and necessary metadata, will be placed in the COS. So we will use the `ibm_watsonx_ai.helpers.DataConnection` and the `ibm_watsonx_ai.helpers.S3Location` class to create Python objects that will represent the references to the processed files. Reference to the final results will be used as an input for the AutoAI RAG experiment. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "6986db8afa2092a0",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:01.583626Z",
                    "start_time": "2025-01-09T15:18:01.581803Z"
                },
                "id": "dbfed6f2-ff47-4ddf-a95b-2a997bb878a0"
            },
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.helpers import DataConnection, S3Location\n",
                "\n",
                "data_url = \"https://arxiv.org/pdf/2405.04324\"\n",
                "\n",
                "te_input_filename = \"granite_code_models_paper.pdf\"\n",
                "te_result_filename = \"granite_code_models_paper.md\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f69a91d41da6d94d",
            "metadata": {},
            "source": [
                "Download and upload training data to the COS bucket. Then define a connection to the uploaded file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "4d2e0021b4ccfae3",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:02.313079Z",
                    "start_time": "2025-01-09T15:18:01.595337Z"
                }
            },
            "outputs": [],
            "source": [
                "import wget\n",
                "\n",
                "wget.download(data_url, te_input_filename)\n",
                "cos_client.upload_file(te_input_filename, cos_bucket_name, te_input_filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "61322fcdd5148ca5",
            "metadata": {},
            "source": [
                "Input file connection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "457123d62baee219",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:03.939674Z",
                    "start_time": "2025-01-09T15:18:02.385866Z"
                }
            },
            "outputs": [],
            "source": [
                "input_data_reference = DataConnection(\n",
                "    connection_asset_id=cos_connection_id,\n",
                "    location=S3Location(bucket=cos_bucket_name, path=te_input_filename),\n",
                ")\n",
                "input_data_reference.set_client(client)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef0a2d3adef6dbe6",
            "metadata": {},
            "source": [
                "Output file connection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "93d11d2581f5d8e6",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:10.145056Z",
                    "start_time": "2025-01-09T15:18:10.142493Z"
                },
                "id": "81c72273-05aa-4f62-92ae-2ed011072e65"
            },
            "outputs": [],
            "source": [
                "result_data_reference = DataConnection(\n",
                "    connection_asset_id=cos_connection_id,\n",
                "    location=S3Location(\n",
                "        bucket=cos_bucket_name,\n",
                "        path=te_result_filename\n",
                "    )\n",
                ")\n",
                "result_data_reference.set_client(client)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9770cd3738d64e83",
            "metadata": {},
            "source": [
                "<a id=\"run-te\"></a>\n",
                "## Process data using the Text Extraction service"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "629b6e92699ab5f7",
            "metadata": {},
            "source": [
                "Initialize the Text Extraction service endpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "ec363d7dd83678fe",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:13.718252Z",
                    "start_time": "2025-01-09T15:18:12.370183Z"
                },
                "id": "fbacb600-3fac-4360-a73b-acd3a22e4dc9"
            },
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.foundation_models.extractions import TextExtractions\n",
                "\n",
                "extraction = TextExtractions(\n",
                "    credentials=credentials,\n",
                "    space_id=SPACE_ID,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ee3d5c200c3b3dd",
            "metadata": {},
            "source": [
                "Run a text extraction job for connections created in the previous step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "dcc836b68a2db40a",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:15.347001Z",
                    "start_time": "2025-01-09T15:18:14.364135Z"
                },
                "id": "10c0ebab-37b0-4eea-b0e8-8be7f09f774a"
            },
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.metanames import TextExtractionsMetaNames\n",
                "\n",
                "response = extraction.run_job(\n",
                "    document_reference=input_data_reference,\n",
                "    results_reference=result_data_reference,\n",
                "    steps={\n",
                "        TextExtractionsMetaNames.OCR: {\n",
                "            \"process_image\": True,\n",
                "            \"languages_list\": [\"en\"],\n",
                "        },\n",
                "        TextExtractionsMetaNames.TABLE_PROCESSING: {\"enabled\": True},\n",
                "    },\n",
                "    results_format=\"markdown\",\n",
                ")\n",
                "\n",
                "job_id = response['metadata']['id']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ca63ff5343081559",
            "metadata": {},
            "source": [
                "Wait for the job to be complete."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "9a933380bd5cfc36",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:48.549914Z",
                    "start_time": "2025-01-09T15:18:16.907951Z"
                },
                "id": "1ec0a420-d284-4577-8d55-8b11831f997d"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Job completed successfully, details: {\n",
                        "  \"entity\": {\n",
                        "    \"assembly_md\": {},\n",
                        "    \"document_reference\": {\n",
                        "      \"connection\": {\n",
                        "        \"id\": \"5841723d-848f-440a-82b0-b6ad59d983ec\"\n",
                        "      },\n",
                        "      \"location\": {\n",
                        "        \"bucket\": \"autoai-rag-with-extraction-experiment\",\n",
                        "        \"file_name\": \"granite_code_models_paper.pdf\"\n",
                        "      },\n",
                        "      \"type\": \"connection_asset\"\n",
                        "    },\n",
                        "    \"parameters\": {\n",
                        "      \"create_embedded_images\": \"disabled\",\n",
                        "      \"languages\": [\n",
                        "        \"en\"\n",
                        "      ],\n",
                        "      \"mode\": \"standard\",\n",
                        "      \"output_dpi\": 72,\n",
                        "      \"output_tokens_and_bbox\": true,\n",
                        "      \"requested_outputs\": [\n",
                        "        \"md\"\n",
                        "      ]\n",
                        "    },\n",
                        "    \"results\": {\n",
                        "      \"completed_at\": \"2025-06-26T15:23:52.153Z\",\n",
                        "      \"location\": [\n",
                        "        \"granite_code_models_paper.md\"\n",
                        "      ],\n",
                        "      \"number_pages_processed\": 28,\n",
                        "      \"running_at\": \"2025-06-26T15:23:03.652Z\",\n",
                        "      \"status\": \"completed\"\n",
                        "    },\n",
                        "    \"results_reference\": {\n",
                        "      \"connection\": {\n",
                        "        \"id\": \"5841723d-848f-440a-82b0-b6ad59d983ec\"\n",
                        "      },\n",
                        "      \"location\": {\n",
                        "        \"bucket\": \"autoai-rag-with-extraction-experiment\",\n",
                        "        \"file_name\": \"granite_code_models_paper.md\"\n",
                        "      },\n",
                        "      \"type\": \"connection_asset\"\n",
                        "    },\n",
                        "    \"steps\": {\n",
                        "      \"ocr\": {\n",
                        "        \"languages_list\": [\n",
                        "          \"en\"\n",
                        "        ]\n",
                        "      },\n",
                        "      \"tables_processing\": {\n",
                        "        \"enabled\": true\n",
                        "      }\n",
                        "    }\n",
                        "  },\n",
                        "  \"metadata\": {\n",
                        "    \"created_at\": \"2025-06-26T15:22:30.491Z\",\n",
                        "    \"id\": \"7de56057-2d0d-48be-91a8-ca1df1a737bd\",\n",
                        "    \"modified_at\": \"2025-06-26T15:24:05.852Z\",\n",
                        "    \"space_id\": \"9f44cc2b-b3d0-4472-824e-4941afb1617b\"\n",
                        "  }\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import time\n",
                "\n",
                "while True:\n",
                "    job_details = extraction.get_job_details(job_id)\n",
                "    status = job_details['entity']['results']['status']\n",
                "\n",
                "    if status == \"completed\":\n",
                "        print(\"Job completed successfully, details: {}\".format(json.dumps(job_details, indent=2)))\n",
                "        break\n",
                "\n",
                "    if status == \"failed\":\n",
                "        print(\"Job failed, details: {}. \\n Try to run job again.\".format(json.dumps(job_details, indent=2)))\n",
                "        break\n",
                "\n",
                "    time.sleep(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7635ca019a2ace22",
            "metadata": {},
            "source": [
                "Get the text extraction result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "15359c4bd8c91558",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:18:49.269284Z",
                    "start_time": "2025-01-09T15:18:48.566344Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## Granite Code Models: A Family of Open Foundation Models for Code Intelligence\n",
                            "\n",
                            "Mayank Mishra⋆ Matt Stallone⋆ Gaoyuan Zhang⋆ Yikang Shen Aditya Prasad Adriana Meza Soria Michele Merler Parameswaran Selvam Saptha Surendran Shivdeep Singh Manish Sethi Xuan-Hong Dang Pengyuan Li Kun-Lung Wu Syed Zawad Andrew Coleman Matthew White Mark Lewis Raju Pavuluri Yan Koyfman Boris Lublinsky Maximilien de Bayser Ibrahim Abdelaziz Kinjal Basu Mayank Agarwal Yi Zhou Chris Johnson Aanchal Goyal Hima Patel Yousaf Shah Petros Zerfos Heiko Ludwig Asim Munawar Maxwell Crouse Pavan Kapanipathi Shweta Salaria Bob Calio Sophia Wen Seetharami Seelam Brian Belgodere Carlos Fonseca Amith Singhee Nirmit Desai David D. Cox Ruchir Puri† Rameswar Panda†\n",
                            "\n",
                            "IBM Research\n",
                            "\n",
                            "⋆Equal\n",
                            "\n",
                            "Contribution\n",
                            "\n",
                            "†Corresponding Authors ruchir@us.ibm.com, rpanda@ibm.com\n",
                            "\n",
                            "## Abstract\n",
                            "\n",
                            "Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being inte grated into software development environments to improve the produc tivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software devel opment workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile “all around” code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.\n",
                            "\n",
                            " https://github.com/ibm-granite/granite-code-models\n",
                            "\n",
                            "## 1 Introduction\n",
                            "\n",
                            "Over the last several decades, software has been woven into the fabric of every aspect of our society. As demand for software development surges, it is more critical than ever to increase software development productivity, and LLMs provide promising path for augmenting human programmers. Prominent enterprise use cases for LLMs in software development productivity include code generation, code explanation, code fixing, unit test and documentation generation, application modernization, vulnerability detection, code translation, and more.\n",
                            "\n",
                            "Recent years have seen rapid progress in LLM’s ability to generate and manipulate code, and a range of models with impressive coding a"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from IPython.display import display, Markdown\n",
                "\n",
                "cos_client.download_file(\n",
                "    Bucket=cos_bucket_name,\n",
                "    Key=te_result_filename,\n",
                "    Filename=te_result_filename\n",
                ")\n",
                "\n",
                "with open(te_result_filename, 'r', encoding='utf-8') as file:\n",
                "    # Display beginning of the result file\n",
                "    display(Markdown((file.read()[:3000])))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ce5b2ba9ba5f967",
            "metadata": {},
            "source": [
                "<a id=\"prepare-autorag\"></a>\n",
                "## Prepare data and connections for the AutoAI RAG experiment"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63872845c85d401d",
            "metadata": {},
            "source": [
                "Upload a `json` file to use for benchmarking to COS and define a connection to this file. \n",
                "\n",
                "Note: `correct_answer_document_ids` must refer to the document processed by text extraction service, not the initial document."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "7491bef8c96b7040",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:20:47.702283Z",
                    "start_time": "2025-01-09T15:20:47.699273Z"
                },
                "id": "e399f617-9515-48d7-a38c-e66c8116dc34"
            },
            "outputs": [],
            "source": [
                "benchmarking_data = [\n",
                "     {\n",
                "        \"question\": \"What are the two main variants of Granite Code models?\",\n",
                "        \"correct_answer\": \"The two main variants are Granite Code Base and Granite Code Instruct.\",\n",
                "        \"correct_answer_document_ids\": [te_result_filename]\n",
                "     },\n",
                "     {\n",
                "        \"question\": \"What is the purpose of Granite Code Instruct models?\",\n",
                "        \"correct_answer\": \"Granite Code Instruct models are finetuned for instruction-following tasks using datasets like CommitPack, OASST, HelpSteer, and synthetic code instruction datasets, aiming to improve reasoning and instruction-following capabilities.\",\n",
                "        \"correct_answer_document_ids\": [te_result_filename]\n",
                "     },\n",
                "     {\n",
                "        \"question\": \"What is the licensing model for Granite Code models?\",\n",
                "        \"correct_answer\": \"Granite Code models are released under the Apache 2.0 license, ensuring permissive and enterprise-friendly usage.\",\n",
                "        \"correct_answer_document_ids\": [te_result_filename]\n",
                "     },\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "c0e92e1d32467d6f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:20:48.687662Z",
                    "start_time": "2025-01-09T15:20:48.178141Z"
                },
                "id": "008a8db8-b1d2-42f8-9f25-1861d1ea4771"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "test_filename = \"benchmark.json\"\n",
                "\n",
                "if not os.path.isfile(test_filename):\n",
                "    with open(test_filename, \"w\") as json_file:\n",
                "        json.dump(benchmarking_data, json_file, indent=4)\n",
                "\n",
                "cos_client.upload_file(test_filename, cos_bucket_name, test_filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1a15c37edb8aa736",
            "metadata": {},
            "source": [
                "Test the data connection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "a5fd521450aa575d",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:20:50.372300Z",
                    "start_time": "2025-01-09T15:20:50.369900Z"
                },
                "id": "49237e6c-aa57-459a-a9b8-99bc00c29573"
            },
            "outputs": [],
            "source": [
                "test_data_reference = DataConnection(\n",
                "    connection_asset_id=cos_connection_id,\n",
                "    location=S3Location(bucket=cos_bucket_name, path=test_filename),\n",
                ")\n",
                "test_data_reference.set_client(client)\n",
                "\n",
                "test_data_references = [test_data_reference]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bcc6c6b167fe11fd",
            "metadata": {},
            "source": [
                "Use the reference to the Text Extraction job result as input for the AutoAI RAG experiment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "4451ab741f192d94",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:20:51.336293Z",
                    "start_time": "2025-01-09T15:20:51.334349Z"
                },
                "id": "eb86615c-5d19-46e1-8179-9af89eaf0d58"
            },
            "outputs": [],
            "source": [
                "input_data_references = [result_data_reference]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5af97452ce3a4da6",
            "metadata": {},
            "source": [
                "<a id=\"run-autorag\"></a>\n",
                "# Run the AutoAI RAG experiment\n",
                "\n",
                "Provide the input information for AutoAI RAG optimizer:\n",
                "- `name` - experiment name\n",
                "- `description` - experiment description\n",
                "- `max_number_of_rag_patterns` - maximum number of RAG patterns to create\n",
                "- `optimization_metrics` - target optimization metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "619457e9a66bcc24",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:20:55.393763Z",
                    "start_time": "2025-01-09T15:20:53.462724Z"
                },
                "id": "089a0f87-c462-4873-9e86-e2f44e5f5ffd"
            },
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.experiment import AutoAI\n",
                "\n",
                "experiment = AutoAI(credentials, space_id=SPACE_ID)\n",
                "\n",
                "rag_optimizer = experiment.rag_optimizer(\n",
                "    name='AutoAI RAG - Text Extraction service experiment',\n",
                "    description = \"AutoAI RAG experiment on documents generated by text extraction service\",\n",
                "    max_number_of_rag_patterns=5,\n",
                "    optimization_metrics=['answer_correctness']\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eebec0a74b713ef7",
            "metadata": {},
            "source": [
                "Call the `run()` method to trigger the AutoAI RAG experiment. Choose one of two modes: \n",
                "\n",
                "- To use the **interactive mode** (synchronous job), specify `background_mode=False` \n",
                "- To use the **background mode** (asynchronous job), specify `background_mode=True`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "aa3fc7d25f0cf32f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:42:37.566687Z",
                    "start_time": "2025-01-09T15:39:11.085027Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "##############################################\n",
                        "\n",
                        "Running 'f2c34e8d-b613-4f25-ab1e-943c5fb8837a'\n",
                        "\n",
                        "##############################################\n",
                        "\n",
                        "\n",
                        "pending............\n",
                        "running......................................................\n",
                        "completed\n",
                        "Training of 'f2c34e8d-b613-4f25-ab1e-943c5fb8837a' finished successfully.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'entity': {'hardware_spec': {'id': 'a6c4923b-b8e4-444c-9f43-8a7ec3020110',\n",
                            "   'name': 'L'},\n",
                            "  'input_data_references': [{'connection': {'id': '5841723d-848f-440a-82b0-b6ad59d983ec'},\n",
                            "    'location': {'bucket': 'autoai-rag-with-extraction-experiment',\n",
                            "     'file_name': 'granite_code_models_paper.md'},\n",
                            "    'type': 'connection_asset'}],\n",
                            "  'parameters': {'constraints': {'max_number_of_rag_patterns': 5},\n",
                            "   'optimization': {'metrics': ['answer_correctness']},\n",
                            "   'output_logs': True},\n",
                            "  'results': [{'context': {'iteration': 0,\n",
                            "     'max_combinations': 240,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 17,\n",
                            "      'location': {'evaluation_results': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/evaluation_results.json',\n",
                            "       'indexing_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern1',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "        'chunk_size': 1024,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'intfloat/multilingual-e5-large',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': '[Document]\\n{document}\\n[End]',\n",
                            "        'model_id': 'ibm/granite-3-8b-instruct',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 131072,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': '<|system|>\\nYou are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.<|user|>\\nYou are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is grounded in context and user query. Always make sure that your response is relevant to the question. \\nAnswer Length: detailed\\n{reference_documents}\\nRespond exclusively in the language of the question, regardless of any other language used in the provided context. Ensure that your entire response is in the same language as the question.\\n{question} \\n\\n<|assistant|>',\n",
                            "        'word_to_token_ratio': 2.573},\n",
                            "       'retrieval': {'method': 'window',\n",
                            "        'number_of_chunks': 5,\n",
                            "        'window_size': 2},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_f2c34e8d_20250626153459',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.125,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.125, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.125, 'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.125, 'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.125, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.125, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.125, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {'name': 'autoai-rag_rt24.1-py3.11'}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 1.0,\n",
                            "       'ci_low': 0.6578,\n",
                            "       'mean': 0.7813,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.8336,\n",
                            "       'ci_low': 0.4882,\n",
                            "       'mean': 0.7,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}},\n",
                            "   {'context': {'iteration': 1,\n",
                            "     'max_combinations': 240,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 11,\n",
                            "      'location': {'evaluation_results': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern2/evaluation_results.json',\n",
                            "       'indexing_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern2/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern2/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern2/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern2/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern2',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "        'chunk_size': 1024,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'intfloat/multilingual-e5-large',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': '[document]: {document}\\n',\n",
                            "        'model_id': 'meta-llama/llama-3-3-70b-instruct',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 131072,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{reference_documents}\\n[conversation]: {question}. Answer with no more than 150 words. If you cannot base your answer on the given document, please state that you do not have an answer. Respond exclusively in the language of the question, regardless of any other language used in the provided context. Ensure that your entire response is in the same language as the question.\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>',\n",
                            "        'word_to_token_ratio': 2.1967},\n",
                            "       'retrieval': {'method': 'window',\n",
                            "        'number_of_chunks': 3,\n",
                            "        'window_size': 4},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_f2c34e8d_20250626153459',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.0,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.0, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.0, 'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.5283019,\n",
                            "         'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.0, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.24528302, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.2264151, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {'name': 'autoai-rag_rt24.1-py3.11'}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 0.7662,\n",
                            "       'ci_low': 0.5556,\n",
                            "       'mean': 0.6895,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.751,\n",
                            "       'ci_low': 0.3462,\n",
                            "       'mean': 0.6085,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}},\n",
                            "   {'context': {'iteration': 2,\n",
                            "     'max_combinations': 240,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 5,\n",
                            "      'location': {'evaluation_results': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern3/evaluation_results.json',\n",
                            "       'indexing_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern3/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern3/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern3/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern3/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern3',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 128,\n",
                            "        'chunk_size': 512,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr-v2',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': '[Document]\\n{document}\\n[End]',\n",
                            "        'model_id': 'ibm/granite-3-8b-instruct',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 131072,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': '<|system|>\\nYou are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.<|user|>\\nYou are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is grounded in context and user query. Always make sure that your response is relevant to the question. \\nAnswer Length: detailed\\n{reference_documents}\\nRespond exclusively in the language of the question, regardless of any other language used in the provided context. Ensure that your entire response is in the same language as the question.\\n{question} \\n\\n<|assistant|>',\n",
                            "        'word_to_token_ratio': 2.573},\n",
                            "       'retrieval': {'method': 'window',\n",
                            "        'number_of_chunks': 3,\n",
                            "        'window_size': 4},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_f2c34e8d_20250626153542',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.093277715,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.046638858, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.20731471,\n",
                            "         'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.459491,\n",
                            "         'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.0, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.124416634, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.06886108, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {'name': 'autoai-rag_rt24.1-py3.11'}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 0.8074,\n",
                            "       'ci_low': 0.6667,\n",
                            "       'mean': 0.7569,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.7267,\n",
                            "       'ci_low': 0.5238,\n",
                            "       'mean': 0.652,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}},\n",
                            "   {'context': {'iteration': 3,\n",
                            "     'max_combinations': 240,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 26,\n",
                            "      'location': {'evaluation_results': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern4/evaluation_results.json',\n",
                            "       'indexing_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern4/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern4/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern4/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern4/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern4',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "        'chunk_size': 512,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'intfloat/multilingual-e5-large',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': '[Document]\\n{document}\\n[End]',\n",
                            "        'model_id': 'ibm/granite-3-3-8b-instruct',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 131072,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': '<|system|>\\nYou are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.<|user|>\\nYou are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is grounded in context and user query. Always make sure that your response is relevant to the question. \\nAnswer Length: detailed\\n{reference_documents}\\nRespond exclusively in the language of the question, regardless of any other language used in the provided context. Ensure that your entire response is in the same language as the question.\\n{question} \\n\\n<|assistant|>',\n",
                            "        'word_to_token_ratio': 2.573},\n",
                            "       'retrieval': {'method': 'window',\n",
                            "        'number_of_chunks': 5,\n",
                            "        'window_size': 1},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_f2c34e8d_20250626153555',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.122112766,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.022782432, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.07525133,\n",
                            "         'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.4554362,\n",
                            "         'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.0, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.22230415, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.10211313, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {'name': 'autoai-rag_rt24.1-py3.11'}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 0.5455,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.1818,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.0881,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.0294,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}},\n",
                            "   {'context': {'iteration': 4,\n",
                            "     'max_combinations': 240,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 14,\n",
                            "      'location': {'evaluation_results': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern5/evaluation_results.json',\n",
                            "       'indexing_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern5/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern5/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern5/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern5/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern5',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "        'chunk_size': 1024,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr-v2',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': '[Document]\\n{document}\\n[End]',\n",
                            "        'model_id': 'ibm/granite-3-8b-instruct',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 131072,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': '<|system|>\\nYou are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.<|user|>\\nYou are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is grounded in context and user query. Always make sure that your response is relevant to the question. \\nAnswer Length: detailed\\n{reference_documents}\\nRespond exclusively in the language of the question, regardless of any other language used in the provided context. Ensure that your entire response is in the same language as the question.\\n{question} \\n\\n<|assistant|>',\n",
                            "        'word_to_token_ratio': 2.573},\n",
                            "       'retrieval': {'method': 'window',\n",
                            "        'number_of_chunks': 3,\n",
                            "        'window_size': 4},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_f2c34e8d_20250626153627',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.0615634,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.009549737, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.06833898,\n",
                            "         'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.4761837,\n",
                            "         'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.0, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.3240124, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.060351793, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {'name': 'autoai-rag_rt24.1-py3.11'}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 0.8182,\n",
                            "       'ci_low': 0.6825,\n",
                            "       'mean': 0.733,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.7949,\n",
                            "       'ci_low': 0.5407,\n",
                            "       'mean': 0.7084,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}}],\n",
                            "  'results_reference': {'location': {'path': 'default_autoai_rag_out',\n",
                            "    'training': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a',\n",
                            "    'training_status': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/training-status.json',\n",
                            "    'training_log': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/output.log',\n",
                            "    'assets_path': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/assets'},\n",
                            "   'type': 'container'},\n",
                            "  'status': {'completed_at': '2025-06-26T15:36:48.763Z',\n",
                            "   'message': {'level': 'info',\n",
                            "    'text': 'AAR019I: AutoAI execution completed.'},\n",
                            "   'running_at': '2025-06-26T15:31:36.000Z',\n",
                            "   'state': 'completed',\n",
                            "   'step': 'generation'},\n",
                            "  'test_data_references': [{'connection': {'id': '5841723d-848f-440a-82b0-b6ad59d983ec'},\n",
                            "    'location': {'bucket': 'autoai-rag-with-extraction-experiment',\n",
                            "     'file_name': 'benchmark.json'},\n",
                            "    'type': 'connection_asset'}],\n",
                            "  'timestamp': '2025-06-26T15:36:49.506Z'},\n",
                            " 'metadata': {'created_at': '2025-06-26T15:29:46.362Z',\n",
                            "  'description': 'AutoAI RAG experiment on documents generated by text extraction service',\n",
                            "  'id': 'f2c34e8d-b613-4f25-ab1e-943c5fb8837a',\n",
                            "  'modified_at': '2025-06-26T15:36:48.887Z',\n",
                            "  'name': 'AutoAI RAG - Text Extraction service experiment',\n",
                            "  'space_id': '9f44cc2b-b3d0-4472-824e-4941afb1617b'}}"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "rag_optimizer.run(\n",
                "    input_data_references=input_data_references,\n",
                "    test_data_references=test_data_references,\n",
                "    background_mode=False\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4563ecfaed85b633",
            "metadata": {},
            "source": [
                "<a id=\"comparison\"></a>\n",
                "## Compare and test of RAG Patterns\n",
                "\n",
                "You can list the trained patterns and information on evaluation metrics in the form of a Pandas DataFrame by calling the `summary()` method. You can use the DataFrame to compare all discovered patterns and select the one you like for further testing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "dc2a28877f1459e6",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:43:10.039483Z",
                    "start_time": "2025-01-09T15:43:10.032861Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>mean_answer_correctness</th>\n",
                            "      <th>mean_faithfulness</th>\n",
                            "      <th>mean_context_correctness</th>\n",
                            "      <th>chunking.method</th>\n",
                            "      <th>chunking.chunk_size</th>\n",
                            "      <th>chunking.chunk_overlap</th>\n",
                            "      <th>embeddings.model_id</th>\n",
                            "      <th>vector_store.distance_metric</th>\n",
                            "      <th>retrieval.method</th>\n",
                            "      <th>retrieval.number_of_chunks</th>\n",
                            "      <th>retrieval.hybrid_ranker</th>\n",
                            "      <th>generation.model_id</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Pattern_Name</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>Pattern1</th>\n",
                            "      <td>0.7813</td>\n",
                            "      <td>0.7000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>recursive</td>\n",
                            "      <td>1024</td>\n",
                            "      <td>256</td>\n",
                            "      <td>intfloat/multilingual-e5-large</td>\n",
                            "      <td>cosine</td>\n",
                            "      <td>window</td>\n",
                            "      <td>5</td>\n",
                            "      <td></td>\n",
                            "      <td>ibm/granite-3-8b-instruct</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Pattern3</th>\n",
                            "      <td>0.7569</td>\n",
                            "      <td>0.6520</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>recursive</td>\n",
                            "      <td>512</td>\n",
                            "      <td>128</td>\n",
                            "      <td>ibm/slate-125m-english-rtrvr-v2</td>\n",
                            "      <td>cosine</td>\n",
                            "      <td>window</td>\n",
                            "      <td>3</td>\n",
                            "      <td></td>\n",
                            "      <td>ibm/granite-3-8b-instruct</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Pattern5</th>\n",
                            "      <td>0.7330</td>\n",
                            "      <td>0.7084</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>recursive</td>\n",
                            "      <td>1024</td>\n",
                            "      <td>256</td>\n",
                            "      <td>ibm/slate-125m-english-rtrvr-v2</td>\n",
                            "      <td>cosine</td>\n",
                            "      <td>window</td>\n",
                            "      <td>3</td>\n",
                            "      <td></td>\n",
                            "      <td>ibm/granite-3-8b-instruct</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Pattern2</th>\n",
                            "      <td>0.6895</td>\n",
                            "      <td>0.6085</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>recursive</td>\n",
                            "      <td>1024</td>\n",
                            "      <td>256</td>\n",
                            "      <td>intfloat/multilingual-e5-large</td>\n",
                            "      <td>cosine</td>\n",
                            "      <td>window</td>\n",
                            "      <td>3</td>\n",
                            "      <td></td>\n",
                            "      <td>meta-llama/llama-3-3-70b-instruct</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Pattern4</th>\n",
                            "      <td>0.1818</td>\n",
                            "      <td>0.0294</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>recursive</td>\n",
                            "      <td>512</td>\n",
                            "      <td>256</td>\n",
                            "      <td>intfloat/multilingual-e5-large</td>\n",
                            "      <td>cosine</td>\n",
                            "      <td>window</td>\n",
                            "      <td>5</td>\n",
                            "      <td></td>\n",
                            "      <td>ibm/granite-3-3-8b-instruct</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "              mean_answer_correctness  mean_faithfulness  \\\n",
                            "Pattern_Name                                               \n",
                            "Pattern1                       0.7813             0.7000   \n",
                            "Pattern3                       0.7569             0.6520   \n",
                            "Pattern5                       0.7330             0.7084   \n",
                            "Pattern2                       0.6895             0.6085   \n",
                            "Pattern4                       0.1818             0.0294   \n",
                            "\n",
                            "              mean_context_correctness chunking.method  chunking.chunk_size  \\\n",
                            "Pattern_Name                                                                  \n",
                            "Pattern1                           1.0       recursive                 1024   \n",
                            "Pattern3                           1.0       recursive                  512   \n",
                            "Pattern5                           1.0       recursive                 1024   \n",
                            "Pattern2                           1.0       recursive                 1024   \n",
                            "Pattern4                           1.0       recursive                  512   \n",
                            "\n",
                            "              chunking.chunk_overlap              embeddings.model_id  \\\n",
                            "Pattern_Name                                                            \n",
                            "Pattern1                         256   intfloat/multilingual-e5-large   \n",
                            "Pattern3                         128  ibm/slate-125m-english-rtrvr-v2   \n",
                            "Pattern5                         256  ibm/slate-125m-english-rtrvr-v2   \n",
                            "Pattern2                         256   intfloat/multilingual-e5-large   \n",
                            "Pattern4                         256   intfloat/multilingual-e5-large   \n",
                            "\n",
                            "             vector_store.distance_metric retrieval.method  \\\n",
                            "Pattern_Name                                                 \n",
                            "Pattern1                           cosine           window   \n",
                            "Pattern3                           cosine           window   \n",
                            "Pattern5                           cosine           window   \n",
                            "Pattern2                           cosine           window   \n",
                            "Pattern4                           cosine           window   \n",
                            "\n",
                            "              retrieval.number_of_chunks retrieval.hybrid_ranker  \\\n",
                            "Pattern_Name                                                       \n",
                            "Pattern1                               5                           \n",
                            "Pattern3                               3                           \n",
                            "Pattern5                               3                           \n",
                            "Pattern2                               3                           \n",
                            "Pattern4                               5                           \n",
                            "\n",
                            "                            generation.model_id  \n",
                            "Pattern_Name                                     \n",
                            "Pattern1              ibm/granite-3-8b-instruct  \n",
                            "Pattern3              ibm/granite-3-8b-instruct  \n",
                            "Pattern5              ibm/granite-3-8b-instruct  \n",
                            "Pattern2      meta-llama/llama-3-3-70b-instruct  \n",
                            "Pattern4            ibm/granite-3-3-8b-instruct  "
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "summary = rag_optimizer.summary()\n",
                "summary"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dd2f1b669bfa61df",
            "metadata": {},
            "source": [
                "### Get the selected pattern\n",
                "\n",
                "Get the RAGPattern object from the RAG Optimizer experiment. By default, the RAGPattern of the best pattern is returned."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "e423064a26b8f02e",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:43:21.184717Z",
                    "start_time": "2025-01-09T15:43:16.112583Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best pattern is: Pattern1\n"
                    ]
                }
            ],
            "source": [
                "best_pattern_name = summary.index.values[0]\n",
                "print('Best pattern is:', best_pattern_name)\n",
                "\n",
                "best_pattern = rag_optimizer.get_pattern()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "4cb7325de0524ebc",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:43:23.819629Z",
                    "start_time": "2025-01-09T15:43:23.816088Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'composition_steps': ['model_selection',\n",
                            "  'chunking',\n",
                            "  'embeddings',\n",
                            "  'retrieval',\n",
                            "  'generation'],\n",
                            " 'duration_seconds': 17,\n",
                            " 'location': {'evaluation_results': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/evaluation_results.json',\n",
                            "  'indexing_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/indexing_inference_notebook.ipynb',\n",
                            "  'inference_notebook': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/indexing_inference_notebook.ipynb',\n",
                            "  'inference_service_code': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/inference_ai_service.gz',\n",
                            "  'inference_service_metadata': 'default_autoai_rag_out/f2c34e8d-b613-4f25-ab1e-943c5fb8837a/Pattern1/inference_service_metadata.json'},\n",
                            " 'name': 'Pattern1',\n",
                            " 'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "   'chunk_size': 1024,\n",
                            "   'method': 'recursive'},\n",
                            "  'embeddings': {'model_id': 'intfloat/multilingual-e5-large',\n",
                            "   'truncate_input_tokens': 512,\n",
                            "   'truncate_strategy': 'left'},\n",
                            "  'generation': {'context_template_text': '[Document]\\n{document}\\n[End]',\n",
                            "   'model_id': 'ibm/granite-3-8b-instruct',\n",
                            "   'parameters': {'decoding_method': 'greedy',\n",
                            "    'max_new_tokens': 1000,\n",
                            "    'min_new_tokens': 1},\n",
                            "   'prompt_template_text': '<|system|>\\nYou are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.<|user|>\\nYou are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is grounded in context and user query. Always make sure that your response is relevant to the question. \\nAnswer Length: detailed\\n{reference_documents}\\nRespond exclusively in the language of the question, regardless of any other language used in the provided context. Ensure that your entire response is in the same language as the question.\\n{question} \\n\\n<|assistant|>',\n",
                            "   'word_to_token_ratio': 2.573},\n",
                            "  'retrieval': {'method': 'window', 'number_of_chunks': 5, 'window_size': 2},\n",
                            "  'vector_store': {'datasource_type': 'chroma',\n",
                            "   'distance_metric': 'cosine',\n",
                            "   'index_name': 'autoai_rag_f2c34e8d_20250626153459',\n",
                            "   'operation': 'upsert',\n",
                            "   'schema': {'fields': [{'description': 'text field',\n",
                            "      'name': 'text',\n",
                            "      'role': 'text',\n",
                            "      'type': 'string'},\n",
                            "     {'description': 'document name field',\n",
                            "      'name': 'document_id',\n",
                            "      'role': 'document_name',\n",
                            "      'type': 'string'},\n",
                            "     {'description': 'chunk starting token position in the source document',\n",
                            "      'name': 'start_index',\n",
                            "      'role': 'start_index',\n",
                            "      'type': 'number'},\n",
                            "     {'description': 'chunk number per document',\n",
                            "      'name': 'sequence_number',\n",
                            "      'role': 'sequence_number',\n",
                            "      'type': 'number'},\n",
                            "     {'description': 'vector embeddings',\n",
                            "      'name': 'vector',\n",
                            "      'role': 'vector_embeddings',\n",
                            "      'type': 'array'}],\n",
                            "    'id': 'autoai_rag_1.0',\n",
                            "    'name': 'Document schema using open-source loaders',\n",
                            "    'type': 'struct'}}},\n",
                            " 'settings_importance': {'chunking': [{'importance': 0.125,\n",
                            "    'parameter': 'chunk_size'},\n",
                            "   {'importance': 0.125, 'parameter': 'chunk_overlap'}],\n",
                            "  'embeddings': [{'importance': 0.125, 'parameter': 'embedding_model'}],\n",
                            "  'generation': [{'importance': 0.125, 'parameter': 'foundation_model'}],\n",
                            "  'retrieval': [{'importance': 0.125, 'parameter': 'retrieval_method'},\n",
                            "   {'importance': 0.125, 'parameter': 'window_size'},\n",
                            "   {'importance': 0.125, 'parameter': 'number_of_chunks'}]}}"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "rag_optimizer.get_pattern_details(pattern_name=best_pattern_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3717292bbd09cd99",
            "metadata": {},
            "source": [
                "Test the RAGPattern by querying it locally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "e3fa414e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.deployments import RuntimeContext\n",
                "\n",
                "runtime_context = RuntimeContext(api_client=client)\n",
                "inference_service_function = best_pattern.inference_service(runtime_context)[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "592456e149f2e372",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:43:33.270994Z",
                    "start_time": "2025-01-09T15:43:26.474640Z"
                },
                "id": "55347731-5421-4c10-ae4c-fe64d0ebd0ed"
            },
            "outputs": [],
            "source": [
                "question = \"Which industry players are mentioned as IBM’s strategic partners?\"\n",
                "\n",
                "context = RuntimeContext(\n",
                "    api_client=client,\n",
                "    request_payload_json={\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
                ")\n",
                "\n",
                "\n",
                "resp = inference_service_function(context)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "6a9c4abbcb4d316b",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:43:33.278401Z",
                    "start_time": "2025-01-09T15:43:33.276771Z"
                },
                "id": "34477967-8812-41bd-93ee-b967261bcef3"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "The document mentions IBM Research AI and Hybrid Cloud Platform, IBM AI Infrastructure team, IBM WatsonX Code Assistant and platform team as IBM's strategic partners.\n",
                        "\n",
                        "Additionally, IBM acknowledges the support of several leaders, including Dario Gil, Sriram Raghavan, Mukesh Khare, Danny Barnett, Talia Gershon, Priya Nagpurkar, Nicholas Fuller.\n",
                        "\n",
                        "Furthermore, IBM thanks and acknowledges Trent Gray-Donald, Keri Olson, Alvin Tan, Hillery Hunter, Dakshi Agrawal, Xuan Liu, Mudhakar Srivatsa, Raghu Kiran Ganti, Carlos Costa, Darrell Reimer, Maja Vukovic, Dinesh Garg, Akash Srivastava, Abhishek Bhandwaldar, Aldo Pareja, Shiv Sudalairaj, Atin Sood, Sandeep Gopisetty, Nick Hill, Ray Rose, Tulio Coppola, Allysson ´ Oliveira, Aadarsh Sahoo, Apoorve Mohan, Yuan Chi Chang, Jitendra Singh, Yuya Ong, Eric Butler, David Brotherton, Rakesh Mohan, David Kung, Dinesh Khandelwal, Naigang Wang, Nelson Mimura Gonzalez, Olivier Tardieu, Tuan Hoang Trong, Luis Angel Bathen, Kevin O’Connor, Christopher Laibinis, Tatsuhiro Chiba, Sunyanan Choochotkaew, Robert Walkup, Antoni Viros i Martin, Adnan Hoque, Davis Wertheimer and Marquita Ellis.\n",
                        "\n",
                        "These individuals and teams are mentioned as IBM's strategic partners in the context of developing and releasing the Granite Code models.\n",
                        "\n",
                        "Reference(s):\n",
                        "Document\n",
                        "\n",
                        "[End]\n",
                        "\n",
                        "Note: The names mentioned are not hyperlinked as they are not clickable references in this format. They are listed as per the provided document.\n"
                    ]
                }
            ],
            "source": [
                "print(inference_service_function(context)[\"body\"][\"choices\"][0][\"message\"][\"content\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f2d80de4b2b1d60",
            "metadata": {},
            "source": [
                "### Deploy the RAGPattern\n",
                "\n",
                "Store the defined RAG function and create a deployed asset to deploy the RAGPattern."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "85769eb45d41892f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:44:02.586747Z",
                    "start_time": "2025-01-09T15:43:33.300291Z"
                },
                "id": "6e9565b6-9937-4adb-b0a0-865293fa9d3a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "######################################################################################\n",
                        "\n",
                        "Synchronous deployment creation for id: '679335fe-1686-493f-8845-5920d26ed862' started\n",
                        "\n",
                        "######################################################################################\n",
                        "\n",
                        "\n",
                        "initializing....................................................................................................................\n",
                        "ready\n",
                        "\n",
                        "\n",
                        "-----------------------------------------------------------------------------------------------\n",
                        "Successfully finished deployment creation, deployment_id='e242a690-0b72-4f94-bf6d-b2f7688e23e7'\n",
                        "-----------------------------------------------------------------------------------------------\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "deployment_details = best_pattern.inference_service.deploy(\n",
                "    name=\"AutoAI RAG deployment - ibm_watsonx_ai documentataion\",\n",
                "    space_id=SPACE_ID,\n",
                "    deploy_params={\"tags\": [\"wx-autoai-rag\"]}\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1ba0e83cfdbf9fa8",
            "metadata": {},
            "source": [
                "### Test the deployed function\n",
                "\n",
                "The RAG service is now deployed in the space. To test the solution, run the cell below. Questions have to be provided in the payload. Their format is provided below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "ddf657bac40a25f0",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:44:08.959582Z",
                    "start_time": "2025-01-09T15:44:02.594917Z"
                },
                "id": "79f5e89f-d5a7-4454-98fd-45325b366dfd"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'predictions': [{'fields': ['answer', 'reference_documents'],\n",
                            "   'values': [[\"\\n\\nBased on the available information, IBM's strategic partners mentioned in the industry include:\\n\\n* Salesforce: A leading customer relationship management (CRM) platform provider, with which IBM has a global strategic partnership to deliver joint solutions for artificial intelligence, blockchain, and the Internet of Things (IoT).\\n* Apple: A technology giant with which IBM has a partnership to develop enterprise mobility solutions, including iOS apps and IBM cloud services.\\n* Red Hat: An open-source software provider acquired by IBM, which has enabled the company to expand its cloud capabilities and offer a hybrid cloud platform.\\n* Box: A cloud content management platform provider with which IBM has a partnership to integrate its cloud storage and content management capabilities.\\n\\nPlease note that this information may not be exhaustive, and there might be other strategic partners not mentioned here.\",\n",
                            "     []]]}]}"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "deployment_id = client.deployments.get_id(deployment_details)\n",
                "\n",
                "payload = {\n",
                "    \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
                "}\n",
                "score_response = client.deployments.run_ai_service(deployment_id, payload)\n",
                "score_response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "db6e9ab2b9cd65d8",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-01-09T15:44:08.972331Z",
                    "start_time": "2025-01-09T15:44:08.970146Z"
                },
                "id": "0157dc60-b62d-4839-86fc-007c838ce66f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Based on the available information, IBM's strategic partners mentioned in the industry include:\n",
                        "\n",
                        "* Salesforce: A leading customer relationship management (CRM) platform provider, with which IBM has a global strategic partnership to deliver joint solutions for artificial intelligence, blockchain, and the Internet of Things (IoT).\n",
                        "* Apple: A technology giant with which IBM has a partnership to develop enterprise mobility solutions, including iOS apps and IBM cloud services.\n",
                        "* Red Hat: An open-source software provider acquired by IBM, which has enabled the company to expand its cloud capabilities and offer a hybrid cloud platform.\n",
                        "* Box: A cloud content management platform provider with which IBM has a partnership to integrate its cloud storage and content management capabilities.\n",
                        "\n",
                        "Please note that this information may not be exhaustive, and there might be other strategic partners not mentioned here.\n"
                    ]
                }
            ],
            "source": [
                "print(score_response[\"predictions\"][0][\"values\"][0][0])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7bfe92044e177db",
            "metadata": {},
            "source": [
                "<a id=\"summary\"></a>\n",
                "## Summary\n",
                "\n",
                " You successfully completed this notebook!\n",
                " \n",
                " You learned how to use AutoAI RAG with documents processed by the TextExtraction service.\n",
                " \n",
                "Check out our _<a href=\"https://ibm.github.io/watson-machine-learning-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3fc6379584dc38f4",
            "metadata": {},
            "source": [
                "### Author:\n",
                " **Witold Nowogórski**, Software Engineer at watsonx.ai."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8cc73dddba1afbbd",
            "metadata": {},
            "source": [
                "Copyright © 2025 IBM. This notebook and its source code are released under the terms of the MIT License."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "autoai_rag",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
