{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
        "# Use watsonx, and `ibm/granite-3-3-8b-instruct` to perform chat conversation with JSON response format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Disclaimers\n",
        "\n",
        "- Use only Projects and Spaces that are available in watsonx context.\n",
        "\n",
        "\n",
        "## Notebook content\n",
        "\n",
        "This notebook provides a detailed demonstration of the steps and code required to showcase support for JSON response format in Chat models.\n",
        "\n",
        "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
        "\n",
        "\n",
        "## Learning goal\n",
        "\n",
        "The purpose of this notebook is to demonstrate how to use JSON response format in Chat models and how to specify the response JSON schema.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "This notebook contains the following parts:\n",
        "\n",
        "- [Setup](#setup)\n",
        "- [Foundation Models on watsonx](#models)\n",
        "- [Work with JSON response format](#json-response-format)\n",
        "- [Work with specified JSON schema response format](#json-specific-format)\n",
        "- [Summary](#summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "## Set up the environment\n",
        "\n",
        "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
        "\n",
        "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watsonxai-runtime\" target=\"_blank\" rel=\"noopener no referrer\">watsonx.ai Runtime Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Install and import the `datasets` and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed ibm-watsonx-ai-1.3.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U \"ibm-watsonx-ai\" | tail -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Define the watsonx.ai credentials\n",
        "Use the code cell below to define the watsonx.ai credentials that are required to work with watsonx Foundation Model inferencing.\n",
        "\n",
        "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">Managing user API keys</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "from ibm_watsonx_ai import Credentials\n",
        "\n",
        "credentials = Credentials(\n",
        "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "    api_key=getpass.getpass(\"Enter your watsonx.ai api key and hit enter: \"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Define the project ID\n",
        "You need to provide the project ID to give the Foundation Model the context for the call. If you have a default project ID set in Watson Studio, the notebook obtains that project ID. Otherwise, you need to provide the project ID in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    project_id = os.environ[\"PROJECT_ID\"]\n",
        "except KeyError:\n",
        "    project_id = input(\"Enter your project_id and hit enter: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"models\"></a>\n",
        "## Set up the Foundation Model on `watsonx.ai`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify the `model_id` of the model you will use for the chat with tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id = \"ibm/granite-3-3-8b-instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model parameters overview\n",
        "\n",
        "In order to receive the response from the model in the JSON format, the `TextChatResponseFormatType.JSON_OBJECT` response format must be specified. You might also need to adjust model parameters depending on the model you use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| PARAMETER         | TYPE                                   | EXAMPLE VALUE                |\n",
            "+===================+========================================+==============================+\n",
            "| frequency_penalty | float, NoneType                        | 0.5                          |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| logprobs          | bool, NoneType                         | True                         |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| top_logprobs      | int, NoneType                          | 3                            |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| presence_penalty  | float, NoneType                        | 0.3                          |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| response_format   | dict, TextChatResponseFormat, NoneType | {'type': 'json_object'}      |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| temperature       | float, NoneType                        | 0.7                          |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| max_tokens        | int, NoneType                          | 100                          |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| time_limit        | int, NoneType                          | 600000                       |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| top_p             | float, NoneType                        | 0.9                          |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| n                 | int, NoneType                          | 1                            |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| logit_bias        | dict, NoneType                         | {'1003': -100, '1004': -100} |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| seed              | int, NoneType                          | 41                           |\n",
            "+-------------------+----------------------------------------+------------------------------+\n",
            "| stop              | list, NoneType                         | ['this', 'the']              |\n",
            "+-------------------+----------------------------------------+------------------------------+\n"
          ]
        }
      ],
      "source": [
        "from ibm_watsonx_ai.foundation_models.schema import TextChatParameters\n",
        "\n",
        "TextChatParameters.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------------------------------------------+-----------------+\n",
            "| PARAMETER   | TYPE                                      | EXAMPLE VALUE   |\n",
            "+=============+===========================================+=================+\n",
            "| type        | str, TextChatResponseFormatType, NoneType | json_object     |\n",
            "+-------------+-------------------------------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "from ibm_watsonx_ai.foundation_models.schema import TextChatResponseFormat, TextChatResponseFormatType\n",
        "\n",
        "TextChatResponseFormat.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"json-response-format\"></a>\n",
        "## Work with JSON response format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize the model\n",
        "\n",
        "Initialize the `ModelInference` class with provided parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "\n",
        "\n",
        "params = TextChatParameters(\n",
        "    response_format=TextChatResponseFormat(TextChatResponseFormatType.JSON_OBJECT),\n",
        "    max_tokens=1024,\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "model = ModelInference(\n",
        "    model_id=model_id,\n",
        "    credentials=credentials,\n",
        "    project_id=project_id,\n",
        "    params=params\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create messages and chat with the model\n",
        "\n",
        "In order to ensure the response is in the correct format, the sent messages must contain an indication that JSON is expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Respond in a JSON format\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Describe methods of calculating pi\"\n",
        "    },\n",
        "]\n",
        "\n",
        "chat_response = model.chat(messages=messages, params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse the response\n",
        "\n",
        "Use the `json` library to parse the chat response content into a Python-native data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"Methods\": [\n",
            "    {\n",
            "      \"name\": \"Gregory-Leibniz Series\",\n",
            "      \"description\": \"This is an infinite series: Pi = 4 * (1 - 1/3 + 1/5 - 1/7 + 1/9 - 1/11 ... ). It converges very slowly, so many terms must be calculated for an accurate result.\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Nilakantha Series\",\n",
            "      \"description\": \"This infinite series is: Pi = 3 + 4/(2*3*4) - 4/(4*5*6) + 4/(6*7*8) - 4/(8*9*10) ... . It converges faster than the Gregory-Leibniz Series.\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Machin-like Formulas\",\n",
            "      \"description\": \"These include formulas like Pi = 16 * arctan(1/5) - 4 * arctan(1/239). Each arctan can be calculated using a Taylor series.\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Gauss–Legendre Algorithm\",\n",
            "      \"description\": \"This algorithm uses the arithmetic-geometric mean to calculate Pi. It converges extremely fast, with about 10 correct digits of Pi per iteration.\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Ramanujan’s Series\",\n",
            "      \"description\": \"A formula discovered by Srinivasa Ramanujan: Pi^(95)/N = secpant90^3 - 18*secpant90 - 18 with secpant90 = 4*arctan(1/5.7) - arctan(1/239). Though complex, it converges quite rapidly.\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Monte Carlo Methods\",\n",
            "      \"description\": \"This method uses random sampling to approximate Pi. It involves calculating the ratio of the area of a quarter circle to the square in which it's inscribed.\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_response_content = json.loads(chat_response[\"choices\"][0][\"message\"][\"content\"])\n",
        "print(json.dumps(json_response_content, ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"json-specific-format\"></a>\n",
        "## Work with specified JSON schema response format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize the model\n",
        "\n",
        "Initialize a new `ModelInference` class with the provided parameters, including the expected JSON schema. For more info about JSON schema, visit: https://json-schema.org/learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = TextChatParameters(\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": {\n",
        "            \"name\": \"Pi calculation methods\",\n",
        "            \"schema\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"cake_name\": {\n",
        "                            \"type\": \"string\",\n",
        "                        },\n",
        "                        \"description\": {\n",
        "                            \"type\": \"string\",\n",
        "                        },\n",
        "                        \"difficulty_score\": {\n",
        "                            \"type\": \"integer\",\n",
        "                        },\n",
        "                        \"expected_price\": {\n",
        "                            \"type\": \"number\",\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "            }\n",
        "        },\n",
        "        \"additionalProperties\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "model = ModelInference(\n",
        "    model_id=model_id,\n",
        "    credentials=credentials,\n",
        "    project_id=project_id,\n",
        "    params=params\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create messages and chat with the model\n",
        "\n",
        "As previously, in order to ensure the response is in the correct format, the sent message must contain an indication that JSON is expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Respond in a JSON format.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Provide a list of cake recipes. Briefly describe what the cake tastes like. Give each a difficulty score between 1-10. Also add an expected price of the cake with accuracy of 2 decimal places.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "chat_response = model.chat(messages=messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse the response\n",
        "\n",
        "As previously, use the `json` library to parse the chat response content into a Python-native data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"cake_name\": \"Classic Vanilla Cake\",\n",
            "    \"description\": \"Light, airy, and naturally sweet vanilla flavor. A go-to recipe for any occasion.\",\n",
            "    \"difficulty_score\": 4,\n",
            "    \"expected_price\": 22.5\n",
            "  },\n",
            "  {\n",
            "    \"cake_name\": \"Red Velvet Cake\",\n",
            "    \"description\": \"Rich and decadent chocolate-flavored cake with a hint of cocoa and a cream cheese frosting, giving it a tangy flavor.\",\n",
            "    \"difficulty_score\": 6,\n",
            "    \"expected_price\": 30.0\n",
            "  },\n",
            "  {\n",
            "    \"cake_name\": \"Chocolate Fudge Cake\",\n",
            "    \"description\": \"Rich, moist, and extremely chocolatey with a fudgy texture, often paired with a creamy chocolate ganache.\",\n",
            "    \"difficulty_score\": 6,\n",
            "    \"expected_price\": 28.75\n",
            "  },\n",
            "  {\n",
            "    \"cake_name\": \"Lemon Drizzle Cake\",\n",
            "    \"description\": \"A zesty, citrusy, and refreshing cake with a slight tartness from lemon, drizzled with a sugary lemon glaze.\",\n",
            "    \"difficulty_score\": 5,\n",
            "    \"expected_price\": 25.0\n",
            "  },\n",
            "  {\n",
            "    \"cake_name\": \"Carrot Cake\",\n",
            "    \"description\": \"Moist and sweet with a hint of spice from cinnamon, crunch from walnuts, and a creamy cream cheese frosting.\",\n",
            "    \"difficulty_score\": 6,\n",
            "    \"expected_price\": 35.5\n",
            "  },\n",
            "  {\n",
            "    \"cake_name\": \"Victoria Sponge\",\n",
            "    \"description\": \"A classic British cake consisting of soft, light sponge filled and topped with fresh strawberry jam and whipped cream.\",\n",
            "    \"difficulty_score\": 3,\n",
            "    \"expected_price\": 20.0\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "json_response_content = json.loads(chat_response[\"choices\"][0][\"message\"][\"content\"])\n",
        "print(json.dumps(json_response_content, ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a id=\"summary\"></a>\n",
        "## Summary and next steps\n",
        "\n",
        "You successfully completed this notebook!\n",
        "\n",
        "You learned how to work with chat models using tools and watsonx.ai.\n",
        "\n",
        "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Author\n",
        "\n",
        "**Rafał Chrzanowski**, Software Engineer Intern at watsonx.ai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright © 2025 IBM. This notebook and its source code are released under the terms of the MIT License."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "watsonx-ai-samples-py-3-11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
