{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb9c52e5-f53a-46c6-b501-5d91916f3921"
   },
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# Use watsonx, watsonx.data, LangChain, and vector indexes to chat with a document (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d503b1b-95e3-4dcd-8ab2-3fb853f047d4"
   },
   "source": [
    "#### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in watsonx context.\n",
    "\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "This notebook demonstrates how to reproduce the behaviour of chat with a document and vector indexes programmatically through watsonx APIs and clients.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
    "\n",
    "## Learning goal\n",
    "\n",
    "The purpose of this notebook is to replicate chat with a documents behavior programmatically by integrating document and vector indexes with watsonx APIs and watsonx.data Milvus.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Build Vector Index](#build)\n",
    "  * [Define vector index properties](#define)\n",
    "  * [Initialize vector store](#initialize)\n",
    "  * [Data Asset Processing](#data_asset_processing)\n",
    "  * [Create embeddings](#embeddings)\n",
    "  * [Create vector index asset](#vector-index)  \n",
    "- [Deploy chat with document AI Service](#deploy)\n",
    "  * [Define AI Service function](#ai-service-function-definition)\n",
    "  * [Test AI Service locally](#test-ai-service-locally)\n",
    "  * [Create deployment](#create-deployment)\n",
    "- [Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03f0ac2d-4a13-4d1b-9be7-1e6973c311a8"
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Contact with your Cloud Pak for Data administrator and ask them for your account credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eebfecce-c006-4ab1-980e-939c2ccaf61a"
   },
   "source": [
    "### Install required packages\n",
    "\n",
    "Install the required libraries for this notebook, ensuring that in an airgap environment the local PyPI repository is populated with these dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcb45793-b5a8-4d94-a7bd-0c49376c55e7"
   },
   "outputs": [],
   "source": [
    "!pip install jq | tail -n 1\n",
    "!pip install docx2txt | tail -n 1\n",
    "!pip install tiktoken | tail -n 1\n",
    "!pip install python-pptx | tail -n 1\n",
    "!pip install unstructured | tail -n 1\n",
    "!pip install \"ibm-watsonx-ai>=1.2.5\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e866a4df-b266-46a8-8030-d390a149f2e3"
   },
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "78f8baa9-7102-40ec-a645-10e0ef78b20d"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "from ibm_watsonx_ai.foundation_models import Embeddings\n",
    "from ibm_watsonx_ai.foundation_models.extensions.rag.chunker import LangChainChunker\n",
    "from ibm_watsonx_ai.foundation_models.extensions.rag.vector_stores import VectorStore\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.document_loaders import (\n",
    "  PyPDFLoader,\n",
    "  CSVLoader,\n",
    "  Docx2txtLoader,\n",
    "  TextLoader,\n",
    "  UnstructuredExcelLoader,\n",
    "  UnstructuredPowerPointLoader,\n",
    "  UnstructuredMarkdownLoader,\n",
    "  JSONLoader,\n",
    "  UnstructuredHTMLLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eb8f2c0-2f41-4fc8-8f63-a7baf8e4b3f4"
   },
   "source": [
    "### Connection to WML\n",
    "\n",
    "Authenticate the Watson Machine Learning service on IBM Cloud Pak for Data. By default, `url`, `username` and `token` will be taken from the set of environment variables. You can overwrite them by providing `url`, `username` and `api_key` as arguments.\n",
    "\n",
    "Initialize the client to work with the watsonx API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "74710eff-fa6c-4eb2-8d3b-880fa5444c32"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "\n",
    "credentials = Credentials(\n",
    "    instance_id=\"openshift\",\n",
    "    version=\"5.1\"\n",
    ")\n",
    "\n",
    "client = APIClient(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f401cb0-f194-4dbb-bfcc-58ad427359e7"
   },
   "source": [
    "Alternatively you can use `username` and `password` to authenticate WML services.\n",
    "\n",
    "```python\n",
    "credentials = Credentials(\n",
    "    username=***,\n",
    "    password=***,\n",
    "    url=***,\n",
    "    instance_id=\"openshift\",\n",
    "    version=\"5.1\"\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9982ce23-3641-4f2f-a5a8-5808357dad01"
   },
   "source": [
    "### Working with projects\n",
    "\n",
    "First of all, you need to create a project that will be used for your work. If you do not have project already created follow below steps.\n",
    "\n",
    "- Open IBM Cloud Pak main page\n",
    "- Click all projects\n",
    "- Create an empty project\n",
    "- Copy `project_id` from url and paste it below\n",
    "\n",
    "**Action**: Assign project ID below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c8499dc5-be3a-4ad5-963e-0efbad71d894"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e5099c5-d43d-4265-9844-d7fd2cf7c773"
   },
   "source": [
    "To be able to interact with all resources available in WML services, you need to set the **project** which you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cc9e26b5-6575-4dee-bac6-be8efbb1cc6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set.default_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c87ebd48-a95f-4bc4-8648-3fb90519050e"
   },
   "source": [
    "<a id=\"build\"></a>\n",
    "\n",
    "## Build Vector Index\n",
    "The following code demonstrates how to generate embeddings from a list of data assets within a Milvus collection and save them as a vector index asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e0f2a56-34c5-4435-8d65-bf1caf3b91a1"
   },
   "source": [
    "<a id=\"define\"></a>\n",
    "\n",
    "### Define vector index\n",
    "\n",
    "This section allows you to configure the properties for creating the vector index.\n",
    "\n",
    "Retrieve a comprehensive list of data assets and connections to facilitate the definition of the vector index properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "35273add-541d-49d0-9189-9d4a2fb31a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data assets:\n",
      "                 NAME  ASSET_TYPE   SIZE                              ASSET_ID\n",
      "0  ModelInference.txt  data_asset  13584  56a9251d-419d-44f4-9676-55137166ba1f\n",
      "\n",
      "\n",
      "Connections:\n",
      "                NAME                                    ID               CREATED                    DATASOURCE_TYPE_ID\n",
      "0  Milvus Connection  c03acccb-03be-4c01-98c3-6b07a53dc043  2025-03-14T07:59:41Z  4484a69b-2c6d-4cec-bfed-fb93332a820b\n"
     ]
    }
   ],
   "source": [
    "print(\"Data assets:\")\n",
    "print(client.data_assets.list().to_string())\n",
    "\n",
    "print(\"\\n\\nConnections:\")\n",
    "print(client.connections.list().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55700146-294c-4286-84d9-8450a1d443a9"
   },
   "source": [
    "Specify the data assets to be incorporated into the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "065d94a4-236a-4708-bee7-09be6cbdc712"
   },
   "outputs": [],
   "source": [
    "data_assets = [\n",
    "    \"ENTER YOUR DATA ASSETS ID HERE\" \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad3e5abe-7342-4011-b839-559bc6d27166"
   },
   "source": [
    "Configure the properties of the watsonx.data Milvus instance and collection by specifying the details of the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "47fa8c6b-8f86-4890-bb2d-52c28539f616"
   },
   "outputs": [],
   "source": [
    "connection_id = \"ENTER YOUR CONNECTION ID HERE\"\n",
    "collection = \"wx_collection\"\n",
    "database = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebc5600b-03b6-4eb1-8c76-3b1e5eb00d78"
   },
   "source": [
    "Configure the vector index settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e93782c-2723-489e-b0de-42996439ad5f"
   },
   "outputs": [],
   "source": [
    "# ID of the embedding model\n",
    "embedding_model_id = \"ibm/granite-embedding-278m-multilingual\"\n",
    "\n",
    "# Additional vector index settings\n",
    "vector_index_name = \"My vector index\"\n",
    "top_k = 5\n",
    "rerank = False\n",
    "chunk_size = 2000\n",
    "chunk_overlap = 200\n",
    "split_pdf_pages = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79ac202c-9f11-4f4e-befe-4210613cf3d4"
   },
   "source": [
    "Utilizing the settings defined above, we construct the payload for the vector index, consolidating all configuration parameters into a structured format for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "def68e4b-c632-4ae6-8894-08105b98b3b4"
   },
   "outputs": [],
   "source": [
    "vector_index_details = {\n",
    "    \"name\": vector_index_name,\n",
    "\t\"data_assets\": data_assets,\n",
    "\t\"store\": {\n",
    "\t\t\"type\": \"watsonx.data\",\n",
    "\t\t\"connection_id\": connection_id,\n",
    "\t\t\"index\": collection,\n",
    "\t\t\"database\": database\n",
    "\t},\n",
    "\t\"settings\": {\n",
    "\t\t\"chunk_size\": chunk_size,\n",
    "\t\t\"chunk_overlap\": chunk_overlap,\n",
    "\t\t\"split_pdf_pages\": split_pdf_pages,\n",
    "\t\t\"top_k\": top_k,\n",
    "\t\t\"rerank\": rerank,\n",
    "\t\t\"embedding_model_id\": embedding_model_id,\n",
    "\t\t\"schema_fields\": {\n",
    "\t\t\t\"document_name\": \"document_name\",\n",
    "\t\t\t\"text\": \"text\",\n",
    "\t\t\t\"page_number\": \"page\"\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"status\": \"ready\"\n",
    "}\n",
    "\n",
    "# The following schema delineates the documents being created within the Milvus collection.\n",
    "vector_store_schema = vector_index_details[\"settings\"][\"schema_fields\"]\n",
    "text_field = vector_store_schema.get(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "302d0d34-db23-41fc-ad24-1457865e8747"
   },
   "source": [
    "<a id=\"initialize\"></a>\n",
    "\n",
    "### Initialize vector store\n",
    "\n",
    "Instantiate the `VectorStore` class for the watsonx.data Milvus collection. By default, `VectorStore` will create a collection with the nama as is stated in `collection` variable. If collection with that name already exist, no new collection is created and running method `VectorStore.add_documents` the new documents will be added to the existing collection. To drop old collection and create a new one with the same name, please set `drop_old` param to `True` in `VectorStore` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "872ccd20-9e16-4e12-a1ae-479794993f8f"
   },
   "outputs": [],
   "source": [
    "embeddings = Embeddings(\n",
    "    model_id=embedding_model_id,\n",
    "\tapi_client=client,\n",
    "    params={\n",
    "        \"truncate_input_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "vector_store = VectorStore(\n",
    "    api_client=client,\n",
    "    connection_id=connection_id,\n",
    "    embeddings=embeddings,\n",
    "    index_name=collection,\n",
    "    database=database,\n",
    "    consistency_level='Strong',\n",
    "    connection_args={'secure': True},\n",
    "    text_field=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0c2893e-cdd9-4c4f-92e1-e857c1934344"
   },
   "source": [
    "<a id=\"data_asset_processing\"></a>\n",
    "\n",
    "### Data Asset Processing\n",
    "\n",
    "The following cells manage the parsing of data assets, their conversion into LangChain documents, and the generation of embeddings in the Milvus collection.\n",
    "\n",
    "Define the document loader to be used for each file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d1662139-eece-4694-9741-46b0507c06da"
   },
   "outputs": [],
   "source": [
    "mime_type_mappings = {\n",
    "    'text/plain': TextLoader,\n",
    "    'application/pdf': PyPDFLoader,\n",
    "    'text/csv': CSVLoader,\n",
    "    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': Docx2txtLoader,\n",
    "    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': UnstructuredExcelLoader,\n",
    "    'application/vnd.openxmlformats-officedocument.presentationml.presentation': UnstructuredPowerPointLoader,\n",
    "    'text/markdown': UnstructuredMarkdownLoader,\n",
    "    'application/json': JSONLoader,\n",
    "    'text/html': UnstructuredHTMLLoader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1af7c160-fc9b-4a3b-88a4-099f41b3082f"
   },
   "source": [
    "Initialize the text splitter used to partition documents into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bb0ebd67-82e1-4085-bb79-e9cbdd659794"
   },
   "outputs": [],
   "source": [
    "text_splitter = LangChainChunker(\n",
    "    method=\"recursive\",\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29445d54-6431-4fa7-b40c-9d21a8c90dd1"
   },
   "source": [
    "Define a function to parse the raw content of a data asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cc6d1b2d-1708-4f4e-9c51-852f54d1e147"
   },
   "outputs": [],
   "source": [
    "def load_document(data_asset: dict) -> list:\n",
    "    \"\"\"\n",
    "    Load and split the document from a data asset.\n",
    "\n",
    "    Args:\n",
    "        data_asset (dict): A dictionary containing metadata and entity details of the data asset.\n",
    "\n",
    "    Returns:\n",
    "        list: The loaded and split content of the document.\n",
    "    \"\"\"\n",
    "    asset_id = data_asset[\"metadata\"][\"asset_id\"]\n",
    "    filename = data_asset[\"metadata\"][\"name\"]\n",
    "    file_path = client.data_assets.download(asset_id, filename)\n",
    "    mime_type = data_asset[\"entity\"][\"data_asset\"][\"mime_type\"]\n",
    "    \n",
    "    # Get the correct loader for the MIME type.\n",
    "    if mime_type == \"application/json\":\n",
    "        loader = mime_type_mappings[mime_type](filename, jq_schema='.', text_content=False)\n",
    "    else:\n",
    "        loader = mime_type_mappings[mime_type](file_path)\n",
    "    return loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb2f575b-e8f8-435d-95d1-6e01172652ac"
   },
   "source": [
    "Define a function to incorporate additional metadata into a LangChain document, such as the document name and an optional page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8de5ff27-80fd-459f-9ac4-8de35dd08b24"
   },
   "outputs": [],
   "source": [
    "def compute_documents_metadata(document_name: str, loaded_documents: list) -> list:\n",
    "    \"\"\"\n",
    "    Compute and update metadata for a list of loaded documents.\n",
    "\n",
    "    Args:\n",
    "        document_name (str): The name to be assigned to each document.\n",
    "        loaded_documents (list): A list of documents from which metadata is extracted and updated.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of documents with enriched metadata.\n",
    "    \"\"\"\n",
    "    filtered_documents = []\n",
    "    for document in loaded_documents:\n",
    "        computed_document_data = {\n",
    "            \"metadata\": {\n",
    "                \"source\": document.model_dump()[\"metadata\"][\"source\"]\n",
    "            }\n",
    "        }\n",
    "        computed_document_data[\"metadata\"][vector_store_schema.get(\"page_number\")] = document.model_dump()[\"metadata\"].get(\"page\", 0)\n",
    "        computed_document_data[\"metadata\"][vector_store_schema.get(\"document_name\")] = document_name\n",
    "        filtered_documents.append(document.model_copy(update=computed_document_data))\n",
    "        \n",
    "    return filtered_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa34b1dd-b1de-4332-b7e7-9009f2840942"
   },
   "source": [
    "Define a function to extract and segment document chunks from an individual data asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2fd01d08-04e2-4952-9f2e-8f99a019377b"
   },
   "outputs": [],
   "source": [
    "def process_document(data_asset: dict) -> list:\n",
    "    \"\"\"\n",
    "    Process a single data asset by loading, enriching, and splitting its content into document chunks.\n",
    "\n",
    "    Args:\n",
    "        data_asset (dict): A dictionary representing the data asset, which contains metadata and other details.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of document chunks obtained after splitting the enriched document.\n",
    "    \"\"\"\n",
    "    print(\"Processing\", data_asset[\"metadata\"][\"name\"])\n",
    "    loaded_documents = load_document(data_asset)\n",
    "    filtered_documents = compute_documents_metadata(data_asset[\"metadata\"][\"name\"], loaded_documents)\n",
    "\n",
    "    return text_splitter.split_documents(filtered_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cf4d6d0-6214-4e84-bc95-220fd0baadb2"
   },
   "source": [
    "Define a function that retrieves document chunks for all available data assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "29a189e6-77f3-4764-a504-86225020a72f"
   },
   "outputs": [],
   "source": [
    "def process_documents(data_asset_ids: list) -> list:\n",
    "    \"\"\"\n",
    "    Process and retrieve documents from the given list of data asset IDs.\n",
    "\n",
    "    Parameters:\n",
    "        data_asset_ids (list): A list of data asset identifiers to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed documents aggregated from the provided data assets.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for data_asset_id in data_asset_ids:\n",
    "        data_asset = client.data_assets.get_details(data_asset_id)\n",
    "        \n",
    "        if data_asset[\"metadata\"][\"asset_type\"] == \"data_asset\":\n",
    "            document = process_document(data_asset)\n",
    "            documents += document\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1125d959-f543-47ca-848a-76c8e2c7d63a"
   },
   "source": [
    "Process the data assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ced4e9d5-afc1-49c7-a5e8-73d1b9b52f97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ModelInference.txt\n",
      "Successfully saved data asset content to file: 'ModelInference.txt'\n"
     ]
    }
   ],
   "source": [
    "documents = process_documents(data_assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a492f505-d515-4366-b0f3-4b5f15b6c8ce"
   },
   "source": [
    "<a id=\"embeddings\"></a>\n",
    "\n",
    "### Create embeddings\n",
    "\n",
    "Generate embeddings for the document chunks and add them into the Milvus collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0dc074e8-536a-41ca-be33-e5765fa4c04b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cfa02bfb834b9c7bfca4d98924e6ebe92f28eaddb702edadced88b9d67ee494a',\n",
       " '2e4d36157aec823e169750b3ae45bdccbff6df4f370b36862b22377a76d5cbe9',\n",
       " '6feed52fc3d6cef08f9d05d9777f3fec0a7966a2005da04cb6ae7167108e5ac5',\n",
       " '26d08eb4cb54eed7e9af2a3cf30fa0c2c9e84364f5e504134245e2678b37cd60',\n",
       " '222c93a51ca5aecf662c803c8181e337e4d1249d1d4976a2660baaef30d2e4af',\n",
       " '6d65582150b5f3080b4afa369d80a8c30984a665f7cca2de75899cb759705f73',\n",
       " '4b487731188fa2803dba310b350c1c79ac4067a106769b760cadbbc4fab367a3',\n",
       " '74e6dd153be95b2db6e021b71a108f573d702b3e3a22dc91adb3cd39dcf34248']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(content=documents, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dc0675a-90fa-473d-84ef-c015374035f5"
   },
   "source": [
    "<a id=\"vector-index\"></a>\n",
    "\n",
    "### Create vector index\n",
    "\n",
    "Invoke the vector indexes API to create a vector index asset that points to this Milvus collection.\n",
    "\n",
    "*Note*: This vector index does not require patching when updates are made to the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a64e469d-aa09-4fae-ac76-1a70b7454a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '3ebf3222-8a3b-4d86-9a8a-1c2f37370adb', 'name': 'My vector index', 'created_at': 1742205546906, 'created_by': '1000331001', 'last_updated_at': 1742205546906, 'last_updated_by': '1000331001', 'data_assets': ['56a9251d-419d-44f4-9676-55137166ba1f'], 'store': {'type': 'watsonx.data', 'connection_id': 'c03acccb-03be-4c01-98c3-6b07a53dc043', 'index': 'wx_collection', 'database': 'default'}, 'settings': {'chunk_size': 2000, 'chunk_overlap': 200, 'split_pdf_pages': True, 'top_k': 5, 'rerank': False, 'embedding_model_id': 'ibm/granite-embedding-278m-multilingual', 'schema_fields': {'document_name': 'document_name', 'text': 'text', 'page_number': 'page'}}, 'status': 'ready'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "vector_index_api_url = f'{credentials.url}/wx/v1/vector_indexes?project_id={project_id}'\n",
    "\n",
    "response = requests.post(\n",
    "    vector_index_api_url, \n",
    "    headers=client._get_headers(), \n",
    "    json=vector_index_details\n",
    ")\n",
    "\n",
    "vector_index = response.json()\n",
    "print(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1efb19b-f744-4b57-9836-003c71d7a509"
   },
   "source": [
    "<a id=\"deploy\"></a>\n",
    "\n",
    "## Deploy AI Service\n",
    "\n",
    "Below is a step-by-step example demonstrating how to deploy a prompt that interacts with a document using vector indexes.\n",
    "\n",
    "*Note*: This feature is available only for models supported by the Chat AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e827d633-a70a-404c-9153-48dae7529a50"
   },
   "source": [
    "You can use the `list` method to display all existing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaf7de29-4ce3-432f-87f5-d8a7e0593e42"
   },
   "outputs": [],
   "source": [
    "client.spaces.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a765de0-274a-4bbe-8723-5c6678369f3d"
   },
   "source": [
    "Define the space for deploying the AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6109a38c-635e-4152-ab7f-be0bfba242c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsetting the project_id ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_id = 'ENTER YOUR SPACE ID HERE'\n",
    "\n",
    "client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "509f9577-cf5a-44f6-b17a-c0e3127515b1"
   },
   "source": [
    "Promote the vector index to the designated space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d6e04ec9-2cd6-4f27-b856-1197b083ecd9"
   },
   "outputs": [],
   "source": [
    "vector_index_id = client.spaces.promote(vector_index.get(\"id\"), project_id, space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cafb4c8-f401-4207-a220-e40949bd4bad"
   },
   "source": [
    "<a id=\"ai-service-function-definition\"></a>\n",
    "\n",
    "### AI Service function definition\n",
    "Define the AI service function to handle data retrieval and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "581ce1a7-0d05-442c-a4d2-313ff4087216"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"space_id\": space_id, \n",
    "    \"vector_index_id\": vector_index_id,\n",
    "    \"url\": credentials.url,\n",
    "}\n",
    "\n",
    "def gen_ai_service(context, params=params, **custom):\n",
    "    # import dependencies\n",
    "    from ibm_watsonx_ai.client import APIClient, Credentials\n",
    "    from ibm_watsonx_ai.foundation_models.extensions.rag import Retriever\n",
    "    from ibm_watsonx_ai.foundation_models.extensions.rag.vector_stores import VectorStore\n",
    "    from ibm_watsonx_ai.foundation_models import ModelInference, Embeddings, Rerank\n",
    "\n",
    "    vector_index_id = params.get(\"vector_index_id\")\n",
    "    space_id = params.get(\"space_id\")\n",
    "    url = params.get(\"url\")\n",
    "\n",
    "    # Inference details\n",
    "    inference_model_id = params.get(\"inference_model_id\")\n",
    "    system_prompt = \"You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. Furthermore, make sure that the response is supported by the given document or context. Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer. Avoid repeating information unless asked.\"\n",
    "    inference_params = {\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    # Setup client\n",
    "    credentials = Credentials(\n",
    "        url=url,\n",
    "        token=context.generate_token(),\n",
    "        instance_id=\"openshift\"\n",
    "    )\n",
    "\n",
    "    client = APIClient(credentials, space_id=space_id)\n",
    "\n",
    "    # Get vector index details\n",
    "    vector_index_details = client.data_assets.get_details(vector_index_id)\n",
    "    vector_index_properties = vector_index_details[\"entity\"][\"vector_index\"]\n",
    "\n",
    "    def rerank(inner_client, documents, query, top_n):\n",
    "        \"\"\"\n",
    "        Rerank a list of documents based on a query using a cross-encoder model.\n",
    "\n",
    "        Parameters:\n",
    "            inner_client: An API client instance used to interact with the underlying service.\n",
    "            documents (list): A list of documents to be reranked.\n",
    "            query (str): The query string used to evaluate the relevance of each document.\n",
    "            top_n (int): The number of top documents to return after reranking.\n",
    "\n",
    "        Returns:\n",
    "            list: A new list of documents ordered by their relevance to the query.\n",
    "        \"\"\"\n",
    "        reranker = Rerank(\n",
    "            model_id=\"cross-encoder/ms-marco-minilm-l-12-v2\",\n",
    "            api_client=inner_client,\n",
    "            params={\n",
    "                \"return_options\": {\n",
    "                    \"top_n\": top_n\n",
    "                },\n",
    "                \"truncate_input_tokens\": 512\n",
    "            }\n",
    "        )\n",
    "\n",
    "        reranked_results = reranker.generate(query=query, inputs=documents)[\"results\"]\n",
    "\n",
    "        new_documents = [documents[result[\"index\"]] for result in reranked_results]\n",
    "            \n",
    "        return new_documents        \n",
    "\n",
    "    def format_messages(messages, documents, system_prompt):\n",
    "        \"\"\"\n",
    "        Format conversation messages by appending contextual information and prepending a system prompt.\n",
    "\n",
    "        Parameters:\n",
    "            messages (list): A list of message dictionaries, where each dictionary must include a \"content\" key.\n",
    "            documents (list): A list of document strings that will be combined to form context.\n",
    "            system_prompt (str): The system prompt to be inserted as the first message in the conversation.\n",
    "\n",
    "        Returns:\n",
    "            list: The updated list of messages including the reformatted last message and the prepended system message.\n",
    "        \"\"\"\n",
    "        context = \"\\n\".join(documents)\n",
    "\n",
    "        # Append context to the last message.\n",
    "        if messages:\n",
    "            content = messages[-1].get(\"content\", \"\")\n",
    "            # Format of this string may be model dependent\n",
    "            messages[-1][\"content\"] = (\n",
    "                f\"Use the following pieces of context to answer the question.\\n\\n\"\n",
    "                f\"{context}\\n\\n\"\n",
    "                f\"Question: {content}\\n\"\n",
    "            )\n",
    "        \n",
    "        # Prepend the system prompt.\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "        \n",
    "        return messages\n",
    "\n",
    "    def inference_model(inference_model_id, inner_client, messages, stream):\n",
    "        \"\"\"\n",
    "        Retrieve document chunks, incorporate contextual information, and generate a grounded response.\n",
    "\n",
    "        Parameters:\n",
    "            inference_model_id: ID of model that will be used for inferencing\n",
    "            inner_client: An API client instance used for connecting to the vector store and the inference model.\n",
    "            messages (list): A list of message dictionaries representing the conversation history. The content of\n",
    "                            the last message is used as the query for document retrieval.\n",
    "            stream (bool): If True, the inference model returns a streaming response; otherwise, it returns a complete response.\n",
    "\n",
    "        Returns:\n",
    "            The generated response from the inference model, either as a stream or as a complete message.\n",
    "        \"\"\"\n",
    "        emb = Embeddings(\n",
    "            model_id=vector_index_properties[\"settings\"][\"embedding_model_id\"],\n",
    "            api_client=inner_client,\n",
    "            params={\n",
    "                \"truncate_input_tokens\": 512\n",
    "            }\n",
    "        )\n",
    "\n",
    "        top_n = 20 if vector_index_properties[\"settings\"].get(\"rerank\") else int(vector_index_properties[\"settings\"][\"top_k\"])\n",
    "\n",
    "        vector_store = VectorStore(\n",
    "            client=inner_client,\n",
    "            connection_id=vector_index_properties[\"store\"][\"connection_id\"],\n",
    "            embeddings=emb,\n",
    "            index_name=vector_index_properties[\"store\"][\"index\"],\n",
    "            database=vector_index_properties[\"store\"][\"database\"],\n",
    "            consistency_level='Strong',\n",
    "            connection_args={'secure': True},\n",
    "            text_field=vector_index_properties[\"settings\"][\"schema_fields\"][\"text\"],\n",
    "            search_params={\"ef\": 2 * top_n} # `ef` param needs to be larger than `top_n` param\n",
    "        )\n",
    "\n",
    "        # Retrieve document chunks from the vector index\n",
    "        query = messages[-1].get(\"content\")\n",
    "\n",
    "        retriever = Retriever(vector_store=vector_store, number_of_chunks=top_n)\n",
    "        documents = retriever.retrieve(query)\n",
    "\n",
    "        def get_doc_content(doc):\n",
    "            \"\"\"\n",
    "            Extract the content from a document.\n",
    "\n",
    "            Parameters:\n",
    "                doc: A document object with a 'page_content' attribute.\n",
    "\n",
    "            Returns:\n",
    "                str: The textual content of the document.\n",
    "            \"\"\"\n",
    "            return doc.page_content\n",
    "\n",
    "        document_contents = list(map(get_doc_content, documents))\n",
    "\n",
    "        # Use reranking if enabled\n",
    "        if vector_index_properties[\"settings\"].get(\"rerank\"):\n",
    "            document_contents = rerank(inner_client, document_contents, query, vector_index_properties[\"settings\"][\"top_k\"])\n",
    "\n",
    "        # Generate grounded response using the inference details\n",
    "        messages = format_messages(messages, document_contents, system_prompt=system_prompt)\n",
    "\n",
    "        model = ModelInference(\n",
    "            model_id=inference_model_id,\n",
    "            params=inference_params,\n",
    "            api_client=inner_client,\n",
    "            space_id=space_id\n",
    "        )\n",
    "\n",
    "        if stream == True:\n",
    "            generated_response = model.chat_stream(messages=messages)\n",
    "        else:\n",
    "            generated_response = model.chat(messages=messages)\n",
    "\n",
    "        return generated_response\n",
    "\n",
    "    def get_inner_client(context):\n",
    "        \"\"\"\n",
    "        Set up and return an inner API client using the provided context.\n",
    "\n",
    "        Parameters:\n",
    "            context: An object used in AI services deployment runtime\n",
    "\n",
    "        Returns:\n",
    "            APIClient: An instance of APIClient configured with the constructed credentials and space ID.\n",
    "        \"\"\"\n",
    "        inner_credentials = Credentials(\n",
    "            url = url,\n",
    "            token = context.get_token(),\n",
    "            instance_id = \"openshift\"\n",
    "        )\n",
    "        inner_client = APIClient(inner_credentials, space_id = space_id)\n",
    "        return inner_client\n",
    "\n",
    "    def generate(context):\n",
    "        payload = context.get_json()\n",
    "        messages = payload.get(\"messages\")\n",
    "        inference_model_id = payload.get(\"model_id\")\n",
    "        inner_client = get_inner_client(context)    \n",
    "        \n",
    "        results = inference_model(inference_model_id, inner_client, messages, False)\n",
    "        \n",
    "        response = {\n",
    "            \"headers\": {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            \"body\": results\n",
    "        }\n",
    "\n",
    "        return response\n",
    "\n",
    "    def generate_stream(context):\n",
    "        payload = context.get_json()\n",
    "        messages = payload.get(\"messages\")\n",
    "        inference_model_id = payload.get(\"model_id\")\n",
    "        inner_client = get_inner_client(context)\n",
    "        response_stream = inference_model(inference_model_id, inner_client, messages, True)\n",
    "\n",
    "        for chunk in response_stream:\n",
    "            yield chunk\n",
    "\n",
    "    return generate, generate_stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f93a2329-ce03-4e7d-8c88-cd9860d4413a"
   },
   "source": [
    "Define the request and response schemas for the AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ba6ca4c-c51e-4838-817e-20136a535b04"
   },
   "outputs": [],
   "source": [
    "request_schema = {\n",
    "    \"application/json\": {\n",
    "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"model_id\": {\n",
    "                \"title\": \"The model to use for the inference.\",\n",
    "                \"type\": \"str\"\n",
    "            },\n",
    "            \"messages\": {\n",
    "                \"title\": \"The messages for this chat session.\",\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"role\": {\n",
    "                            \"title\": \"The role of the message author.\",\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"user\",\"assistant\"]\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"title\": \"The contents of the message.\",\n",
    "                            \"type\": \"string\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"role\",\"content\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"model_id\", \"messages\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "response_schema = {\n",
    "    \"application/json\": {\n",
    "        \"oneOf\": [\n",
    "            {\n",
    "                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"AI Service response for /ai_service_stream\",\n",
    "                \"properties\": {\n",
    "                    \"choices\": {\n",
    "                        \"description\": \"A list of chat completion choices.\",\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"index\": {\n",
    "                                    \"type\": \"integer\",\n",
    "                                    \"title\": \"The index of this result.\"\n",
    "                                },\n",
    "                                \"delta\": {\n",
    "                                    \"description\": \"A message result.\",\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"content\": {\n",
    "                                            \"description\": \"The contents of the message.\",\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"role\": {\n",
    "                                            \"description\": \"The role of the author of this message.\",\n",
    "                                            \"type\": \"string\"\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"required\": [\n",
    "                                        \"role\"\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"choices\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"AI Service response for /ai_service\",\n",
    "                \"properties\": {\n",
    "                    \"choices\": {\n",
    "                        \"description\": \"A list of chat completion choices\",\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"index\": {\n",
    "                                    \"type\": \"integer\",\n",
    "                                    \"description\": \"The index of this result.\"\n",
    "                                },\n",
    "                                \"message\": {\n",
    "                                    \"description\": \"A message result.\",\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"role\": {\n",
    "                                            \"description\": \"The role of the author of this message.\",\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"content\": {\n",
    "                                            \"title\": \"Message content.\",\n",
    "                                            \"type\": \"string\"\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"required\": [\n",
    "                                        \"role\"\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"choices\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c147a4b-58a8-4246-b4ad-7d0b1d352a99"
   },
   "source": [
    "<a id=\"test-ai-service-locally\"></a>\n",
    "\n",
    "### Test AI Service locally\n",
    "Before creating deployment, we can test locally the prepared AI service to detect potential errors at an early stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "da3d524f-7809-445b-ba37-af51797872d4"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.deployments import RuntimeContext\n",
    "\n",
    "context = RuntimeContext(api_client=client)\n",
    "\n",
    "streaming = False\n",
    "findex = 1 if streaming else 0\n",
    "local_function = gen_ai_service(context)[findex]\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f6340b0-704c-4b81-a1ed-46228f7edc45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'headers': {'Content-Type': 'application/json'}, 'body': {'id': 'chatcmpl-58a5ae1f-1238-4759-9d80-4d63a20bcb57', 'model_id': 'ibm/granite-3-8b-instruct', 'model': 'ibm/granite-3-8b-instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The document describes the ModelInference class in the ibm_watsonx_ai.foundation_models.inference module. This class is used to interact with models or deployments for text generation tasks. It provides methods such as generate, generate_text, get_details, and get_identifying_params.\\n\\nThe generate method takes a prompt and parameters as input and returns a completion text. The generate_text method is similar but also allows for raw response and guardrails options. The guardrails option enables detection of potentially hateful, abusive, and/or profane language (HAP) in both the prompt and generated text.\\n\\nThe get_details method returns details about the model or deployment, and the get_identifying_params method represents the setup of Model Inference in a dictionary.\\n\\nThe class can be initialized with various parameters including model_id, deployment_id, params, credentials, project_id, space_id, verify, api_client, and persistent_connection. The persistent_connection parameter, when set to True, allows for maintaining a persistent connection.\\n\\nThe class also includes a close_persistent_connection method, which is only applicable if persistent_connection was set to True during initialization.\\n\\nThe document also provides examples of how to use the ModelInference class with both models and deployments, and how to handle HAP detection warnings.\\n\\nNote: The method is not supported for deployments, available only for base models.'}, 'finish_reason': 'stop'}], 'created': 1742205695, 'model_version': '1.0.0', 'created_at': '2025-03-17T10:01:39.839Z', 'usage': {'completion_tokens': 304, 'prompt_tokens': 2185, 'total_tokens': 2489}}}\n"
     ]
    }
   ],
   "source": [
    "inference_model_id = \"ibm/granite-3-8b-instruct\"\n",
    "\n",
    "local_question = \"Summarize the document\"\n",
    "\n",
    "messages.append({ \"role\" : \"user\", \"content\": local_question })\n",
    "\n",
    "context = RuntimeContext(api_client=client, request_payload_json={\"model_id\": inference_model_id, \"messages\": messages})\n",
    "\n",
    "response = local_function(context)\n",
    "\n",
    "if streaming:\n",
    "    for chunk in response:\n",
    "        print(chunk, end=\"\\n\\n\", flush=True)\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4261a355-c44f-40ec-97d1-4058139e69d4"
   },
   "source": [
    "<a id=\"create-deployment\"></a>\n",
    "\n",
    "### Create deployment\n",
    "After making sure that AI service works as expected, we can proceed to the deployment creation step. \n",
    "\n",
    "Retrieve the software specification used by the AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ec053a46-39e5-4503-89df-94f599cbba5e"
   },
   "outputs": [],
   "source": [
    "software_spec_id = client.software_specifications.get_id_by_name(\"runtime-24.1-py3.11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15129d93-a0fb-4bb8-9dd5-9b9be633fbf4"
   },
   "source": [
    "Create the AI service asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8acd7074-349b-4aec-827d-75b6250adf36"
   },
   "outputs": [],
   "source": [
    "ai_service_metadata = {\n",
    "    client.repository.AIServiceMetaNames.NAME: vector_index_name,\n",
    "    client.repository.AIServiceMetaNames.DESCRIPTION: \"\",\n",
    "    client.repository.AIServiceMetaNames.SOFTWARE_SPEC_ID: software_spec_id,\n",
    "    client.repository.AIServiceMetaNames.CUSTOM: {},\n",
    "    client.repository.AIServiceMetaNames.REQUEST_DOCUMENTATION: request_schema,\n",
    "    client.repository.AIServiceMetaNames.RESPONSE_DOCUMENTATION: response_schema\n",
    "}\n",
    "\n",
    "ai_service_details = client.repository.store_ai_service(meta_props=ai_service_metadata, ai_service=gen_ai_service)\n",
    "ai_service_id = client.repository.get_ai_service_id(ai_service_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ff66239-8bbb-4a11-831b-d65ce092d487"
   },
   "source": [
    "Deploy the AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "b5372f7b-31b6-44f7-8d51-091066f41865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "######################################################################################\n",
      "\n",
      "Synchronous deployment creation for id: '62a04437-70f2-459a-b52b-8d6325765d99' started\n",
      "\n",
      "######################################################################################\n",
      "\n",
      "\n",
      "initializing\n",
      "Note: online_url is deprecated and will be removed in a future release. Use serving_urls instead.\n",
      ".......\n",
      "ready\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Successfully finished deployment creation, deployment_id='0b15cf4e-47f5-4541-aa18-60409e43b0bd'\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deployment_metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: vector_index_name,\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "    client.deployments.ConfigurationMetaNames.CUSTOM: {},\n",
    "    client.deployments.ConfigurationMetaNames.DESCRIPTION: f\"{vector_index_name} description\"\n",
    "}\n",
    "\n",
    "function_deployment_details = client.deployments.create(ai_service_id, meta_props=deployment_metadata, space_id=space_id)\n",
    "deployment_id = client.deployments.get_id(function_deployment_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c172273-1d9d-4744-8fae-832cbeb767f9"
   },
   "source": [
    "Evaluate the deployment of the AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35ad8828-3ca6-4ec3-a0ca-dfe331350af7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'The document describes the ModelInference class in the ibm_watsonx_ai.foundation_models.inference module. This class is used to interact with models or deployments for text generation tasks. It provides methods such as generate, generate_text, get_details, and get_identifying_params.\\n\\nThe generate method takes a prompt and parameters as input and returns a completion text. The generate_text method is similar but also allows for raw response and guardrails options. The guardrails option enables detection of potentially hateful, abusive, and/or profane language (HAP) in both the prompt and generated text.\\n\\nThe get_details method returns details about the model or deployment, and the get_identifying_params method represents the setup of Model Inference in a dictionary.\\n\\nThe class can be initialized with various parameters including model_id, deployment_id, params, credentials, project_id, space_id, verify, api_client, and persistent_connection. The persistent_connection parameter, when set to True, allows for maintaining a persistent connection.\\n\\nThe class also includes a close_persistent_connection method, which is only applicable if persistent_connection was set to True during initialization.\\n\\nThe document also provides examples of how to use the ModelInference class with both models and deployments, and how to handle HAP detection warnings.\\n\\nNote: The method is not supported for deployments, available only for base models.', 'role': 'assistant'}}], 'created': 1742205774, 'created_at': '2025-03-17T10:02:58.603Z', 'id': 'chatcmpl-4ae8b401-53b2-4caf-b559-7e33c822826b', 'model': 'ibm/granite-3-8b-instruct', 'model_id': 'ibm/granite-3-8b-instruct', 'model_version': '1.0.0', 'usage': {'completion_tokens': 304, 'prompt_tokens': 2185, 'total_tokens': 2489}}\n"
     ]
    }
   ],
   "source": [
    "inference_model_id = \"ibm/granite-3-8b-instruct\"\n",
    "\n",
    "remote_question = \"Summarize the document\"\n",
    "payload = {\"model_id\": inference_model_id, \"messages\": [{\"role\": \"user\", \"content\": remote_question}]}\n",
    "\n",
    "result = client.deployments.run_ai_service(deployment_id, payload)\n",
    "\n",
    "if \"error\" in result:\n",
    "    print(result[\"error\"])\n",
    "else:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd6affe2-fe84-4494-9ae6-80d1587f8be6"
   },
   "outputs": [],
   "source": [
    "stream_results = client.deployments.run_ai_service_stream(deployment_id, payload)\n",
    "\n",
    "for chunk in stream_results:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c26e99cc-c88d-42ce-8fbf-205101c801db"
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary and next steps\n",
    "\n",
    "You successfully completed this notebook!\n",
    "\n",
    "You learned how to reproduce the behaviour of chat with a document and vector indexes programmatically through watsonx APIs and clients.\n",
    " \n",
    "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2025 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
