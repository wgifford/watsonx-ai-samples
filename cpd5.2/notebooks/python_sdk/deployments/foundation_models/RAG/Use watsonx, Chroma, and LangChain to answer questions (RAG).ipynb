{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "![image](https://raw.githubusercontent.com/IBM/watsonx-ai-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
                "# Use watsonx, Chroma, and LangChain to answer questions (RAG)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "#### Disclaimers\n",
                "\n",
                "- Use only Projects and Spaces that are available in watsonx context.\n",
                "\n",
                "## Notebook content\n",
                "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
                "\n",
                "Some familiarity with Python is helpful. This notebook uses Python 3.12.\n",
                "\n",
                "### About Retrieval Augmented Generation\n",
                "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
                "\n",
                "In its simplest form, RAG requires 3 steps:\n",
                "\n",
                "- Index knowledge base passages (once)\n",
                "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
                "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
                "\n",
                "## Contents\n",
                "\n",
                "This notebook contains the following parts:\n",
                "\n",
                "- [Setup](#setup)\n",
                "- [Document data loading](#data)\n",
                "- [Build up knowledge base](#build_base)\n",
                "- [Foundation Models on watsonx](#models)\n",
                "- [Generate a retrieval-augmented response to a question](#predict)\n",
                "- [Summary and next steps](#summary)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"setup\"></a>\n",
                "## Set up the environment\n",
                "\n",
                "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
                "\n",
                "-  Contact with your Cloud Pak for Data administrator and ask them for your account credentials\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Install dependencies\n",
                "**Note:** `ibm-watsonx-ai` documentation can be found <a href=\"https://ibm.github.io/watsonx-ai-python-sdk/index.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.4.26 charset-normalizer-3.4.2 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.25 langchain-core-0.3.63 langchain-text-splitters-0.3.8 langsmith-0.3.43 orjson-3.10.18 packaging-24.2 pydantic-2.11.5 pydantic-core-2.33.2 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-extensions-4.13.2 typing-inspection-0.4.1 urllib3-2.4.0 zstandard-0.23.0\n",
                        "\u001b[1A\u001b[2KSuccessfully installed ibm-cos-sdk-2.14.1 ibm-cos-sdk-core-2.14.1 ibm-cos-sdk-s3transfer-2.14.1 ibm-watsonx-ai-1.3.23 jmespath-1.0.1 langchain_ibm-0.3.11 lomond-0.3.3 numpy-2.2.6 pandas-2.2.3 pytz-2025.2 requests-2.32.2 tabulate-0.9.0 tzdata-2025.2\n",
                        "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.4 aiosignal-1.3.2 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.6.0 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 multidict-6.4.4 mypy-extensions-1.1.0 propcache-0.3.1 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0 yarl-1.20.0\n",
                        "\u001b[1A\u001b[2KSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 chromadb-1.0.11 click-8.2.1 coloredlogs-15.0.1 deprecated-1.2.18 distro-1.9.0 durationpy-0.10 fastapi-0.115.9 filelock-3.18.0 flatbuffers-25.2.10 fsspec-2025.5.1 google-auth-2.40.2 googleapis-common-protos-1.70.0 grpcio-1.71.0 hf-xet-1.1.2 httptools-0.6.4 huggingface-hub-0.32.3 humanfriendly-10.0 importlib-metadata-8.6.1 importlib-resources-6.5.2 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 kubernetes-32.0.1 langchain_chroma-0.2.4 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.1.0 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.22.0 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-grpc-1.33.1 opentelemetry-instrumentation-0.54b1 opentelemetry-instrumentation-asgi-0.54b1 opentelemetry-instrumentation-fastapi-0.54b1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 opentelemetry-util-http-0.54b1 overrides-7.7.0 posthog-4.2.0 protobuf-5.29.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 referencing-0.36.2 requests-oauthlib-2.0.0 rich-14.0.0 rpds-py-0.25.1 rsa-4.9.1 shellingham-1.5.4 starlette-0.45.3 sympy-1.14.0 tokenizers-0.21.1 tqdm-4.67.1 typer-0.16.0 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5 websocket-client-1.8.0 websockets-15.0.1 wrapt-1.17.2 zipp-3.22.0\n",
                        "Successfully installed wget-3.2\n",
                        "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Pillow-11.2.1 jinja2-3.1.6 joblib-1.5.1 networkx-3.5 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.3 sentence-transformers-4.1.0 setuptools-80.9.0 threadpoolctl-3.6.0 torch-2.7.0 transformers-4.52.4\n"
                    ]
                }
            ],
            "source": [
                "%pip install -U \"langchain>=0.3,<0.4\" | tail -n 1\n",
                "%pip install -U \"langchain_ibm>=0.3,<0.4\" | tail -n 1\n",
                "%pip install -U \"langchain_community>=0.3,<0.4\" | tail -n 1\n",
                "%pip install -U \"langchain_chroma>=0.2,<0.3\" | tail -n 1\n",
                "%pip install -U wget | tail -n 1\n",
                "%pip install -U sentence-transformers | tail -n 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "#### Define credentials\n",
                "\n",
                "Authenticate the watsonx.ai Runtime service on IBM Cloud Pak for Data. You need to provide the **admin's** `username` and the platform `url`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "username = \"PASTE YOUR USERNAME HERE\"\n",
                "url = \"PASTE THE PLATFORM URL HERE\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use the **admin's** `api_key` to authenticate watsonx.ai Runtime services:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import getpass\n",
                "from ibm_watsonx_ai import Credentials\n",
                "\n",
                "credentials = Credentials(\n",
                "    username=username,\n",
                "    api_key=getpass.getpass(\"Enter your watsonx.ai API key and hit enter: \"),\n",
                "    url=url,\n",
                "    instance_id=\"openshift\",\n",
                "    version=\"5.2\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Alternatively you can use the **admin's** `password`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import getpass\n",
                "from ibm_watsonx_ai import Credentials\n",
                "\n",
                "if \"credentials\" not in locals() or not credentials.api_key:\n",
                "    credentials = Credentials(\n",
                "        username=username,\n",
                "        password=getpass.getpass(\"Enter your watsonx.ai password and hit enter: \"),\n",
                "        url=url,\n",
                "        instance_id=\"openshift\",\n",
                "        version=\"5.2\",\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Working with projects\n",
                "\n",
                "First of all, you need to create a project that will be used for your work. If you do not have a project created already, follow the steps below:\n",
                "\n",
                "- Open IBM Cloud Pak main page\n",
                "- Click all projects\n",
                "- Create an empty project\n",
                "- Copy `project_id` from url and paste it below\n",
                "\n",
                "**Action**: Assign project ID below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "try:\n",
                "    project_id = os.environ[\"PROJECT_ID\"]\n",
                "except KeyError:\n",
                "    project_id = input(\"Please enter your project_id (hit enter): \")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Create `APIClient` instance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai import APIClient\n",
                "\n",
                "client = APIClient(credentials, project_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"data\"></a>\n",
                "## Document data loading\n",
                "\n",
                "Download the file with State of the Union."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import wget\n",
                "\n",
                "filename = \"state_of_the_union.txt\"\n",
                "url = \"https://raw.github.com/IBM/watsonx-ai-samples/master/cpd5.2/data/foundation_models/state_of_the_union.txt\"\n",
                "\n",
                "if not os.path.isfile(filename):\n",
                "    wget.download(url, out=filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"build_base\"></a>\n",
                "## Build up knowledge base\n",
                "\n",
                "The current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
                "\n",
                "In this basic example, we take the State of the Union speech content (filename), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "from langchain.document_loaders import TextLoader\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "from langchain_chroma import Chroma\n",
                "\n",
                "loader = TextLoader(filename)\n",
                "documents = loader.load()\n",
                "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
                "texts = text_splitter.split_documents(documents)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "The dataset we are using is already split into self-contained passages that can be ingested by Chroma."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Specify embedding model\n",
                "\n",
                "This notebook uses embedding model `ibm/slate-125m-english-rtrvr`, which has to be available on your Cloud Pak for Data environment for this notebook to run successfully.  \n",
                "You can list available embedding models by running the cell below.\n",
                "\n",
                "**Note**: You can feed a custom embedding function to be used by `chromadb`. The performance of `chromadb` may differ depending on the embedding model used. In following example we use watsonx.ai Embedding service."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ibm/slate-125m-english-rtrvr\n"
                    ]
                }
            ],
            "source": [
                "if len(client.foundation_models.EmbeddingModels):\n",
                "    print(*client.foundation_models.EmbeddingModels, sep=\"\\n\")\n",
                "else:\n",
                "    print(\n",
                "        \"Embedding models are missing in this environment. Install embedding models to proceed.\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.foundation_models import Embeddings\n",
                "\n",
                "embeddings = Embeddings(\n",
                "    model_id=client.foundation_models.EmbeddingModels.SLATE_125M_ENGLISH_RTRVR,\n",
                "    credentials=credentials,\n",
                "    project_id=project_id,\n",
                ")\n",
                "docsearch = Chroma.from_documents(texts, embeddings)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Compatibility watsonx.ai Embeddings with LangChain\n",
                "\n",
                " LangChain retrievals use `embed_documents` and `embed_query` under the hood to generate embedding vectors for uploaded documents and user query respectively. watsonx.ai python sdk `Embeddings` class has these methods implemented."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Help on class Embeddings in module ibm_watsonx_ai.foundation_models.embeddings.embeddings:\n",
                        "\n",
                        "class Embeddings(ibm_watsonx_ai.foundation_models.embeddings.base_embeddings.BaseEmbeddings, ibm_watsonx_ai.wml_resource.WMLResource)\n",
                        " |  Embeddings(*, model_id: 'str', params: 'ParamsType | None' = None, credentials: 'Credentials | dict[str, str] | None' = None, project_id: 'str | None' = None, space_id: 'str | None' = None, api_client: 'APIClient | None' = None, verify: 'bool | str | None' = None, persistent_connection: 'bool' = True, batch_size: 'int' = 1000, concurrency_limit: 'int' = 5, max_retries: 'int | None' = None, delay_time: 'float | None' = None, retry_status_codes: 'list[int] | None' = None) -> 'None'\n",
                        " |\n",
                        " |  Instantiate the embeddings service.\n",
                        " |\n",
                        " |  :param model_id: the type of model to use\n",
                        " |  :type model_id: str, optional\n",
                        " |\n",
                        " |  :param params: parameters to use during generate requests, use ``ibm_watsonx_ai.metanames.EmbedTextParamsMetaNames().show()`` to view the list of MetaNames\n",
                        " |  :type params: dict, optional\n",
                        " |\n",
                        " |  :param credentials: credentials for the Watson Machine Learning instance\n",
                        " |  :type credentials: dict, optional\n",
                        " |\n",
                        " |  :param project_id: ID of the Watson Studio project\n",
                        " |  :type project_id: str, optional\n",
                        " |\n",
                        " |  :param space_id: ID of the Watson Studio space\n",
                        " |  :type space_id: str, optional\n",
                        " |\n",
                        " |  :param api_client: initialized APIClient object with a set project ID or space ID. If passed, ``credentials`` and ``project_id``/``space_id`` are not required.\n",
                        " |  :type api_client: APIClient, optional\n",
                        " |\n",
                        " |  :param verify: You can pass one of following as verify:\n",
                        " |\n",
                        " |      * the path to a CA_BUNDLE file\n",
                        " |      * the path of a directory with certificates of trusted CAs\n",
                        " |      * `True` - default path to truststore will be taken\n",
                        " |      * `False` - no verification will be made\n",
                        " |  :type verify: bool or str, optional\n",
                        " |\n",
                        " |  :param persistent_connection: defines whether to keep a persistent connection when evaluating the `generate`, 'embed_query', and 'embed_documents` methods with one prompt\n",
                        " |                                or batch of prompts that meet the length limit. For more details, see `Generate embeddings <https://cloud.ibm.com/apidocs/watsonx-ai#text-embeddings>`_.\n",
                        " |                                To close the connection, run `embeddings.close_persistent_connection()`, defaults to True. Added in 1.1.2.\n",
                        " |  :type persistent_connection: bool, optional\n",
                        " |\n",
                        " |  :param batch_size: Number of elements to be embedded sending in one call (used only for sync methods), defaults to 1000\n",
                        " |  :type batch_size: int, optional\n",
                        " |\n",
                        " |  :param concurrency_limit: number of requests to be sent in parallel when generating embedding vectors (used only for sync methods), max is 10, defaults to 5\n",
                        " |  :type concurrency_limit: int, optional\n",
                        " |\n",
                        " |  :param max_retries: number of retries performed when request was not successful and status code is in retry_status_codes, defaults to 10\n",
                        " |  :type max_retries: int, optional\n",
                        " |\n",
                        " |  :param delay_time: delay time to retry request, factor in exponential backoff formula: wx_delay_time * pow(2.0, attempt), defaults to 0.5s\n",
                        " |  :type delay_time: float, optional\n",
                        " |\n",
                        " |  :param retry_status_codes: list of status codes which will be considered for retry mechanism, defaults to [429, 503, 504, 520]\n",
                        " |  :type retry_status_codes: list[int], optional\n",
                        " |\n",
                        " |\n",
                        " |  .. note::\n",
                        " |      When the ``credentials`` parameter is passed, one of these parameters is required: [``project_id``, ``space_id``].\n",
                        " |\n",
                        " |  .. hint::\n",
                        " |      You can copy the project_id from the Project's Manage tab (Project -> Manage -> General -> Details).\n",
                        " |\n",
                        " |  **Example:**\n",
                        " |\n",
                        " |  .. code-block:: python\n",
                        " |\n",
                        " |      from ibm_watsonx_ai import Credentials\n",
                        " |      from ibm_watsonx_ai.foundation_models import Embeddings\n",
                        " |      from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
                        " |      from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
                        " |\n",
                        " |     embed_params = {\n",
                        " |          EmbedParams.TRUNCATE_INPUT_TOKENS: 3,\n",
                        " |          EmbedParams.RETURN_OPTIONS: {\n",
                        " |          'input_text': True\n",
                        " |          }\n",
                        " |      }\n",
                        " |\n",
                        " |      embedding = Embeddings(\n",
                        " |          model_id=EmbeddingTypes.IBM_SLATE_30M_ENG,\n",
                        " |          params=embed_params,\n",
                        " |          credentials=Credentials(\n",
                        " |              api_key = IAM_API_KEY,\n",
                        " |              url = \"https://us-south.ml.cloud.ibm.com\"),\n",
                        " |          project_id=\"*****\"\n",
                        " |          )\n",
                        " |\n",
                        " |  Method resolution order:\n",
                        " |      Embeddings\n",
                        " |      ibm_watsonx_ai.foundation_models.embeddings.base_embeddings.BaseEmbeddings\n",
                        " |      abc.ABC\n",
                        " |      ibm_watsonx_ai.wml_resource.WMLResource\n",
                        " |      builtins.object\n",
                        " |\n",
                        " |  Methods defined here:\n",
                        " |\n",
                        " |  __init__(self, *, model_id: 'str', params: 'ParamsType | None' = None, credentials: 'Credentials | dict[str, str] | None' = None, project_id: 'str | None' = None, space_id: 'str | None' = None, api_client: 'APIClient | None' = None, verify: 'bool | str | None' = None, persistent_connection: 'bool' = True, batch_size: 'int' = 1000, concurrency_limit: 'int' = 5, max_retries: 'int | None' = None, delay_time: 'float | None' = None, retry_status_codes: 'list[int] | None' = None) -> 'None'\n",
                        " |      Initialize self.  See help(type(self)) for accurate signature.\n",
                        " |\n",
                        " |  async aembed_documents(self, texts: 'list[str]', params: 'ParamsType | None' = None) -> 'list[list[float]]'\n",
                        " |      Returns list of embedding vectors for provided texts in an asynchronous manner.\n",
                        " |\n",
                        " |      :param texts: list of texts for which embedding vectors will be generated, max length is determined by API (for more information, please refer to the documentation: https://cloud.ibm.com/apidocs/watsonx-ai#text-embeddings)\n",
                        " |      :type texts: list[str]\n",
                        " |      :param params: MetaProps for the embedding generation, use ``ibm_watsonx_ai.metanames.EmbedTextParamsMetaNames().show()`` to view the list of MetaNames, defaults to None\n",
                        " |      :type params: ParamsType | None, optional\n",
                        " |\n",
                        " |      :return: list of embedding vectors\n",
                        " |      :rtype: list[list[float]]\n",
                        " |\n",
                        " |      **Example:**\n",
                        " |\n",
                        " |      .. code-block:: python\n",
                        " |\n",
                        " |          q = [\n",
                        " |              \"What is a Generative AI?\",\n",
                        " |              \"Generative AI refers to a type of artificial intelligence that can original content.\"\n",
                        " |              ]\n",
                        " |\n",
                        " |          embedding_vectors = await embedding.aembed_documents(texts=q)\n",
                        " |          print(embedding_vectors)\n",
                        " |\n",
                        " |  async aembed_query(self, text: 'str', params: 'ParamsType | None' = None) -> 'list[float]'\n",
                        " |      Returns an embedding vector for a provided text in an asynchronous manner.\n",
                        " |\n",
                        " |      :param text: text for which embedding vector will be generated\n",
                        " |      :type text: str\n",
                        " |      :param params: MetaProps for the embedding generation, use ``ibm_watsonx_ai.metanames.EmbedTextParamsMetaNames().show()`` to view the list of MetaNames, defaults to None\n",
                        " |      :type params: ParamsType | None, optional\n",
                        " |      :return: embedding vector\n",
                        " |      :rtype: list[float]\n",
                        " |\n",
                        " |      **Example:**\n",
                        " |\n",
                        " |      .. code-block:: python\n",
                        " |\n",
                        " |          q = \"What is a Generative AI?\"\n",
                        " |          embedding_vector = await embedding.aembed_query(text=q)\n",
                        " |          print(embedding_vector)\n",
                        " |\n",
                        " |  async agenerate(self, inputs: 'list[str]', params: 'ParamsType | None' = None) -> 'dict'\n",
                        " |      Generate embeddings vectors for the given input with the given\n",
                        " |      parameters in an asynchronous manner. Returns a REST API response.\n",
                        " |\n",
                        " |      :param inputs: list of texts for which embedding vectors will be generated, max length is determined by API (for more information, please refer to the documentation: https://cloud.ibm.com/apidocs/watsonx-ai#text-embeddings)\n",
                        " |      :type inputs: list[str]\n",
                        " |      :param params: MetaProps for the embedding generation, use ``ibm_watsonx_ai.metanames.EmbedTextParamsMetaNames().show()`` to view the list of MetaNames, defaults to None\n",
                        " |      :type params: ParamsType | None, optional\n",
                        " |\n",
                        " |      :return: scoring results containing generated embeddings vectors\n",
                        " |      :rtype: dict\n",
                        " |\n",
                        " |  close_persistent_connection(self) -> 'None'\n",
                        " |      Only applicable if persistent_connection was set to True in Embeddings initialization.\n",
                        " |      Calling this method closes the current `httpx.Client` and recreates a new `httpx.Client` with default values:\n",
                        " |      timeout: httpx.Timeout(read=30 * 60, write=30 * 60, connect=10, pool=30 * 60)\n",
                        " |      limit: httpx.Limits(max_connections=10, max_keepalive_connections=10, keepalive_expiry=HTTPX_KEEPALIVE_EXPIRY)\n",
                        " |\n",
                        " |  embed_documents(self, texts: 'list[str]', params: 'ParamsType | None' = None, concurrency_limit: 'int' = 5) -> 'list[list[float]]'\n",
                        " |      Returns list of embedding vectors for provided texts.\n",
                        " |\n",
                        " |      :param texts: list of texts for which embedding vectors will be generated\n",
                        " |      :type texts: list[str]\n",
                        " |      :param params: MetaProps for the embedding generation, use ``ibm_watsonx_ai.metanames.EmbedTextParamsMetaNames().show()`` to view the list of MetaNames, defaults to None\n",
                        " |      :type params: ParamsType | None, optional\n",
                        " |      :param concurrency_limit: number of requests to be sent in parallel, max is 10, defaults to 5\n",
                        " |      :type concurrency_limit: int, optional\n",
                        " |\n",
                        " |      :return: list of embedding vectors\n",
                        " |      :rtype: list[list[float]]\n",
                        " |\n",
                        " |      **Example:**\n",
                        " |\n",
                        " |      .. code-block:: python\n",
                        " |\n",
                        " |          q = [\n",
                        " |              \"What is a Generative AI?\",\n",
                        " |              \"Generative AI refers to a type of artificial intelligence that can original content.\"\n",
                        " |              ]\n",
                        " |\n",
                        " |          embedding_vectors = embedding.embed_documents(texts=q)\n",
                        " |          print(embedding_vectors)\n",
                        " |\n",
                        " |  embed_query(self, text: 'str', params: 'ParamsType | None' = None) -> 'list[float]'\n",
                        " |      Returns an embedding vector for a provided text.\n",
                        " |\n",
                        " |      :param text: text for which embedding vector will be generated\n",
                        " |      :type text: str\n",
                        " |      :param params: MetaProps for the embedding generation, use ``ibm_watsonx_ai.metanames.EmbedTextParamsMetaNames().show()`` to view the list of MetaNames, defaults to None\n",
                        " |      :type params: ParamsType | None, optional\n",
                        " |      :return: embedding vector\n",
                        " |      :rtype: list[float]\n",
                        " |\n",
                        " |      **Example:**\n",
                        " |\n",
                        " |      .. code-block:: python\n",
                        " |\n",
                        " |          q = \"What is a Generative AI?\"\n",
                        " |          embedding_vector = embedding.embed_query(text=q)\n",
                        " |          print(embedding_vector)\n",
                        " |\n",
                        " |  generate(self, inputs: 'list[str]', params: 'ParamsType | None' = None, concurrency_limit: 'int' = 5) -> 'dict'\n",
                        " |      Generate embeddings vectors for the given input with the given\n",
                        " |      parameters. Returns a REST API response.\n",
                        " |\n",
                        " |      :param inputs: list of texts for which embedding vectors will be generated\n",
                        " |      :type inputs: list[str]\n",
                        " |      :param params: MetaProps for the embedding generation, use ``ibm_watsonx_ai.metanames.EmbedTextParamsMetaNames().show()`` to view the list of MetaNames, defaults to None\n",
                        " |      :type params: ParamsType | None, optional\n",
                        " |      :param concurrency_limit: number of requests to be sent in parallel, max is 10, defaults to 5\n",
                        " |      :type concurrency_limit: int, optional\n",
                        " |      :return: scoring results containing generated embeddings vectors\n",
                        " |      :rtype: dict\n",
                        " |\n",
                        " |  to_dict(self) -> 'dict'\n",
                        " |      Serialize Embeddings.\n",
                        " |\n",
                        " |      :return: serializes this Embeddings so that it can be reconstructed by ``from_dict`` class method.\n",
                        " |      :rtype: dict\n",
                        " |\n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Data and other attributes defined here:\n",
                        " |\n",
                        " |  __abstractmethods__ = frozenset()\n",
                        " |\n",
                        " |  __annotations__ = {}\n",
                        " |\n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Class methods inherited from ibm_watsonx_ai.foundation_models.embeddings.base_embeddings.BaseEmbeddings:\n",
                        " |\n",
                        " |  from_dict(data: 'dict', **kwargs: 'Any') -> 'BaseEmbeddings | None'\n",
                        " |      Deserialize ``BaseEmbeddings`` into a concrete one using arguments.\n",
                        " |\n",
                        " |      :return: concrete Embeddings or None if data is incorrect\n",
                        " |      :rtype: BaseEmbeddings | None\n",
                        " |\n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Data descriptors inherited from ibm_watsonx_ai.foundation_models.embeddings.base_embeddings.BaseEmbeddings:\n",
                        " |\n",
                        " |  __dict__\n",
                        " |      dictionary for instance variables\n",
                        " |\n",
                        " |  __weakref__\n",
                        " |      list of weak references to the object\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "help(Embeddings)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"models\"></a>\n",
                "## Foundation Models on `watsonx.ai`\n",
                "\n",
                "IBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/ibm_watsonx\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "#### Specify text model\n",
                "\n",
                "This notebook uses text model `ibm/granite-3-2b-instruct`, which has to be available on your Cloud Pak for Data environment for this notebook to run successfully.  \n",
                "You can list available text models by running the cell below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ibm/granite-13b-instruct-v2\n",
                        "ibm/granite-3-2b-instruct\n"
                    ]
                }
            ],
            "source": [
                "if len(client.foundation_models.TextModels):\n",
                "    print(*client.foundation_models.TextModels, sep=\"\\n\")\n",
                "else:\n",
                "    print(\n",
                "        \"Text models are missing in this environment. Install text models to proceed.\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_id = client.foundation_models.TextModels.GRANITE_3_2B_INSTRUCT"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Defining the model parameters\n",
                "We need to provide a set of model parameters that will influence the result:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
                "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
                "\n",
                "parameters = {\n",
                "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
                "    GenParams.MIN_NEW_TOKENS: 1,\n",
                "    GenParams.MAX_NEW_TOKENS: 100,\n",
                "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"],\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### LangChain CustomLLM wrapper for watsonx model\n",
                "Initialize the `WatsonxLLM` class from LangChain with defined parameters and `ibm/granite-3-2b-instruct`. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "from langchain_ibm import WatsonxLLM\n",
                "\n",
                "if credentials.get(\"apikey\"):\n",
                "    watsonx_granite = WatsonxLLM(\n",
                "        model_id=model_id.value,\n",
                "        url=credentials.get(\"url\"),\n",
                "        username=credentials.get(\"username\"),\n",
                "        apikey=credentials.get(\"apikey\"),\n",
                "        instance_id=credentials.get(\"instance_id\"),\n",
                "        project_id=project_id,\n",
                "        params=parameters,\n",
                "    )\n",
                "else:\n",
                "    watsonx_granite = WatsonxLLM(\n",
                "        model_id=model_id.value,\n",
                "        url=credentials.get(\"url\"),\n",
                "        username=credentials.get(\"username\"),\n",
                "        password=credentials.get(\"password\"),\n",
                "        instance_id=credentials.get(\"instance_id\"),\n",
                "        project_id=project_id,\n",
                "        params=parameters,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"predict\"></a>\n",
                "## Generate a retrieval-augmented response to a question"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Build the `RetrievalQA` (question answering chain) to automate the RAG task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chains import RetrievalQA\n",
                "\n",
                "qa = RetrievalQA.from_chain_type(\n",
                "    llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever()\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Select questions\n",
                "\n",
                "Get questions from the previously loaded test dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
                            " 'result': \" The president nominated Circuit Court of Appeals Judge Ketanji Brown Jackson as a replacement for Justice Stephen Breyer on the United States Supreme Court. He described her as one of the nation's top legal minds and expressed confidence that she would continue Justice Breyer's legacy of excellence.\"}"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "query = \"What did the president say about Ketanji Brown Jackson\"\n",
                "qa.invoke(query)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"summary\"></a>\n",
                "## Summary and next steps\n",
                "\n",
                "You successfully completed this notebook!\n",
                " \n",
                "You learned how to answer question using RAG using watsonx and LangChain.\n",
                " \n",
                "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "Copyright © 2023-2025 IBM. This notebook and its source code are released under the terms of the MIT License."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "watsonx-ai-samples-py-312",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
