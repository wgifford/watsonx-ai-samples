{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "986ab08a-c4d0-4c1b-8f97-7e04c35bff4d",
   "cell_type": "markdown",
   "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Use AutoAI RAG with watsonx Text Extraction service",
   "metadata": {
    "id": "99f649dc-d43c-493c-9fa0-3ccaf04f8e9f"
   }
  },
  {
   "id": "928ab92e-fbd1-4380-8385-ea57f18eff43",
   "cell_type": "markdown",
   "source": "#### Disclaimers\n\n- Use only Projects and Spaces that are available in the watsonx context.\n\n\n## Notebook content\n\nThis notebook demonstrates how to process data using the IBM watsonx.ai Text Extraction service and use the result in an AutoAI RAG experiment.\nThe data used in this notebook is from the [Granite Code Models paper](https://arxiv.org/pdf/2405.04324).\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n\n## Learning goal\n\nThe learning goals of this notebook are:\n\n- Process data using the IBM watsonx.ai Text Extraction service\n- Create an AutoAI RAG job that will find the best RAG pattern based on processed data\n\n\n## Contents\n\nThis notebook contains the following parts:\n- [Set up the environment](#setup)\n- [Prepare data and connections for the Text Extraction service](#prepare-te)\n- [Process data using the Text Extraction service](#run-te)\n- [Prepare data and connections for the AutoAI RAG experiment](#prepare-autorag)\n- [Run the AutoAI RAG experiment](#run-autorag)\n- [Compare and test RAG Patterns](#comparision)\n- [Summary](#summary)",
   "metadata": {
    "id": "41dfc040-37ac-4843-8e47-154dde15fda3"
   }
  },
  {
   "id": "0215053b-043c-4491-b519-c0bf2158e619",
   "cell_type": "markdown",
   "source": "<a id=\"setup\"></a>\n# Set up the environment\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Contact your Cloud Pak for Data administrator and ask them for your account credentials",
   "metadata": {
    "id": "e2e60734-2fa7-4bd2-b091-81d029c5ed41"
   }
  },
  {
   "id": "bf2c4ec8-fee4-46ff-96d3-751ecb47418d",
   "cell_type": "markdown",
   "source": "### Install and import the required modules and dependencies",
   "metadata": {
    "id": "2c4a2638-87a7-459e-b8c2-729233f894b3"
   }
  },
  {
   "id": "375c0f64-6452-4399-9a83-6cd0c1fd1f97",
   "cell_type": "code",
   "source": "!pip install -U 'ibm-watsonx-ai[rag]>=1.3.13' | tail -n 1",
   "metadata": {
    "id": "80453737-02c1-46d8-be2c-384f449a773c"
   },
   "outputs": [
   ],
   "execution_count": 1
  },
  {
   "id": "0656c11b-f53e-49f2-bd31-7b5e34875837",
   "cell_type": "markdown",
   "source": "### Connect to WML\nAuthenticate the Watson Machine Learning service on IBM Cloud Pak for Data. You need to provide the platform `url`, your `username`, and your `api_key`.\n\n- `url` - url which points to your CPD instance.\n- `username` - username to your CPD instance.\n- `api_key` - api_key to your CPD instance.",
   "metadata": {
    "id": "d6d18b26-5ba6-4ad8-95f0-ae53015db946"
   }
  },
  {
   "id": "40e1162d-00e3-48e4-bc7d-67624b849ccc",
   "cell_type": "code",
   "source": "url = \"PASTE YOUR CPD INSTANCE URL HERE\"\napi_key = \"PASTE YOUR CPD INSTANCE API KEY HERE\"\nusername = \"PASTE YOUR CPD INSTANCE USERNAME HERE\"",
   "metadata": {
    "id": "dc47fe65-3f71-4cb9-8d7a-840b8579bd2f"
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "id": "2bec1d73-97a7-4c10-bd9e-632c1bb6496c",
   "cell_type": "code",
   "source": "from ibm_watsonx_ai import Credentials\n\ncredentials = Credentials(\n    username=username,\n    api_key=api_key,\n    url=url,\n    instance_id=\"openshift\",\n    version=\"5.2\"\n)",
   "metadata": {
    "id": "c9a6c94f-cde4-49dc-be30-33b779d56067"
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "id": "7fed2a8e-67e0-4f12-91a1-e3cfc6b5f58f",
   "cell_type": "markdown",
   "source": "Alternatively, you can use your username and password to authenticate WML services.\n```\ncredentials = Credentials(\n    username=***,\n    password=***,\n    url=***,\n    instance_id=\"openshift\",\n    version=\"5.2\"\n)\n```",
   "metadata": {
    "id": "f8712f98-dcfa-455c-901e-03c7583a375a"
   }
  },
  {
   "id": "3d1de052-6bcf-48f4-92bc-7604deaf4330",
   "cell_type": "markdown",
   "source": "Create an instance of APIClient with authentication details",
   "metadata": {
    "id": "825bdbe8-f090-4021-9273-ba243b46754c"
   }
  },
  {
   "id": "310b0043-e52b-4abc-a265-0f386e20d545",
   "cell_type": "code",
   "source": "from ibm_watsonx_ai import APIClient\n\nclient = APIClient(credentials)",
   "metadata": {
    "id": "b472da5c-2e21-4c4b-ae7b-61a85d5e6dd0"
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "id": "06727769-283c-4f94-b4ed-68ac1df87b8f",
   "cell_type": "markdown",
   "source": "### Working with spaces\n\nFirst, you need to create a space for your work. If you do not have a space already created, you can use `{PLATFORM_URL}/ml-runtime/spaces?context=icp4data` to create one.\n\n- Click **New Deployment Space**\n- Create an empty space\n- Go to the space `Settings` tab\n- Copy the `space_id` and paste it below\n\n`PLATFORM_URL` is the url which points to your CPD instance.\n\n**Tip**: You can also use SDK to prepare the space for your work. Find more information in the [Space Management sample notebook](https://github.ibm.com/WML/watsonx-ai-samples/blob/dev/cpd5.1/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n\n**Action**: Assign the space ID below",
   "metadata": {
    "id": "a49ad029-ec44-4279-8dd0-4d53611e76d6"
   }
  },
  {
   "id": "4b4952e1-da3c-4d6c-8ca3-c0e0b3062869",
   "cell_type": "code",
   "source": "space_id = 'PASTE YOUR SPACE ID HERE'",
   "metadata": {
    "id": "622aecab-caaf-4b28-a6fa-4d9f36226346"
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "id": "c4b062fe-4231-49f6-add6-6362cb5983ad",
   "cell_type": "markdown",
   "source": "To be able to interact with all resources available in Watson Machine Learning, set the space that you are using.",
   "metadata": {
    "id": "9afea1a8-bca8-4680-9da8-eda2e2186b01"
   }
  },
  {
   "id": "b13a852b-363b-4acf-b2aa-fd8c7061a554",
   "cell_type": "code",
   "source": "client.set.default_space(space_id)",
   "metadata": {
    "id": "7b50b1b3-0a0f-4a20-99c4-eb2faa13b324"
   },
   "outputs": [
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'SUCCESS'"
     },
     "metadata": {}
    }
   ],
   "execution_count": 6
  },
  {
   "id": "34328ec6-8e73-4333-9ccf-bd0129ef4df6",
   "cell_type": "markdown",
   "source": "### Create an instance of COS client",
   "metadata": {
    "id": "ef47f43b-f2dc-4f28-bdf4-d924c01ddcdc"
   }
  },
  {
   "id": "fc9386c2-4b2a-47d9-b834-657cd26282c6",
   "cell_type": "markdown",
   "source": "Connect to the Cloud Object Storage instance for the by using the `ibm_boto3` package (for detailed explanation how to do this see [IBM Cloud Object Storage connection](https://dataplatform.cloud.ibm.com/docs/content/wsj/manage-data/conn-cos.html?context=wx)).",
   "metadata": {
    "id": "8c4287c9-172f-40a1-b53e-a8124f6ebed3"
   }
  },
  {
   "id": "4bfc3907-f985-4590-ab38-53f96cd0fa3d",
   "cell_type": "markdown",
   "source": "**Action**: Assign COS credentials below",
   "metadata": {
    "id": "7574bc86-b3e6-43da-afad-3434a78fecc4"
   }
  },
  {
   "id": "2dab6fa2-85f5-48f9-a2ea-2c0e04eca8c3",
   "cell_type": "code",
   "source": "cos_bucket_name = \"PASTE YOUR COS BUCKET NAME HERE\"\nendpoint_url = \"PASTE YOUR COS BUCKET ENDPOINT HERE\"\naccess_key = \"PASTE YOUR COS BUCKET ACCESS KEY HERE\"\nsecret_access_key = \"PASTE YOU COS BUCKET SECRET ACCESS KEY HERE\"",
   "metadata": {
    "id": "2d286fbb-ae31-407d-bfcf-32b30b014e9b"
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "id": "e41acd88-19b7-4980-90df-388b845c666b",
   "cell_type": "markdown",
   "source": "Initialize the connection to the created and get the connection ID.",
   "metadata": {
    "id": "cfff7111-b2df-4cac-b207-8f9c4180b137"
   }
  },
  {
   "id": "290425c4-d49f-4798-88f6-0d38ac1f9981",
   "cell_type": "code",
   "source": "connection_details = client.connections.create(\n    {\n        \"datasource_type\": client.connections.get_datasource_type_uid_by_name(\n            \"bluemixcloudobjectstorage\"\n        ),\n        \"name\": \"Connection to COS for tests\",\n        \"properties\": {\n            \"bucket\": cos_bucket_name,\n            \"access_key\": access_key,\n            \"secret_key\": secret_access_key,\n            \"iam_url\": client.service_instance._href_definitions.get_iam_token_url(),\n            \"url\": endpoint_url,\n        },\n    }\n)\n\ncos_connection_id = client.connections.get_id(\n    connection_details\n)",
   "metadata": {
    "id": "e9853590-dfca-421f-af58-96b330db3e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Creating connections...\nSUCCESS\n"
    }
   ],
   "execution_count": 8
  },
  {
   "id": "5a32c840-2883-43fc-8c3c-8b3529168628",
   "cell_type": "markdown",
   "source": "<a id=\"prepare-te\"></a>\n## Prepare data and connections for the Text Extraction service\n\nThe document, from which we are going to extract text, is located in the IBM Cloud Object Storage (COS). In this notebook, we will use the [Granite Code Models paper](https://arxiv.org/pdf/2405.04324) as a source text document. The final results file, which will contain extracted text and necessary metadata, will be placed in the COS. So we will use the `ibm_watsonx_ai.helpers.DataConnection` and the `ibm_watsonx_ai.helpers.S3Location` class to create Python objects that will represent the references to the processed files. Reference to the final results will be used as an input for the AutoAI RAG experiment. ",
   "metadata": {
    "id": "69503c8b-69f2-4db4-8787-45fa1ab03e3b"
   }
  },
  {
   "id": "91844b81-2bc6-439b-924c-e2711d21aec3",
   "cell_type": "code",
   "source": "from ibm_watsonx_ai.helpers import DataConnection, S3Location\n\ndata_url = \"https://arxiv.org/pdf/2405.04324\"\n\ntext_extraction_input_filename = \"granite_code_models.pdf\"\ntext_extraction_result_filename = \"granite_code_models.md\"",
   "metadata": {
    "id": "cccb74a1-c687-44bc-bd35-0d1585ad30c8"
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "id": "8a95845b-2ee8-4ed1-88b2-92c332248d06",
   "cell_type": "markdown",
   "source": "Create an input connection.",
   "metadata": {
    "id": "0ef58fcb-5ec7-4478-a6b2-ffc57cfa75e4"
   }
  },
  {
   "id": "6239c0c1-65c0-48f8-acb7-a3e112c1acc8",
   "cell_type": "code",
   "source": "input_data_reference = DataConnection(\n    connection_asset_id=cos_connection_id,\n    location=S3Location(\n        bucket=cos_bucket_name,\n        path=text_extraction_input_filename,\n    )\n)\ninput_data_reference.set_client(client)",
   "metadata": {
    "id": "debb8457-e9a2-4fc6-b5f8-52a2b4c1ee05"
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "id": "88939d9a-522b-40a4-91bf-faf69f8fa6d8",
   "cell_type": "markdown",
   "source": "Download the document from the url and upload to the COS Bucket using created connection.",
   "metadata": {
    "id": "a975c03a-8e2d-4707-a4e9-8bd5b4f56685"
   }
  },
  {
   "id": "e68473ef-424c-4d30-a1af-5551c3355168",
   "cell_type": "code",
   "source": "import requests\nimport tempfile\n\nresponse = requests.get(data_url)\nwith tempfile.NamedTemporaryFile() as tmp:\n    tmp.write(response.content)\n    input_data_reference.write(tmp.name)",
   "metadata": {
    "id": "8ae40129-2285-41a4-8ab1-9a88b4205aac"
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "id": "f54eac12-7d72-4f4c-a0c4-5541433be97e",
   "cell_type": "markdown",
   "source": "Output file connection.",
   "metadata": {
    "id": "02dd46dc-10d3-4111-bcf2-a61f2ece9dfd"
   }
  },
  {
   "id": "e4fe36e3-4365-4a36-8c3c-289ef591be39",
   "cell_type": "code",
   "source": "result_data_reference = DataConnection(\n    connection_asset_id=cos_connection_id,\n    location=S3Location(\n        bucket=cos_bucket_name,\n        path=text_extraction_result_filename\n    )\n)\nresult_data_reference.set_client(client)",
   "metadata": {
    "id": "215b4a49-3c06-4001-95a7-f9ff66587d28"
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "id": "200df191-779f-406d-946f-a4c1c968543b",
   "cell_type": "markdown",
   "source": "<a id=\"run-te\"></a>\n## Process data using the Text Extraction service\n\nInitialize the Text Extraction service endpoint.\n",
   "metadata": {
    "id": "b384e743-c214-41ad-a122-e657b6783e60"
   }
  },
  {
   "id": "fb0f855f-a5aa-4eb4-b866-1b8e97623dc3",
   "cell_type": "code",
   "source": "from ibm_watsonx_ai.foundation_models.extractions import TextExtractionsV2\n\nextraction = TextExtractionsV2(api_client=client)",
   "metadata": {
    "id": "52c90c3a-4ed8-4ad8-853b-316578164b6b"
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "id": "3f63fb9b-ee90-4810-b220-dce80b5645c2",
   "cell_type": "markdown",
   "source": "Run a text extraction job for connections created in the previous step.\n",
   "metadata": {
    "id": "4a28a797-14c1-4c69-b3ce-b69f605c74c2"
   }
  },
  {
   "id": "5716472a-85aa-43c7-8fc3-4d3f59e3e2ef",
   "cell_type": "code",
   "source": "from ibm_watsonx_ai.metanames import TextExtractionsV2ParametersMetaNames\nfrom ibm_watsonx_ai.foundation_models.extractions import TextExtractionsV2ResultFormats\n\nresponse = extraction.run_job(\n    document_reference=input_data_reference,\n    results_reference=result_data_reference,\n    parameters={\n        TextExtractionsV2ParametersMetaNames.OCR_MODE: \"enabled\",\n        TextExtractionsV2ParametersMetaNames.LANGUAGES: [\"en\"]\n    },\n    result_formats=TextExtractionsV2ResultFormats.MARKDOWN,\n)\n\njob_id = response['metadata']['id']",
   "metadata": {
    "id": "767f52d6-9f8c-4e5d-83c3-a94ac51e7345"
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "id": "f093b49c-e9ec-4c2f-a493-e42c66180914",
   "cell_type": "markdown",
   "source": "Wait for the job to be complete.",
   "metadata": {
    "id": "1181143e-526f-4dad-8b60-fe4d174ec1f0"
   }
  },
  {
   "id": "cef0c0e8-cb4e-4bbd-94a1-7b9426ea0528",
   "cell_type": "code",
   "source": "import json\nimport time\n\nwhile True:\n    job_details = extraction.get_job_details(job_id)\n    status = job_details['entity']['results']['status']\n    \n    if status == \"completed\":\n        print(\"Job completed successfully, details: {}\".format(json.dumps(job_details, indent=2)))\n        break\n    \n    if status == \"failed\":\n        print(\"Job failed, details: {}. \\n Try to run job again.\".format(json.dumps(job_details, indent=2)))\n        break\n\n    time.sleep(10)",
   "metadata": {
    "id": "07a099f0-4eb1-41f2-8a82-869316eaddf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Job completed successfully, details: {\n  \"entity\": {\n    \"document_reference\": {\n      \"connection\": {\n        \"id\": \"9a6d9083-0657-41ec-a384-17327a90c3dd\"\n      },\n      \"location\": {\n        \"bucket\": \"cf809fbf-8020-4c69-8435-7220aa26c522\",\n        \"file_name\": \"granite_code_models.pdf\"\n      },\n      \"type\": \"connection_asset\"\n    },\n    \"parameters\": {\n      \"create_embedded_images\": \"disabled\",\n      \"languages\": [\n        \"en\"\n      ],\n      \"mode\": \"standard\",\n      \"ocr_mode\": \"enabled\",\n      \"output_dpi\": 72,\n      \"output_tokens_and_bbox\": true,\n      \"requested_outputs\": [\n        \"md\"\n      ]\n    },\n    \"results\": {\n      \"completed_at\": \"2025-05-15T10:15:35.666Z\",\n      \"number_pages_processed\": 28,\n      \"running_at\": \"2025-05-15T10:14:48.293Z\",\n      \"status\": \"completed\"\n    },\n    \"results_reference\": {\n      \"connection\": {\n        \"id\": \"9a6d9083-0657-41ec-a384-17327a90c3dd\"\n      },\n      \"location\": {\n        \"bucket\": \"cf809fbf-8020-4c69-8435-7220aa26c522\",\n        \"file_name\": \"granite_code_models.md\"\n      },\n      \"type\": \"connection_asset\"\n    }\n  },\n  \"metadata\": {\n    \"created_at\": \"2025-05-15T10:14:45.638Z\",\n    \"id\": \"d465b6b7-558e-45fc-88e7-018c7b446961\",\n    \"modified_at\": \"2025-05-15T10:15:35.879Z\",\n    \"space_id\": \"6f0b87a7-ae96-4467-b023-7fb5767d4798\"\n  }\n}\n"
    }
   ],
   "execution_count": 15
  },
  {
   "id": "36e545b5-01e6-472c-98cd-0b032da37714",
   "cell_type": "markdown",
   "source": "Get the text extraction result.",
   "metadata": {
    "id": "d79f7249-ed95-499a-aceb-611a47eb50e8"
   }
  },
  {
   "id": "55c549f9-7836-49ba-912a-fb720a99542e",
   "cell_type": "code",
   "source": "from IPython.display import display, Markdown\n\nresult_data_reference.download(filename=text_extraction_result_filename)\nwith open(text_extraction_result_filename, 'r', encoding='utf-8') as file:\n    # Display beginning of the result file\n    display(Markdown((file.read()[:3000])))",
   "metadata": {
    "id": "eb065c5b-03f4-42f3-b8d8-dcdfab6225b8"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## Granite Code Models: A Family of Open Foundation Models for Code Intelligence\n\nMayank Mishra⋆ Matt Stallone⋆ Gaoyuan Zhang⋆ Yikang Shen Aditya Prasad Adriana Meza Soria Michele Merler Parameswaran Selvam Saptha Surendran Shivdeep Singh Manish Sethi Xuan-Hong Dang Pengyuan Li Kun-Lung Wu Syed Zawad Andrew Coleman Matthew White Mark Lewis Raju Pavuluri Yan Koyfman Boris Lublinsky Maximilien de Bayser Ibrahim Abdelaziz Kinjal Basu Mayank Agarwal Yi Zhou Chris Johnson Aanchal Goyal Hima Patel Yousaf Shah Petros Zerfos Heiko Ludwig Asim Munawar Maxwell Crouse Pavan Kapanipathi Shweta Salaria Bob Calio Sophia Wen Seetharami Seelam Brian Belgodere Carlos Fonseca Amith Singhee Nirmit Desai David D. Cox Ruchir Puri† Rameswar Panda†\n\nIBM Research\n\n⋆Equal\n\nContribution\n\n†Corresponding Authors ruchir@us.ibm.com, rpanda@ibm.com\n\n## Abstract\n\nLarge Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being inte grated into software development environments to improve the produc tivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software devel opment workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile “all around” code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.\n\n https://github.com/ibm-granite/granite-code-models\n\n## 1 Introduction\n\nOver the last several decades, software has been woven into the fabric of every aspect of our society. As demand for software development surges, it is more critical than ever to increase software development productivity, and LLMs provide promising path for augmenting human programmers. Prominent enterprise use cases for LLMs in software development productivity include code generation, code explanation, code fixing, unit test and documentation generation, application modernization, vulnerability detection, code translation, and more.\n\nRecent years have seen rapid progress in LLM’s ability to generate and manipulate code, and a range of models with impressive coding a"
     },
     "metadata": {}
    }
   ],
   "execution_count": 16
  },
  {
   "id": "56d90e9c-f131-4e80-8a27-e03a41563eec",
   "cell_type": "markdown",
   "source": "<a id=\"prepare-autorag\"></a>\n## Prepare data and connections for the AutoAI RAG experiment",
   "metadata": {
    "id": "bd42cd80-1c6b-48f7-a877-1b76527ea17f"
   }
  },
  {
   "id": "274af1ea-c353-496a-8ed0-2aca92d52565",
   "cell_type": "markdown",
   "source": "Define a connection to COS Bucket and upload a `json` file to use for benchmarking. \n\nNote: `correct_answer_document_ids` must refer to the document processed by text extraction service, not the initial document.",
   "metadata": {
    "id": "62e48008-5698-431b-b482-38a0bfcbe042"
   }
  },
  {
   "id": "5cbb07c0-4a7b-4dba-8fa8-3701d844075b",
   "cell_type": "markdown",
   "source": "Benchmarking data.",
   "metadata": {
    "id": "863ae616-0275-4f86-bee7-34416fefc56b"
   }
  },
  {
   "id": "d40c67cc-962c-4e41-93da-8639212fea35",
   "cell_type": "code",
   "source": "benchmarking_data = [\n {\n    \"question\": \"What are the two main variants of Granite Code models?\",\n    \"correct_answer\": \"The two main variants are Granite Code Base and Granite Code Instruct.\",\n    \"correct_answer_document_ids\": [text_extraction_result_filename]\n },\n {\n    \"question\": \"What is the purpose of Granite Code Instruct models?\",\n    \"correct_answer\": \"Granite Code Instruct models are finetuned for instruction-following tasks using datasets like CommitPack, OASST, HelpSteer, and synthetic code instruction datasets, aiming to improve reasoning and instruction-following capabilities.\",\n    \"correct_answer_document_ids\": [text_extraction_result_filename]\n },\n {\n    \"question\": \"What is the licensing model for Granite Code models?\",\n    \"correct_answer\": \"Granite Code models are released under the Apache 2.0 license, ensuring permissive and enterprise-friendly usage.\",\n    \"correct_answer_document_ids\": [text_extraction_result_filename]\n },\n]",
   "metadata": {
    "id": "19bf7999-e632-4fb7-9fef-12482b9499af"
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "id": "ec901d42-9935-4dfa-8c87-aaba73361176",
   "cell_type": "markdown",
   "source": "Create a connection.",
   "metadata": {
    "id": "34ba5a5b-a184-4a5e-a8e1-552d05aa77a6"
   }
  },
  {
   "id": "535e9136-a5d7-42e3-95fb-a912589c7da0",
   "cell_type": "code",
   "source": "test_filename = \"granite_code_models_benchmark.json\"\n\ntest_data_reference = DataConnection(\n    connection_asset_id=cos_connection_id,\n    location=S3Location(bucket=cos_bucket_name, path=test_filename),\n)\ntest_data_reference.set_client(client)",
   "metadata": {
    "id": "985f0b68-c263-4a92-bf2d-988468933cbd"
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "id": "1d8828c1-1e32-401d-8f44-ef5d5655cd40",
   "cell_type": "markdown",
   "source": "Upload benchmarking data to COS Bucket.",
   "metadata": {
    "id": "0a6adc2d-66c0-4c40-a97f-a5c16102587e"
   }
  },
  {
   "id": "d3f95789-a5e4-44f3-b8be-eb7d430fb7bd",
   "cell_type": "code",
   "source": "with tempfile.NamedTemporaryFile(mode=\"w\") as tmp:\n    json.dump(benchmarking_data, tmp, indent=4)\n    tmp.flush()\n    tmp_path = tmp.name\n    test_data_reference.write(tmp_path)",
   "metadata": {
    "id": "e2902e45-c0d6-4131-ac78-3bd745c8e8ea"
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "id": "95880097-3786-46b1-8908-9eccc9384326",
   "cell_type": "markdown",
   "source": "Define the connections to the test data and input data for the AutoAI RAG experiment, using the output of the Text Extraction job as the input connection.",
   "metadata": {
    "id": "ec5848e2-c5f7-4696-a308-6075f4910893"
   }
  },
  {
   "id": "12649982-9ce5-4c3d-9f23-1137b172ce77",
   "cell_type": "code",
   "source": "input_data_references = [result_data_reference]\ntest_data_references = [test_data_reference]",
   "metadata": {
    "id": "54b0cf73-0f8e-4be4-bd1b-de044c72e263"
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "id": "b856cbfe-75c8-418c-9d9c-cf78d7d13072",
   "cell_type": "markdown",
   "source": "<a id=\"run-autorag\"></a>\n# Run the AutoAI RAG experiment\n\nProvide the input information for AutoAI RAG optimizer:\n- `name` - experiment name\n- `description` - experiment description\n- `max_number_of_rag_patterns` - maximum number of RAG patterns to create\n- `optimization_metrics` - target optimization metrics",
   "metadata": {
    "id": "27c5d4de-398d-4ef2-b24b-c1614f2d1977"
   }
  },
  {
   "id": "2980d4ef-0f85-4be7-acd5-b5b6975afcde",
   "cell_type": "code",
   "source": "from ibm_watsonx_ai.experiment import AutoAI\n\nexperiment = AutoAI(credentials, space_id=space_id)\n\nrag_optimizer = experiment.rag_optimizer(\n    name='AutoAI RAG - Text Extraction service experiment',\n    description = \"AutoAI RAG experiment on documents generated by text extraction service\",\n    max_number_of_rag_patterns=4,\n    optimization_metrics=['answer_correctness']\n) ",
   "metadata": {
    "id": "0e694a9f-a4ca-43d5-9ade-c7a3c0a47592"
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "id": "6fa0ce2e-9848-4b8d-8199-9b7e87b01b66",
   "cell_type": "markdown",
   "source": "Call the `run()` method to trigger the AutoAI RAG experiment. Choose one of two modes: \n\n- To use the **interactive mode** (synchronous job), specify `background_mode=False` \n- To use the **background mode** (asynchronous job), specify `background_mode=True`",
   "metadata": {
    "id": "f2d978fd-98f5-40c8-b610-d177682b2885"
   }
  },
  {
   "id": "a9ee9255-b710-4e18-a486-528310f3da20",
   "cell_type": "code",
   "source": "_ = rag_optimizer.run(\n    input_data_references=input_data_references,\n    test_data_references=test_data_references,\n    background_mode=False\n)",
   "metadata": {
    "id": "d8e5b70c-b15e-45db-9dfe-5c194afb12e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n\n##############################################\n\nRunning '89fff5da-8405-4aba-abd6-b21b66f8cd03'\n\n##############################################\n\n\npending...\nrunning........................................................................................................................................................................................................................................................\ncompleted\nTraining of '89fff5da-8405-4aba-abd6-b21b66f8cd03' finished successfully.\n"
    }
   ],
   "execution_count": 22
  },
  {
   "id": "fab3418b-6fe1-42b3-8896-d2a56959c53e",
   "cell_type": "markdown",
   "source": "<a id=\"comparison\"></a>\n## Compare and test of RAG Patterns\n\nYou can list the trained patterns and information on evaluation metrics in the form of a Pandas DataFrame by calling the `summary()` method. You can use the DataFrame to compare all discovered patterns and select the one you like for further testing.",
   "metadata": {
    "id": "ea9b7109-b30e-4236-8697-6e81ccfc8b52"
   }
  },
  {
   "id": "f7eaaf5e-18a4-4c79-81b2-20e109199c1d",
   "cell_type": "code",
   "source": "summary = rag_optimizer.summary()\nsummary",
   "metadata": {
    "id": "41b179b5-82a4-4d03-87ab-c56541293722"
   },
   "outputs": [
    {
     "execution_count": 23,
     "output_type": "execute_result",
     "data": {
      "text/plain": "              mean_answer_correctness  mean_faithfulness  \\\nPattern_Name                                               \nPattern1                       0.7937             0.5730   \nPattern3                       0.7937             0.8994   \nPattern4                       0.6287             0.7439   \nPattern2                       0.5687             0.5961   \n\n              mean_context_correctness chunking.method  chunking.chunk_size  \\\nPattern_Name                                                                  \nPattern1                           1.0       recursive                  512   \nPattern3                           1.0       recursive                 1024   \nPattern4                           1.0       recursive                  512   \nPattern2                           1.0       recursive                  512   \n\n              chunking.chunk_overlap              embeddings.model_id  \\\nPattern_Name                                                            \nPattern1                         128   intfloat/multilingual-e5-large   \nPattern3                         512   intfloat/multilingual-e5-large   \nPattern4                         256  ibm/slate-125m-english-rtrvr-lm   \nPattern2                         256  ibm/slate-125m-english-rtrvr-v2   \n\n             vector_store.distance_metric retrieval.method  \\\nPattern_Name                                                 \nPattern1                           cosine           window   \nPattern3                           cosine           window   \nPattern4                           cosine           window   \nPattern2                           cosine           simple   \n\n              retrieval.number_of_chunks          generation.model_id  \nPattern_Name                                                           \nPattern1                               5    ibm/granite-3-8b-instruct  \nPattern3                               5      mistralai/mistral-large  \nPattern4                               3      mistralai/mistral-large  \nPattern2                               3  ibm/granite-13b-instruct-v2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_answer_correctness</th>\n      <th>mean_faithfulness</th>\n      <th>mean_context_correctness</th>\n      <th>chunking.method</th>\n      <th>chunking.chunk_size</th>\n      <th>chunking.chunk_overlap</th>\n      <th>embeddings.model_id</th>\n      <th>vector_store.distance_metric</th>\n      <th>retrieval.method</th>\n      <th>retrieval.number_of_chunks</th>\n      <th>generation.model_id</th>\n    </tr>\n    <tr>\n      <th>Pattern_Name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Pattern1</th>\n      <td>0.7937</td>\n      <td>0.5730</td>\n      <td>1.0</td>\n      <td>recursive</td>\n      <td>512</td>\n      <td>128</td>\n      <td>intfloat/multilingual-e5-large</td>\n      <td>cosine</td>\n      <td>window</td>\n      <td>5</td>\n      <td>ibm/granite-3-8b-instruct</td>\n    </tr>\n    <tr>\n      <th>Pattern3</th>\n      <td>0.7937</td>\n      <td>0.8994</td>\n      <td>1.0</td>\n      <td>recursive</td>\n      <td>1024</td>\n      <td>512</td>\n      <td>intfloat/multilingual-e5-large</td>\n      <td>cosine</td>\n      <td>window</td>\n      <td>5</td>\n      <td>mistralai/mistral-large</td>\n    </tr>\n    <tr>\n      <th>Pattern4</th>\n      <td>0.6287</td>\n      <td>0.7439</td>\n      <td>1.0</td>\n      <td>recursive</td>\n      <td>512</td>\n      <td>256</td>\n      <td>ibm/slate-125m-english-rtrvr-lm</td>\n      <td>cosine</td>\n      <td>window</td>\n      <td>3</td>\n      <td>mistralai/mistral-large</td>\n    </tr>\n    <tr>\n      <th>Pattern2</th>\n      <td>0.5687</td>\n      <td>0.5961</td>\n      <td>1.0</td>\n      <td>recursive</td>\n      <td>512</td>\n      <td>256</td>\n      <td>ibm/slate-125m-english-rtrvr-v2</td>\n      <td>cosine</td>\n      <td>simple</td>\n      <td>3</td>\n      <td>ibm/granite-13b-instruct-v2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 23
  },
  {
   "id": "99df89b2-f553-407d-8986-45ca402e8756",
   "cell_type": "markdown",
   "source": "### Get the selected pattern\n\nGet the RAGPattern object from the RAG Optimizer experiment. By default, the RAGPattern of the best pattern is returned.",
   "metadata": {
    "id": "bbdd6940-a59d-4693-96c2-c01151a9650e"
   }
  },
  {
   "id": "e6facc27-fbbb-4d6e-824f-c62b14bb492d",
   "cell_type": "code",
   "source": "best_pattern_name = summary.index.values[0]\nprint('Best pattern is:', best_pattern_name)\n\nbest_pattern = rag_optimizer.get_pattern()",
   "metadata": {
    "id": "5f631b81-5dd8-4ea7-b1d5-163f25122c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best pattern is: Pattern1\n"
    }
   ],
   "execution_count": 24
  },
  {
   "id": "e1d0e52c-a7d7-4847-ac56-cc6aca44d1b7",
   "cell_type": "markdown",
   "source": "Test the RAGPattern by querying it locally.",
   "metadata": {
    "id": "6d6d1467-a784-483f-9c47-1e30afd60c3e"
   }
  },
  {
   "id": "aba19879-5cdf-4ac0-92c8-ec6d85e2ac65",
   "cell_type": "code",
   "source": "from ibm_watsonx_ai.deployments import RuntimeContext\n\nruntime_context = RuntimeContext(api_client=client)\ninference_service_function = best_pattern.inference_service(runtime_context)[0]",
   "metadata": {
    "id": "aba19879-5cdf-4ac0-92c8-ec6d85e2ac65"
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "id": "4f786ff9-ac3d-45f5-b237-cbb1805251ec",
   "cell_type": "code",
   "source": "question = \"What training objectives are used for the models?\"\n\ncontext = RuntimeContext(\n    api_client=client,\n    request_payload_json={\"messages\": [{\"role\": \"user\", \"content\": question}]},\n)\nprint(inference_service_function(context)[\"body\"][\"choices\"][0][\"message\"][\"content\"])",
   "metadata": {
    "id": "4f786ff9-ac3d-45f5-b237-cbb1805251ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nThe models are trained using the causal language modeling objective and the Fill-In the-Middle (FIM) objective. The FIM objective is designed to predict inserted tokens given a context and subsequent text. The models are trained to work with both PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle) modes, using relevant formatting control tokens. The overall loss is computed as a weighted combination of the two objectives, with α empirically set to 0.5 during training. The FIM objective is only used during pretraining, and is dropped during instruction finetuning.\n\nReference(s):\nDocument\n4.2 Training Objective\nFor training of all our models, we use the causal language modeling objective and Fill-In the-Middle (FIM) (Bavarian et al., 2022) objective. The FIM objective is tasked to predict inserted tokens with the given context and subsequent text. We train our models to work with both PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle) modes, with relevant formatting control tokens, same as StarCoder (Li et al., 2023a). The overall loss is computed as a weighted combination of the 2 objectives:\nL = αLCLM + (1 − α)LF IM (1)\nWe emperically set α = 0.5 during training and find that this works well in practice leading to SOTA performance on both code completion and code infilling tasks. It should be noted that the FIM objective is only used during pretraining, however we drop it during instruction finetuning i.e we set α = 1.\n\nIs there anything else you would like to know?\n"
    }
   ],
   "execution_count": 37
  },
  {
   "id": "39cb08bc-52cb-4a42-8385-39bc99c5401f",
   "cell_type": "markdown",
   "source": "### Deploy the RAGPattern\n\nTo deploy the RAGPattern, store the defined RAG function and then create a deployed asset.",
   "metadata": {
    "id": "cfa94f4f-e2d8-4b90-b9ec-9137d9835b77"
   }
  },
  {
   "id": "0885815b-db43-4b6b-b090-2e7ff8e13ecb",
   "cell_type": "code",
   "source": "deployment_details = best_pattern.inference_service.deploy(\n    name=\"AutoAI RAG with Text Extraction service - test deployment\",\n    space_id=space_id,\n    deploy_params={\"tags\": [\"wx-autoai-rag\"]}\n)",
   "metadata": {
    "id": "42ff32e0-d45e-42a7-8c4a-76a4344e9664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n\n######################################################################################\n\nSynchronous deployment creation for id: '5f644c35-e6cf-43cb-ad04-7b0b13e67601' started\n\n######################################################################################\n\n\ninitializing\nNote: online_url and serving_urls are deprecated and will be removed in a future release. Use inference instead.\n......\nready\n\n\n-----------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_id='8cabd997-4968-4675-a3e0-a48c4b6df6fb'\n-----------------------------------------------------------------------------------------------\n\n\n"
    }
   ],
   "execution_count": 27
  },
  {
   "id": "d6cdba91-8784-4b8d-add1-49645cf175c0",
   "cell_type": "markdown",
   "source": "### Test the deployed function\n\nThe RAG service is now deployed in our space. To test the solution, run the cell below. Questions have to be provided in the payload. Their format is provided below.",
   "metadata": {
    "id": "fb7cb314-f921-469a-b5c3-ed65d44762c6"
   }
  },
  {
   "id": "cf9080da-4938-4944-824e-d6f9e2ac1e03",
   "cell_type": "code",
   "source": "deployment_id = client.deployments.get_id(deployment_details)\n\npayload = {\n    \"messages\": [{\"role\": \"user\", \"content\": question}]\n}\nscore_response = client.deployments.run_ai_service(deployment_id, payload)",
   "metadata": {
    "id": "2311ebd8-ef98-4ba3-aec1-152950292908"
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "id": "f38a1e7d-23d8-4f4f-984b-7688e6f2b3f6",
   "cell_type": "code",
   "source": "from langchain_core.documents import Document\nfrom IPython.display import display, Markdown\nfrom ibm_watsonx_ai.foundation_models.extensions.rag.utils import verbose_search\n\nreference_docs = score_response[\"choices\"][0][\"reference_documents\"]\nanswer = score_response[\"choices\"][0][\"message\"][\"content\"]\nverbose_search(question, [Document(**d) for d in reference_docs])\ndisplay(Markdown(f\"**Answer:** {answer}\"))",
   "metadata": {
    "id": "ab88bd8f-40a1-4eb7-a1e4-1e928104419a"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Question:** What training objectives are used for the models?"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        page_content  \\\n0  remove final 8 layers from the original model ...   \n1  Phase 1 (code only training): During phase 1, ...   \n2  5 Instruction Tuning Finetuning code LLMs on a...   \n3  Here, we present Granite Code models, a series...   \n4  languages, there is no single model that is be...   \n\n             sequence_number             document_id  \n0       [55, 56, 57, 58, 59]  granite_code_models.md  \n1       [59, 60, 61, 62, 63]  granite_code_models.md  \n2       [72, 73, 74, 75, 76]  granite_code_models.md  \n3       [18, 19, 20, 21, 22]  granite_code_models.md  \n4  [141, 142, 143, 144, 145]  granite_code_models.md  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page_content</th>\n      <th>sequence_number</th>\n      <th>document_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>remove final 8 layers from the original model ...</td>\n      <td>[55, 56, 57, 58, 59]</td>\n      <td>granite_code_models.md</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Phase 1 (code only training): During phase 1, ...</td>\n      <td>[59, 60, 61, 62, 63]</td>\n      <td>granite_code_models.md</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5 Instruction Tuning Finetuning code LLMs on a...</td>\n      <td>[72, 73, 74, 75, 76]</td>\n      <td>granite_code_models.md</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Here, we present Granite Code models, a series...</td>\n      <td>[18, 19, 20, 21, 22]</td>\n      <td>granite_code_models.md</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>languages, there is no single model that is be...</td>\n      <td>[141, 142, 143, 144, 145]</td>\n      <td>granite_code_models.md</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Answer:** \nThe models are trained using the causal language modeling objective and the Fill-In the-Middle (FIM) objective. The FIM objective is designed to predict inserted tokens given a context and subsequent text. The models are trained to work with both PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle) modes, using relevant formatting control tokens. The overall loss is computed as a weighted combination of the two objectives, with α empirically set to 0.5 during training. The FIM objective is only used during pretraining, and is dropped during instruction finetuning.\n\nReference(s):\nDocument\n4.2 Training Objective\nFor training of all our models, we use the causal language modeling objective and Fill-In the-Middle (FIM) (Bavarian et al., 2022) objective. The FIM objective is tasked to predict inserted tokens with the given context and subsequent text. We train our models to work with both PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle) modes, with relevant formatting control tokens, same as StarCoder (Li et al., 2023a). The overall loss is computed as a weighted combination of the 2 objectives:\nL = αLCLM + (1 − α)LF IM (1)\nWe emperically set α = 0.5 during training and find that this works well in practice leading to SOTA performance on both code completion and code infilling tasks. It should be noted that the FIM objective is only used during pretraining, however we drop it during instruction finetuning i.e we set α = 1.\n\nIs there anything else you would like to know?"
     },
     "metadata": {}
    }
   ],
   "execution_count": 29
  },
  {
   "id": "b19c8e71-27ba-432d-a821-357416fcd125",
   "cell_type": "markdown",
   "source": "<a id=\"summary\"></a>\n## Summary\n\n You successfully completed this notebook!\n \n You learned how to use AutoAI RAG with documents processed by the TextExtraction service.\n \nCheck out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts.",
   "metadata": {
    "id": "de5ce429-131b-4635-8c5a-e11fa74aa4cb"
   }
  },
  {
   "id": "a23242d0-83b2-4b67-ac49-5968fcdc6158",
   "cell_type": "markdown",
   "source": "### Author:\n **Witold Nowogórski**, Software Engineer at watsonx.ai.",
   "metadata": {
    "id": "cd56103d-fbaf-495b-8438-e45a91dd1d2c"
   }
  },
  {
   "id": "d6b352c4-b197-41a1-83b4-d616354d6c27",
   "cell_type": "markdown",
   "source": "Copyright © 2025 IBM. This notebook and its source code are released under the terms of the MIT License.",
   "metadata": {
    "id": "9c97642a-0344-4a1b-b8db-86fa1ef6d1ba"
   }
  }
 ]
}
