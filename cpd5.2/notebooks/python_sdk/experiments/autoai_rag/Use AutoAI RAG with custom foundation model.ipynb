{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![image](https://raw.githubusercontent.com/IBM/watsonx-ai-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
                "# AutoAI RAG experiment with custom foundation model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Disclaimers\n",
                "\n",
                "- Use only Projects and Spaces that are available in the watsonx context.\n",
                "\n",
                "\n",
                "## Notebook content\n",
                "\n",
                "This notebook demonstrates how to deploy custom foundation model and use this model in AutoAI RAG experiment.\n",
                "The data used in this notebook is from the [Granite Code Models paper](https://arxiv.org/pdf/2405.04324).\n",
                "\n",
                "Some familiarity with Python is helpful. This notebook uses Python 3.12.\n",
                "\n",
                "\n",
                "## Learning goal\n",
                "\n",
                "The learning goals of this notebook are:\n",
                "\n",
                "- How to deploy your own foundation models with huggingface hub\n",
                "- Create an AutoAI RAG job that will find the best RAG pattern based on custom foundation model used during the experiment\n",
                "\n",
                "\n",
                "## Contents\n",
                "\n",
                "This notebook contains the following parts:\n",
                "- [Set up the environment](#Set-up-the-environment)\n",
                "- [Prerequisites](#Prerequisites)\n",
                "- [Create API Client instance.](#Create-API-Client-instance.)\n",
                "- [Download custom model from hugging face](#Download-custom-model-from-hugging-face)\n",
                "- [Deploy the model](#Deploy-the-model)\n",
                "- [Prepare the data for the AutoAI RAG experiment](#Prepare-the-data-for-the-AutoAI-RAG-experiment)\n",
                "- [Run the AutoAI RAG experiment](#Run-the-AutoAI-RAG-experiment)\n",
                "- [Query generated pattern locally](#Query-generated-pattern-locally)\n",
                "- [Summary](#Summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set up the environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: wget in /Users/michalsteczko/anaconda3/envs/autoai_rag/lib/python3.11/site-packages (3.2)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n",
                        "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/michalsteczko/anaconda3/envs/autoai_rag/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4->ibm-watsonx-ai[rag]>=1.3.26) (0.6.1)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install -U wget | tail -n 1\n",
                "%pip install -U 'ibm-watsonx-ai[rag]>=1.3.26' | tail -n 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"prerequisites\"></a>\n",
                "\n",
                "## Prerequisites\n",
                "Please fill below values to be able to move forward:\n",
                "- URL - url which points to your CPD instance\n",
                "- USERNAME - username to your CPD instance\n",
                "- PASSWORD - password to your CPD instance associated with your username.\n",
                "- INSTANCE_ID - your CPD instance ID\n",
                "- VERSION - CPD version which your instance supports, but it has to be at least `5.2`\n",
                "- PROJECT_ID - ID of the project associated with your username and instance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "URL = \"PUT YOUR CPD INSTANCE URL HERE\"\n",
                "USERNAME = \"PUT YOUR USERNAME HERE\"\n",
                "PASSWORD = \"PUT YOUR PASSWORD HERE\"\n",
                "INSTANCE_ID = \"PUT YOUR INSTANCE ID HERE\"\n",
                "VERSION = \"5.2\"\n",
                "\n",
                "PROJECT_ID = \"PUT YOUR PROJECT ID HERE\"\n",
                "\n",
                "BUCKET_BENCHMARK_JSON_FILE_PATH = \"benchmark.json\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create API Client instance.\n",
                "This client will allow us to connect with the IBM services."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai import APIClient, Credentials\n",
                "\n",
                "credentials = Credentials(\n",
                "    url=URL,\n",
                "    username=USERNAME,\n",
                "    password=PASSWORD,\n",
                "    instance_id=INSTANCE_ID,\n",
                "    version=VERSION,\n",
                "    verify=False,\n",
                ")\n",
                "\n",
                "client = APIClient(credentials=credentials, project_id=PROJECT_ID)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create custom PVC for custom foundation model\n",
                "\n",
                "Once you have our model files in our COS bucket you need to create our custom model specification. <br />\n",
                "Please note that this is beyond the scope of this notebook, refer to this [documentation](https://www.ibm.com/docs/en/software-hub/5.2.x?topic=setup-deploying-custom-foundation-models) to be able to use your custom model on your CPD cluster."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Deploy the model\n",
                "Check the docs to avoid any problems during model deployment [here](https://ibm.github.io/watsonx-ai-python-sdk/fm_custom_models.html#id2)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create custom model repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'deepseek-r1-distill-llama-8b'"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from ibm_watsonx_ai.foundation_models import get_custom_model_specs\n",
                "\n",
                "custom_model_spec = get_custom_model_specs(api_client=client, limit=1)\n",
                "model_id = custom_model_spec[\"resources\"][0][\"model_id\"]\n",
                "model_id"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "sw_spec_id = client.software_specifications.get_id_by_name(\"watsonx-cfm-caikit-1.0\")\n",
                "sw_metadata = {\n",
                "    client.repository.ModelMetaNames.NAME: \"My custom deployment\",\n",
                "    client.repository.ModelMetaNames.SOFTWARE_SPEC_ID: sw_spec_id,\n",
                "    client.repository.ModelMetaNames.TYPE: client.repository.ModelAssetTypes.CUSTOM_FOUNDATION_MODEL_1_0,\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>ID</th>\n",
                            "      <th>NAME</th>\n",
                            "      <th>CREATED</th>\n",
                            "      <th>FRAMEWORK</th>\n",
                            "      <th>TYPE</th>\n",
                            "      <th>SPEC_STATE</th>\n",
                            "      <th>SPEC_REPLACEMENT</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>e2a38f94-31c5-4106-880b-f41423fd42db</td>\n",
                            "      <td>My custom deployment</td>\n",
                            "      <td>2025-06-27T13:35:53Z</td>\n",
                            "      <td>custom_foundation_model_1.0</td>\n",
                            "      <td>model</td>\n",
                            "      <td>supported</td>\n",
                            "      <td></td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                     ID                  NAME  \\\n",
                            "0  f075d432-3353-4d28-9fe9-106580387391  My custom deployment   \n",
                            "\n",
                            "                CREATED                    FRAMEWORK   TYPE SPEC_STATE  \\\n",
                            "0  2025-06-27T13:35:53Z  custom_foundation_model_1.0  model  supported   \n",
                            "\n",
                            "  SPEC_REPLACEMENT  \n",
                            "0                   "
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "stored_model_details = client.repository.store_model(\n",
                "    model=model_id, meta_props=sw_metadata\n",
                ")\n",
                "stored_model_asset_id = client.repository.get_model_id(stored_model_details)\n",
                "client.repository.list(framework_filter=\"custom_foundation_model_1.0\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create custom foundation model hardware specification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "hardware_spec_meta_props = {\n",
                "    client.hardware_specifications.ConfigurationMetaNames.NAME: \"Custom GPU hw spec\",\n",
                "    client.hardware_specifications.ConfigurationMetaNames.NODES: {\n",
                "        \"cpu\": {\"units\": \"2\"},\n",
                "        \"mem\": {\"size\": \"128Gi\"},\n",
                "        \"gpu\": {\"num_gpu\": 1},\n",
                "    },\n",
                "}\n",
                "\n",
                "hw_spec_details = client.hardware_specifications.store(hardware_spec_meta_props)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "hw_spec_details = client.hardware_specifications.get_details(\n",
                "    client.hardware_specifications.get_id_by_name(\"Custom GPU hw spec\")\n",
                ")\n",
                "hw_spec_id = client.hardware_specifications.get_id(hw_spec_details)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Perform custom model deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "######################################################################################\n",
                        "\n",
                        "Synchronous deployment creation for id: 'e2a38f94-31c5-4106-880b-f41423fd42db' started\n",
                        "\n",
                        "######################################################################################\n",
                        "\n",
                        "\n",
                        "initializing\n",
                        "Note: online_url and serving_urls are deprecated and will be removed in a future release. Use inference instead.\n",
                        "..........\n",
                        "ready\n",
                        "\n",
                        "\n",
                        "-----------------------------------------------------------------------------------------------\n",
                        "Successfully finished deployment creation, deployment_id='30702554-42cc-43e8-82a5-5dc7826d719d'\n",
                        "-----------------------------------------------------------------------------------------------\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "MAX_SEQUENCE_LENGTH = 32_000\n",
                "MAX_NEW_TOKENS = 1000\n",
                "MIN_NEW_TOKENS = 1\n",
                "MAX_BATCH_SIZE = 1024\n",
                "metadata = {\n",
                "    client.deployments.ConfigurationMetaNames.NAME: \"My custom foundation model\",\n",
                "    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"My custom foundation model\",\n",
                "    client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
                "    # optionally overwrite model parameters here\n",
                "    client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: {\n",
                "        \"name\": \"Custom GPU hw spec\"\n",
                "    },\n",
                "    client.deployments.ConfigurationMetaNames.FOUNDATION_MODEL: {\n",
                "        \"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
                "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
                "        \"max_batch_size\": MAX_BATCH_SIZE,\n",
                "    },\n",
                "    client.deployments.ConfigurationMetaNames.SERVING_NAME: \"pllum_12b_instruct\",  # must be unique\n",
                "}\n",
                "deployment_details = client.deployments.create(stored_model_asset_id, metadata)\n",
                "deployment_id = deployment_details[\"metadata\"][\"id\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare the data for the AutoAI RAG experiment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Download `granite_code_models.pdf` document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'granite_code_models.pdf'"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import wget\n",
                "\n",
                "data_url = \"https://arxiv.org/pdf/2405.04324\"\n",
                "\n",
                "byom_input_filename = \"granite_code_models.pdf\"\n",
                "\n",
                "wget.download(data_url, byom_input_filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create data asset with your training data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating data asset...\n",
                        "SUCCESS\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'fb05d631-8c5e-4641-bc72-6633ba06cbe2'"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "document_asset_details = client.data_assets.create(name=byom_input_filename, file_path=byom_input_filename)\n",
                "\n",
                "document_asset_id = client.data_assets.get_id(document_asset_details)\n",
                "document_asset_id"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.helpers import DataConnection\n",
                "\n",
                "input_data_references = [DataConnection(data_asset_id=document_asset_id)]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create your own benchmark.json file to ask the questions related to the document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "local_benchmark_json_filename = \"benchmark.json\"\n",
                "\n",
                "benchmarking_data = [\n",
                "    {\n",
                "        \"question\": \"What are the two main variants of Granite Code models?\",\n",
                "        \"correct_answer\": \"The two main variants are Granite Code Base and Granite Code Instruct.\",\n",
                "        \"correct_answer_document_ids\": [byom_input_filename],\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the purpose of Granite Code Instruct models?\",\n",
                "        \"correct_answer\": \"Granite Code Instruct models are finetuned for instruction-following tasks using datasets like CommitPack, OASST, HelpSteer, and synthetic code instruction datasets, aiming to improve reasoning and instruction-following capabilities.\",\n",
                "        \"correct_answer_document_ids\": [byom_input_filename],\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the licensing model for Granite Code models?\",\n",
                "        \"correct_answer\": \"Granite Code models are released under the Apache 2.0 license, ensuring permissive and enterprise-friendly usage.\",\n",
                "        \"correct_answer_document_ids\": [byom_input_filename],\n",
                "    },\n",
                "]\n",
                "\n",
                "with open(local_benchmark_json_filename, mode=\"w\", encoding=\"utf-8\") as fp:\n",
                "    json.dump(benchmarking_data, fp, indent=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create data asset with benchmark.json file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating data asset...\n",
                        "SUCCESS\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'fc14fca8-93c4-4b5e-a72f-d19fcc0aeaf4'"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_asset_details = client.data_assets.create(name=local_benchmark_json_filename, file_path=local_benchmark_json_filename)\n",
                "\n",
                "test_asset_id = client.data_assets.get_id(test_asset_details)\n",
                "test_asset_id"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_data_references = [DataConnection(data_asset_id=test_asset_id)]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run the AutoAI RAG experiment\n",
                "\n",
                "Provide the input information for AutoAI RAG optimizer:\n",
                "- `custom_prompt_template_text` - custom prompt template text which will be used to query your own foundation model\n",
                "- `custom_context_template_text` - custom context template text which will be used to query your own foundation model\n",
                "- `name` - experiment name\n",
                "- `description` - experiment description\n",
                "- `max_number_of_rag_patterns` - maximum number of RAG patterns to create\n",
                "- `optimization_metrics` - target optimization metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "##############################################\n",
                        "\n",
                        "Running '7878e0b8-94a6-4e9a-b1b4-f3c62fa70786'\n",
                        "\n",
                        "##############################################\n",
                        "\n",
                        "\n",
                        "pending....\n",
                        "running...........................................................................................................................................\n",
                        "completed\n",
                        "Training of '7878e0b8-94a6-4e9a-b1b4-f3c62fa70786' finished successfully.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'entity': {'hardware_spec': {'id': 'a6c4923b-b8e4-444c-9f43-8a7ec3020110',\n",
                            "   'name': 'L'},\n",
                            "  'input_data_references': [{'location': {'href': '/v2/assets/fb05d631-8c5e-4641-bc72-6633ba06cbe2?project_id=19a157bf-5c1c-4773-812a-520d8068d84a',\n",
                            "     'id': 'fb05d631-8c5e-4641-bc72-6633ba06cbe2'},\n",
                            "    'type': 'data_asset'}],\n",
                            "  'parameters': {'constraints': {'generation': {'foundation_models': [{'context_template_text': 'My document {document}',\n",
                            "       'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "       'parameters': {'max_sequence_length': 32000},\n",
                            "       'project_id': '19a157bf-5c1c-4773-812a-520d8068d84a',\n",
                            "       'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.'}]},\n",
                            "    'max_number_of_rag_patterns': 4},\n",
                            "   'optimization': {'metrics': ['faithfulness']},\n",
                            "   'output_logs': True},\n",
                            "  'results': [{'context': {'iteration': 0,\n",
                            "     'max_combinations': 40,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 193,\n",
                            "      'location': {'evaluation_results': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/evaluation_results.json',\n",
                            "       'indexing_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern1',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 128,\n",
                            "        'chunk_size': 512,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': 'My document {document}',\n",
                            "        'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 32000,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.',\n",
                            "        'word_to_token_ratio': 2.0},\n",
                            "       'retrieval': {'method': 'simple', 'number_of_chunks': 3},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_7878e0b8_20250627145642',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.125,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.125, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.125, 'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.125, 'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.125, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.125, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.125, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 0.7937,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.5079,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.1414,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.072,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}},\n",
                            "   {'context': {'iteration': 1,\n",
                            "     'max_combinations': 40,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 266,\n",
                            "      'location': {'evaluation_results': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/evaluation_results.json',\n",
                            "       'indexing_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern2',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "        'chunk_size': 512,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': 'My document {document}',\n",
                            "        'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 32000,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.',\n",
                            "        'word_to_token_ratio': 2.0},\n",
                            "       'retrieval': {'method': 'window',\n",
                            "        'number_of_chunks': 3,\n",
                            "        'window_size': 1},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_7878e0b8_20250627150244',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.0,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.0, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.0, 'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.0, 'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 1.0, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.0, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.0, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 1.0,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.4815,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.3475,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.2144,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}},\n",
                            "   {'context': {'iteration': 2,\n",
                            "     'max_combinations': 40,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 180,\n",
                            "      'location': {'evaluation_results': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/evaluation_results.json',\n",
                            "       'indexing_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern3',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "        'chunk_size': 1024,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': 'My document {document}',\n",
                            "        'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 32000,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.',\n",
                            "        'word_to_token_ratio': 2.0},\n",
                            "       'retrieval': {'method': 'simple', 'number_of_chunks': 5},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_7878e0b8_20250627150710',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.18471761,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.2972388, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.0, 'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.0, 'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.35138428,\n",
                            "         'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.061107427, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.10555187, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 0.4545,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.2256,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.3934,\n",
                            "       'ci_low': 0.0323,\n",
                            "       'mean': 0.1634,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}}],\n",
                            "  'results_reference': {'location': {'path': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results',\n",
                            "    'training': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786',\n",
                            "    'training_status': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/training-status.json',\n",
                            "    'training_log': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/output.log',\n",
                            "    'assets_path': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/assets'},\n",
                            "   'type': 'fs'},\n",
                            "  'status': {'completed_at': '2025-06-27T15:10:12.169Z',\n",
                            "   'message': {'level': 'warning',\n",
                            "    'text': 'AAR021W: Warnings:\\n AR1128W: Unable to retrieve chunks and generate answers for deployment with ID: 30702554-42cc-43e8-82a5-5dc7826d719d.'},\n",
                            "   'running_at': '2025-06-27T14:56:38.000Z',\n",
                            "   'state': 'completed',\n",
                            "   'step': 'generation'},\n",
                            "  'test_data_references': [{'location': {'href': '/v2/assets/fc14fca8-93c4-4b5e-a72f-d19fcc0aeaf4?project_id=19a157bf-5c1c-4773-812a-520d8068d84a',\n",
                            "     'id': 'fc14fca8-93c4-4b5e-a72f-d19fcc0aeaf4'},\n",
                            "    'type': 'data_asset'}],\n",
                            "  'timestamp': '2025-06-27T15:10:17.754Z'},\n",
                            " 'metadata': {'created_at': '2025-06-27T14:56:03.343Z',\n",
                            "  'description': 'AutoAI RAG experiment with custom foundation model.',\n",
                            "  'id': '7878e0b8-94a6-4e9a-b1b4-f3c62fa70786',\n",
                            "  'modified_at': '2025-06-27T15:10:12.177Z',\n",
                            "  'name': 'AutoAI RAG - Custom foundation model experiment',\n",
                            "  'project_id': '19a157bf-5c1c-4773-812a-520d8068d84a'}}"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from ibm_watsonx_ai.experiment import AutoAI\n",
                "from ibm_watsonx_ai.foundation_models.schema import (\n",
                "    AutoAIRAGCustomModelConfig,\n",
                "    AutoAIRAGModelParams,\n",
                ")\n",
                "\n",
                "experiment = AutoAI(credentials, project_id=PROJECT_ID)\n",
                "\n",
                "custom_prompt_template_text = (\n",
                "    \"Answer my question {question} related to these documents {reference_documents}.\"\n",
                ")\n",
                "custom_context_template_text = \"My document {document}\"\n",
                "\n",
                "parameters = AutoAIRAGModelParams(max_sequence_length=32_000)\n",
                "custom_foundation_model_config = AutoAIRAGCustomModelConfig(\n",
                "    deployment_id=deployment_id,\n",
                "    project_id=PROJECT_ID,\n",
                "    prompt_template_text=custom_prompt_template_text,\n",
                "    context_template_text=custom_context_template_text,\n",
                "    parameters=parameters,\n",
                ")\n",
                "\n",
                "rag_optimizer = experiment.rag_optimizer(\n",
                "    name=\"AutoAI RAG - Custom foundation model experiment\",\n",
                "    description=\"AutoAI RAG experiment with custom foundation model.\",\n",
                "    max_number_of_rag_patterns=4,\n",
                "    optimization_metrics=[\"faithfulness\"],\n",
                "    foundation_models=[custom_foundation_model_config],\n",
                ")\n",
                "\n",
                "rag_optimizer.run(\n",
                "    test_data_references=test_data_references,\n",
                "    input_data_references=input_data_references,\n",
                "    background_mode=False\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'entity': {'hardware_spec': {'id': 'a6c4923b-b8e4-444c-9f43-8a7ec3020110',\n",
                            "   'name': 'L'},\n",
                            "  'input_data_references': [{'location': {'href': '/v2/assets/fb05d631-8c5e-4641-bc72-6633ba06cbe2?project_id=19a157bf-5c1c-4773-812a-520d8068d84a',\n",
                            "     'id': 'fb05d631-8c5e-4641-bc72-6633ba06cbe2'},\n",
                            "    'type': 'data_asset'}],\n",
                            "  'parameters': {'constraints': {'generation': {'foundation_models': [{'context_template_text': 'My document {document}',\n",
                            "       'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "       'parameters': {'max_sequence_length': 32000},\n",
                            "       'project_id': '19a157bf-5c1c-4773-812a-520d8068d84a',\n",
                            "       'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.'}]},\n",
                            "    'max_number_of_rag_patterns': 4},\n",
                            "   'optimization': {'metrics': ['faithfulness']},\n",
                            "   'output_logs': True},\n",
                            "  'results': [{'context': {'iteration': 0,\n",
                            "     'max_combinations': 40,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 193,\n",
                            "      'location': {'evaluation_results': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/evaluation_results.json',\n",
                            "       'indexing_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern1/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern1',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 128,\n",
                            "        'chunk_size': 512,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': 'My document {document}',\n",
                            "        'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 32000,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.',\n",
                            "        'word_to_token_ratio': 2.0},\n",
                            "       'retrieval': {'method': 'simple', 'number_of_chunks': 3},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_7878e0b8_20250627145642',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.125,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.125, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.125, 'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.125, 'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.125, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.125, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.125, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 0.7937,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.5079,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.1414,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.072,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}},\n",
                            "   {'context': {'iteration': 1,\n",
                            "     'max_combinations': 40,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 266,\n",
                            "      'location': {'evaluation_results': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/evaluation_results.json',\n",
                            "       'indexing_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern2',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "        'chunk_size': 512,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': 'My document {document}',\n",
                            "        'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 32000,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.',\n",
                            "        'word_to_token_ratio': 2.0},\n",
                            "       'retrieval': {'method': 'window',\n",
                            "        'number_of_chunks': 3,\n",
                            "        'window_size': 1},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_7878e0b8_20250627150244',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.0,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.0, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.0, 'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.0, 'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 1.0, 'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.0, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.0, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 1.0,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.4815,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.3475,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.2144,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}},\n",
                            "   {'context': {'iteration': 2,\n",
                            "     'max_combinations': 40,\n",
                            "     'rag_pattern': {'composition_steps': ['model_selection',\n",
                            "       'chunking',\n",
                            "       'embeddings',\n",
                            "       'retrieval',\n",
                            "       'generation'],\n",
                            "      'duration_seconds': 180,\n",
                            "      'location': {'evaluation_results': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/evaluation_results.json',\n",
                            "       'indexing_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/indexing_inference_notebook.ipynb',\n",
                            "       'inference_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/indexing_inference_notebook.ipynb',\n",
                            "       'inference_service_code': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/inference_ai_service.gz',\n",
                            "       'inference_service_metadata': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern3/inference_service_metadata.json'},\n",
                            "      'name': 'Pattern3',\n",
                            "      'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "        'chunk_size': 1024,\n",
                            "        'method': 'recursive'},\n",
                            "       'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr',\n",
                            "        'truncate_input_tokens': 512,\n",
                            "        'truncate_strategy': 'left'},\n",
                            "       'generation': {'context_template_text': 'My document {document}',\n",
                            "        'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "        'parameters': {'decoding_method': 'greedy',\n",
                            "         'max_new_tokens': 1000,\n",
                            "         'max_sequence_length': 32000,\n",
                            "         'min_new_tokens': 1},\n",
                            "        'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.',\n",
                            "        'word_to_token_ratio': 2.0},\n",
                            "       'retrieval': {'method': 'simple', 'number_of_chunks': 5},\n",
                            "       'vector_store': {'datasource_type': 'chroma',\n",
                            "        'distance_metric': 'cosine',\n",
                            "        'index_name': 'autoai_rag_7878e0b8_20250627150710',\n",
                            "        'operation': 'upsert',\n",
                            "        'schema': {'fields': [{'description': 'text field',\n",
                            "           'name': 'text',\n",
                            "           'role': 'text',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'document name field',\n",
                            "           'name': 'document_id',\n",
                            "           'role': 'document_name',\n",
                            "           'type': 'string'},\n",
                            "          {'description': 'chunk starting token position in the source document',\n",
                            "           'name': 'start_index',\n",
                            "           'role': 'start_index',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'chunk number per document',\n",
                            "           'name': 'sequence_number',\n",
                            "           'role': 'sequence_number',\n",
                            "           'type': 'number'},\n",
                            "          {'description': 'vector embeddings',\n",
                            "           'name': 'vector',\n",
                            "           'role': 'vector_embeddings',\n",
                            "           'type': 'array'}],\n",
                            "         'id': 'autoai_rag_1.0',\n",
                            "         'name': 'Document schema using open-source loaders',\n",
                            "         'type': 'struct'}}},\n",
                            "      'settings_importance': {'chunking': [{'importance': 0.18471761,\n",
                            "         'parameter': 'chunk_size'},\n",
                            "        {'importance': 0.2972388, 'parameter': 'chunk_overlap'}],\n",
                            "       'embeddings': [{'importance': 0.0, 'parameter': 'embedding_model'}],\n",
                            "       'generation': [{'importance': 0.0, 'parameter': 'foundation_model'}],\n",
                            "       'retrieval': [{'importance': 0.35138428,\n",
                            "         'parameter': 'retrieval_method'},\n",
                            "        {'importance': 0.061107427, 'parameter': 'window_size'},\n",
                            "        {'importance': 0.10555187, 'parameter': 'number_of_chunks'}]}},\n",
                            "     'software_spec': {}},\n",
                            "    'metrics': {'test_data': [{'ci_high': 0.4545,\n",
                            "       'ci_low': 0.0,\n",
                            "       'mean': 0.2256,\n",
                            "       'metric_name': 'answer_correctness'},\n",
                            "      {'ci_high': 0.3934,\n",
                            "       'ci_low': 0.0323,\n",
                            "       'mean': 0.1634,\n",
                            "       'metric_name': 'faithfulness'},\n",
                            "      {'mean': 1.0, 'metric_name': 'context_correctness'}]}}],\n",
                            "  'results_reference': {'location': {'path': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results',\n",
                            "    'training': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786',\n",
                            "    'training_status': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/training-status.json',\n",
                            "    'training_log': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/output.log',\n",
                            "    'assets_path': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/assets'},\n",
                            "   'type': 'fs'},\n",
                            "  'status': {'completed_at': '2025-06-27T15:10:12.169Z',\n",
                            "   'message': {'level': 'warning',\n",
                            "    'text': 'AAR021W: Warnings:\\n AR1128W: Unable to retrieve chunks and generate answers for deployment with ID: 30702554-42cc-43e8-82a5-5dc7826d719d.'},\n",
                            "   'running_at': '2025-06-27T14:56:38.000Z',\n",
                            "   'state': 'completed',\n",
                            "   'step': 'generation'},\n",
                            "  'test_data_references': [{'location': {'href': '/v2/assets/fc14fca8-93c4-4b5e-a72f-d19fcc0aeaf4?project_id=19a157bf-5c1c-4773-812a-520d8068d84a',\n",
                            "     'id': 'fc14fca8-93c4-4b5e-a72f-d19fcc0aeaf4'},\n",
                            "    'type': 'data_asset'}],\n",
                            "  'timestamp': '2025-06-27T15:10:17.754Z'},\n",
                            " 'metadata': {'created_at': '2025-06-27T14:56:03.343Z',\n",
                            "  'description': 'AutoAI RAG experiment with custom foundation model.',\n",
                            "  'id': '7878e0b8-94a6-4e9a-b1b4-f3c62fa70786',\n",
                            "  'modified_at': '2025-06-27T15:10:12.177Z',\n",
                            "  'name': 'AutoAI RAG - Custom foundation model experiment',\n",
                            "  'project_id': '19a157bf-5c1c-4773-812a-520d8068d84a'}}"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "rag_optimizer.get_run_details()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>mean_faithfulness</th>\n",
                            "      <th>mean_answer_correctness</th>\n",
                            "      <th>mean_context_correctness</th>\n",
                            "      <th>chunking.method</th>\n",
                            "      <th>chunking.chunk_size</th>\n",
                            "      <th>chunking.chunk_overlap</th>\n",
                            "      <th>embeddings.model_id</th>\n",
                            "      <th>vector_store.distance_metric</th>\n",
                            "      <th>retrieval.method</th>\n",
                            "      <th>retrieval.number_of_chunks</th>\n",
                            "      <th>retrieval.hybrid_ranker</th>\n",
                            "      <th>generation.model_id</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Pattern_Name</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>Pattern2</th>\n",
                            "      <td>0.2144</td>\n",
                            "      <td>0.4815</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>recursive</td>\n",
                            "      <td>512</td>\n",
                            "      <td>256</td>\n",
                            "      <td>ibm/slate-125m-english-rtrvr</td>\n",
                            "      <td>cosine</td>\n",
                            "      <td>window</td>\n",
                            "      <td>3</td>\n",
                            "      <td></td>\n",
                            "      <td>30702554-42cc-43e8-82a5-5dc7826d719d</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Pattern3</th>\n",
                            "      <td>0.1634</td>\n",
                            "      <td>0.2256</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>recursive</td>\n",
                            "      <td>1024</td>\n",
                            "      <td>256</td>\n",
                            "      <td>ibm/slate-125m-english-rtrvr</td>\n",
                            "      <td>cosine</td>\n",
                            "      <td>simple</td>\n",
                            "      <td>5</td>\n",
                            "      <td></td>\n",
                            "      <td>30702554-42cc-43e8-82a5-5dc7826d719d</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Pattern1</th>\n",
                            "      <td>0.0720</td>\n",
                            "      <td>0.5079</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>recursive</td>\n",
                            "      <td>512</td>\n",
                            "      <td>128</td>\n",
                            "      <td>ibm/slate-125m-english-rtrvr</td>\n",
                            "      <td>cosine</td>\n",
                            "      <td>simple</td>\n",
                            "      <td>3</td>\n",
                            "      <td></td>\n",
                            "      <td>30702554-42cc-43e8-82a5-5dc7826d719d</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "              mean_faithfulness  mean_answer_correctness  \\\n",
                            "Pattern_Name                                               \n",
                            "Pattern2                 0.2144                   0.4815   \n",
                            "Pattern3                 0.1634                   0.2256   \n",
                            "Pattern1                 0.0720                   0.5079   \n",
                            "\n",
                            "              mean_context_correctness chunking.method  chunking.chunk_size  \\\n",
                            "Pattern_Name                                                                  \n",
                            "Pattern2                           1.0       recursive                  512   \n",
                            "Pattern3                           1.0       recursive                 1024   \n",
                            "Pattern1                           1.0       recursive                  512   \n",
                            "\n",
                            "              chunking.chunk_overlap           embeddings.model_id  \\\n",
                            "Pattern_Name                                                         \n",
                            "Pattern2                         256  ibm/slate-125m-english-rtrvr   \n",
                            "Pattern3                         256  ibm/slate-125m-english-rtrvr   \n",
                            "Pattern1                         128  ibm/slate-125m-english-rtrvr   \n",
                            "\n",
                            "             vector_store.distance_metric retrieval.method  \\\n",
                            "Pattern_Name                                                 \n",
                            "Pattern2                           cosine           window   \n",
                            "Pattern3                           cosine           simple   \n",
                            "Pattern1                           cosine           simple   \n",
                            "\n",
                            "              retrieval.number_of_chunks retrieval.hybrid_ranker  \\\n",
                            "Pattern_Name                                                       \n",
                            "Pattern2                               3                           \n",
                            "Pattern3                               5                           \n",
                            "Pattern1                               3                           \n",
                            "\n",
                            "                               generation.model_id  \n",
                            "Pattern_Name                                        \n",
                            "Pattern2      30702554-42cc-43e8-82a5-5dc7826d719d  \n",
                            "Pattern3      30702554-42cc-43e8-82a5-5dc7826d719d  \n",
                            "Pattern1      30702554-42cc-43e8-82a5-5dc7826d719d  "
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "summary = rag_optimizer.summary()\n",
                "summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best pattern is: Pattern2\n"
                    ]
                }
            ],
            "source": [
                "best_pattern_name = summary.index.values[0]\n",
                "print(\"Best pattern is:\", best_pattern_name)\n",
                "\n",
                "best_pattern = rag_optimizer.get_pattern()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'composition_steps': ['model_selection',\n",
                            "  'chunking',\n",
                            "  'embeddings',\n",
                            "  'retrieval',\n",
                            "  'generation'],\n",
                            " 'duration_seconds': 266,\n",
                            " 'location': {'evaluation_results': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/evaluation_results.json',\n",
                            "  'indexing_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/indexing_inference_notebook.ipynb',\n",
                            "  'inference_notebook': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/indexing_inference_notebook.ipynb',\n",
                            "  'inference_service_code': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/inference_ai_service.gz',\n",
                            "  'inference_service_metadata': '/projects/19a157bf-5c1c-4773-812a-520d8068d84a/assets/autorag/results/7878e0b8-94a6-4e9a-b1b4-f3c62fa70786/Pattern2/inference_service_metadata.json'},\n",
                            " 'name': 'Pattern2',\n",
                            " 'settings': {'chunking': {'chunk_overlap': 256,\n",
                            "   'chunk_size': 512,\n",
                            "   'method': 'recursive'},\n",
                            "  'embeddings': {'model_id': 'ibm/slate-125m-english-rtrvr',\n",
                            "   'truncate_input_tokens': 512,\n",
                            "   'truncate_strategy': 'left'},\n",
                            "  'generation': {'context_template_text': 'My document {document}',\n",
                            "   'deployment_id': '30702554-42cc-43e8-82a5-5dc7826d719d',\n",
                            "   'parameters': {'decoding_method': 'greedy',\n",
                            "    'max_new_tokens': 1000,\n",
                            "    'min_new_tokens': 1},\n",
                            "   'prompt_template_text': 'Answer my question {question} related to these documents {reference_documents}.',\n",
                            "   'word_to_token_ratio': 2.0},\n",
                            "  'retrieval': {'method': 'window', 'number_of_chunks': 3, 'window_size': 1},\n",
                            "  'vector_store': {'datasource_type': 'chroma',\n",
                            "   'distance_metric': 'cosine',\n",
                            "   'index_name': 'autoai_rag_7878e0b8_20250627150244',\n",
                            "   'operation': 'upsert',\n",
                            "   'schema': {'fields': [{'description': 'text field',\n",
                            "      'name': 'text',\n",
                            "      'role': 'text',\n",
                            "      'type': 'string'},\n",
                            "     {'description': 'document name field',\n",
                            "      'name': 'document_id',\n",
                            "      'role': 'document_name',\n",
                            "      'type': 'string'},\n",
                            "     {'description': 'chunk starting token position in the source document',\n",
                            "      'name': 'start_index',\n",
                            "      'role': 'start_index',\n",
                            "      'type': 'number'},\n",
                            "     {'description': 'chunk number per document',\n",
                            "      'name': 'sequence_number',\n",
                            "      'role': 'sequence_number',\n",
                            "      'type': 'number'},\n",
                            "     {'description': 'vector embeddings',\n",
                            "      'name': 'vector',\n",
                            "      'role': 'vector_embeddings',\n",
                            "      'type': 'array'}],\n",
                            "    'id': 'autoai_rag_1.0',\n",
                            "    'name': 'Document schema using open-source loaders',\n",
                            "    'type': 'struct'}}},\n",
                            " 'settings_importance': {'chunking': [{'importance': 0.0,\n",
                            "    'parameter': 'chunk_size'},\n",
                            "   {'importance': 0.0, 'parameter': 'chunk_overlap'}],\n",
                            "  'embeddings': [{'importance': 0.0, 'parameter': 'embedding_model'}],\n",
                            "  'generation': [{'importance': 0.0, 'parameter': 'foundation_model'}],\n",
                            "  'retrieval': [{'importance': 1.0, 'parameter': 'retrieval_method'},\n",
                            "   {'importance': 0.0, 'parameter': 'window_size'},\n",
                            "   {'importance': 0.0, 'parameter': 'number_of_chunks'}]}}"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "rag_optimizer.get_pattern_details(pattern_name=best_pattern_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Query generated pattern locally"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/michalsteczko/anaconda3/envs/autoai_rag/lib/python3.11/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
                        "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
                    ]
                }
            ],
            "source": [
                "from ibm_watsonx_ai.deployments import RuntimeContext\n",
                "\n",
                "runtime_context = RuntimeContext(api_client=client)\n",
                "inference_service_function = best_pattern.inference_service(runtime_context)[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'body': {'choices': [{'index': 0,\n",
                            "    'message': {'role': 'assistant',\n",
                            "     'content': ' The\\nclusters are equipped with 100Gbps and 200Gbps HDR InfiniBand links, respectively.\\nWe utilize NVIDIA’s Megatron-LM (Shoeybi et al., 2019; Narayanan et al., 2021) for\\ndistributed training, which is optimized for large language models. We use the same Megatron\\nLM framework for all our models, ensuring consistency in training infrastructure.\\n4.5 Model Architecture\\nThe architecture of the Granite Code models is based on the original transformer architecture\\n(Douglas & Smith, 2019) with modifications for code modeling. The base model has 16\\nlayers, 8 attention heads, and a token embedding dimension of 512. For the 3B model, we\\nuse the standard transformer architecture with a multi-head attention mechanism. The 8B model\\nincorporates Grouped-Query Attention (GQA) (Ainslie et al., 2023) to improve inference\\nefficiency. The 20B model uses learned absolute position embeddings and Multi-Query\\nAttention (Shazeer, 2019). The 34B model is built upon the 20B model with depth\\nupscaling (Kim et al., 2024) to double the model depth, resulting in 88 layers.\\nThe models are trained with different context lengths depending on their size: 2048, 4096,\\n8192, and 8192 tokens respectively for 3B, 8B, 20B, and 34B models.\\nFor the MLP block, we use GELU activation function (Hendrycks & Gimpel, 2023) for the 20B\\nand 34B models, while using GLU (Shazeer, 2020) for the 3B and 8B models. For\\nnormalization, we use LayerNorm (Ba et al., 2016) for all models except the 8B model,\\nwhich uses RMSNorm (Zhang & Sennrich, 2019) for computational efficiency.\\n5.1 Evaluation Protocol\\nWe evaluate the Granite Code models on a comprehensive set of benchmarks including\\nHumanEvalPack (Muennighoff et al., 2023), MBPP(+) (Austin et al., 2021; Liu et al.,\\n2023a), RepoBench (Liu et al., 2023b), ReCode (Wang et al., 2022), and more. This\\nset of benchmarks encompasses many different kinds of coding tasks beyond just code\\nsynthesis in Python, e.g., code fixing, code explanation, code editing, code translation,\\netc., across most major programming languages (Python, JavaScript, Java, Go, C++, Rust,\\netc.). Our evaluation protocol includes both automated metrics and human evaluations.\\nFor automated metrics, we use standard evaluation scripts that measure code synthesis,\\nfixing, and explanation. For human evaluations, we conduct a series of studies where\\nparticipants are asked to compare the output of Granite Code models with other open-source\\ncode models on various coding tasks. We also perform ablation studies to understand the\\nimpact of different architectural choices on model performance.\\n5.2 Results\\nOur findings reveal that among open-source models, the Granite Code models overall show\\nvery strong performance across all model sizes and benchmarks (often outperforming other\\nopen-source code models that are twice large compared to Granite). As an illustration,\\nfigure 1 (top) shows a comparison of Granite-8B-Code-Base with other open-source base code\\nLLMs, including recent high-performing general purpose base LLMs like Mistral (Jiang et al.,\\n2023b) and LLama-3 (AI@Meta, 2024) on HumanEvalPack (Muennighoff et al., 2023). While\\nCodeGemma and StarCoder2 perform reasonably well in generating code, they perform\\nsignificantly worse on the code fixing and explanation variants of HumanEvalPack. On av-\\nerage, Granite-8B-Code-Base achieves 73% accuracy on code fixing, compared to 60% for\\nCodeGemma and 55% for StarCoder2. Similarly, on code explanation, Granite-8B-\\nCode-Base achieves 70% accuracy, outperforming CodeGemma (58%) and StarCoder2\\n(50%). These results demonstrate that Granite Code models are not only capable of\\ngenerating high-quality code but also excelling in code-related tasks that require reasoning\\nand understanding, such as code fixing and explanation.\\n\\n6.3 Conclusion\\nIn conclusion, we present Granite Code models, a series of highly capable code LLMs designed\\nto support enterprise software development across a wide range of coding tasks. Our\\nresults show that Granite Code models achieve state-of-the-art performance across a variety\\nof'},\n",
                            "    'reference_documents': [{'page_content': 'code, while others focus primarily on coding-related tasks (e.g. StarCoder (Li et al., 2023a;\\nLozhkov et al., 2024), CodeGen (Nijkamp et al., 2023), CodeLlama (Rozi `ere et al., 2023), and\\nCodeGemma (CodeGemma Team et al., 2024)).\\nHowever, there remain important gaps in the current field of LLMs for code, especially in\\nthe context of enterprise software development. First, while very large, generalist LLMs can\\nachieve excellent coding performance, their size makes them expensive to deploy. Smaller\\ncode-focused models (Li et al., 2023a; Lozhkov et al., 2024; Nijkamp et al., 2023; Rozi `ere et al.,\\n2023; CodeGemma Team et al., 2024) can achieve excellent code generation performance in\\na smaller and more flexible package, but performance in coding tasks beyond generation\\n(e.g. fixing and explanation) can lag behind code generation performance.\\nIn many enterprise contexts, code LLM adoption can be further complicated by factors\\nbeyond the performance of the models. For instance, even open models are sometimes\\nplagued by a lack of transparency about the data sources and data processing methods\\nthat went into model, which can erode trust in models in mission critical and regulated\\ncontexts. Furthermore, license terms in today’s open LLMs can encumber and complicate\\nan enterprise’s ability to use a model.\\nHere, we present Granite Code models, a series of highly capable code LLMs, designed to\\nsupport enterprise software development across a wide range of coding tasks. Granite Code\\nmodels has two main variants that we release in four different sizes (3B, 8B, 20B, and 34B):\\n2\\nIBM Granite Code Models\\n•Granite Code Base: base foundation models for code-related tasks;\\n•Granite Code Instruct: instruction following models finetuned using a combination\\nof Git commits paired with human instructions and open-source synthetically\\ngenerated code instruction datasets.\\nThe base models in the series have been trained from scratch with a two-phase training\\nstrategy. In phase 1, our model is trained on 3 to 4 trillion tokens sourced from 116 pro-\\ngramming languages, ensuring a comprehensive understanding of programming languages\\nand syntax. In phase 2, our model is further trained on 500 billion tokens with a carefully\\ndesigned mixture of high-quality data from code and natural language domains to improve\\nthe model’s ability to reason. We use the unsupervised language modeling objective to\\ntrain the base models in both the phases of training. The instruct models are derived by\\nfurther finetuning the above trained base models on a combination of a filtered variant of\\nCommitPack (Muennighoff et al., 2023), natural language instruction following datasets\\n(OASST (K ¨opf et al., 2023), HelpSteer (Wang et al., 2023)) and open-source math datasets\\n(MathInstruct (Yue et al., 2023) and MetaMathQA (Yu et al., 2023)), including synthetically\\ngenerated code datasets for improving instruction following and reasoning capabilities.\\nWe conduct extensive evaluations of our code LLMs on a comprehensive set of benchmarks,\\nincluding HumanEvalPack (Muennighoff et al., 2023), MBPP(+) (Austin et al., 2021; Liu\\net al., 2023a), RepoBench (Liu et al., 2023b), ReCode (Wang et al., 2022), and more. This set of\\nbenchmarks encompasses many different kinds of coding tasks beyond just code synthesis\\nin Python, e.g., code fixing, code explanation, code editing, code translation, etc., across\\nmost major programming languages (Python, JavaScript, Java, Go, C++, Rust, etc.).\\nOur findings reveal that among open-source models, the Granite Code models overall show\\nvery strong performance across all model sizes and benchmarks (often outperforming other\\nopen-source code models that are twice large compared to Granite). As an illustration, fig-\\nure 1 (top) shows a comparison of Granite-8B-Code-Base with other open-source base code\\nLLMs, including recent high-performing general purpose base LLMs like Mistral (Jiang et al.,\\n2023b) and LLama-3 (AI@Meta, 2024) on HumanEvalPack (Muennighoff et al., 2023). While\\nCodeGemma and StarCoder2 perform reasonably well in generating code, they perform\\nsignificantly worse on the code fixing and explanation variants of HumanEvalPack. On av-',\n",
                            "      'metadata': {'sequence_number': [6, 7, 8, 9, 10],\n",
                            "       'document_id': 'granite_code_models.pdf'}},\n",
                            "     {'page_content': 'activation function (Ramachandran et al., 2017) with GLU (Shazeer, 2020) for the MLP , also\\ncommonly referred to as swiglu. For normalization, we use RMSNorm (Zhang & Sennrich,\\n2019) since it’s computationally more efficient than LayerNorm (Ba et al., 2016). The 3B\\nmodel is trained with a context length of 2048 tokens.\\n8B: The 8B model has a similar architecture as the 3B model with the exception of using\\nGrouped-Query Attention (GQA) (Ainslie et al., 2023). Using GQA offers a better tradeoff\\nbetween model performance and inference efficiency at this scale. We train the 8B model\\nwith a context length of 4096 tokens.\\n20B: The 20B code model is trained with learned absolute position embeddings. We use\\nMulti-Query Attention (Shazeer, 2019) during training for efficient downstream inference.\\nFor the MLP block, we use the GELU activation function (Hendrycks & Gimpel, 2023). For\\nnormalizing the activations, we use LayerNorm (Ba et al., 2016). This model is trained with\\na context length of 8192 tokens.\\n34B: To train the 34B model, we follow the approach by Kim et al. for depth upscaling of\\nthe 20B model. Specifically, we first duplicate the 20B code model with 52 layers and then\\n5https://www.clamav.net/\\n5\\nIBM Granite Code Models\\nFigure 2: An overview of depth upscaling (Kim et al., 2024) for efficient training of Granite-\\n34B-Code. We utilize the 20B model after 1.6T tokens to start training of 34B model with the\\nsame code pretraining data without any changes to the training and inference framework.\\nremove final 8 layers from the original model and initial 8 layers from its duplicate to form\\ntwo models. Finally, we concatenate both models to form Granite-34B-Code model with\\n88 layers (see Figure 2 for an illustration). After the depth upscaling, we observe that the\\ndrop in performance compared to 20B model is pretty small contrary to what is observed by\\nKim et al.. This performance is recovered pretty quickly after we continue pretraining of the\\nupscaled 34B model. Similar, to 20B, we use a 8192 token context during pretraining.\\n4 Pretraining\\nIn this section, we provide details on two phase training (Sec. 4.1), training objectives\\n(Sec. 4.2), optimization (Sec. 4.3) and infrastructure (Sec. 4.4) used in pretraining the models.\\n4.1 Two Phase Training\\nGranite Code models are trained on 3.5T to 4.5T tokens of code data and natural language\\ndatasets related to code. Data is tokenized via byte pair encoding (BPE, (Sennrich et al.,\\n2015)), employing the same tokenizer as StarCoder (Li et al., 2023a). Following (Shen et al.,\\n2024; Hu et al., 2024), we utilize high-quality data with two phases of training as follows.\\n•Phase 1 (code only training) : During phase 1, both 3B and 8B models are trained for\\n4 trillion tokens of code data comprising 116 languages. The 20B parameter model\\nis trained on 3 trillion tokens of code. The 34B model is trained on 1.4T tokens after\\nthe depth upscaling which is done on the 1.6T checkpoint of 20B model.\\n•Phase 2 (code + language training) : In phase 2, we include additional high-quality\\npublicly available data from various domains, including technical, mathematics,\\nand web documents, to further improve the model’s performance in reasoning and\\nproblem solving skills, which are essential for code generation. We train all our\\nmodels for 500B tokens (80% code and 20% language data) in phase 2 training.\\n4.2 Training Objective\\nFor training of all our models, we use the causal language modeling objective and Fill-In-\\nthe-Middle (FIM) (Bavarian et al., 2022) objective. The FIM objective is tasked to predict\\ninserted tokens with the given context and subsequent text. We train our models to work\\nwith both PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle) modes, with relevant\\nformatting control tokens, same as StarCoder (Li et al., 2023a).\\nThe overall loss is computed as a weighted combination of the 2 objectives:\\nL=αLCLM + (1−α)LFIM (1)\\nWe emperically set α=0.5 during training and find that this works well in practice leading\\nto SOTA performance on both code completion and code infilling tasks. It should be\\n6\\nIBM Granite Code Models',\n",
                            "      'metadata': {'sequence_number': [21, 22, 23, 24, 25],\n",
                            "       'document_id': 'granite_code_models.pdf'}},\n",
                            "     {'page_content': 'datasets related to code. Data is tokenized via byte pair encoding (BPE, (Sennrich et al.,\\n2015)), employing the same tokenizer as StarCoder (Li et al., 2023a). Following (Shen et al.,\\n2024; Hu et al., 2024), we utilize high-quality data with two phases of training as follows.\\n•Phase 1 (code only training) : During phase 1, both 3B and 8B models are trained for\\n4 trillion tokens of code data comprising 116 languages. The 20B parameter model\\nis trained on 3 trillion tokens of code. The 34B model is trained on 1.4T tokens after\\nthe depth upscaling which is done on the 1.6T checkpoint of 20B model.\\n•Phase 2 (code + language training) : In phase 2, we include additional high-quality\\npublicly available data from various domains, including technical, mathematics,\\nand web documents, to further improve the model’s performance in reasoning and\\nproblem solving skills, which are essential for code generation. We train all our\\nmodels for 500B tokens (80% code and 20% language data) in phase 2 training.\\n4.2 Training Objective\\nFor training of all our models, we use the causal language modeling objective and Fill-In-\\nthe-Middle (FIM) (Bavarian et al., 2022) objective. The FIM objective is tasked to predict\\ninserted tokens with the given context and subsequent text. We train our models to work\\nwith both PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle) modes, with relevant\\nformatting control tokens, same as StarCoder (Li et al., 2023a).\\nThe overall loss is computed as a weighted combination of the 2 objectives:\\nL=αLCLM + (1−α)LFIM (1)\\nWe emperically set α=0.5 during training and find that this works well in practice leading\\nto SOTA performance on both code completion and code infilling tasks. It should be\\n6\\nIBM Granite Code Models\\nnoted that the FIM objective is only used during pretraining, however we drop it during\\ninstruction finetuning i.e we set α=1.\\n4.3 Optimization\\nWe use AdamW optimizer (Kingma & Ba, 2017) with β1=0.9,β2=0.95 and weight decay\\nof 0.1 for training all our Granite code models. For the phase-1 pretraining, the learning\\nrate follows a cosine schedule starting from 3 ×10−4which decays to 3 ×10−5with an\\ninitial linear warmup step of 2k iterations. For phase-2 pretraining, we start from 3 ×10−4\\n(1.5×10−4for 20B and 34B models) and adopt an exponential decay schedule to anneal it\\nto 10% of the initial learning rate. We use a batch size of 4M-5M tokens depending on the\\nmodel size during both phases of pretraining.\\nTo accelerate training, we use FlashAttention 2 (Dao et al., 2022; Dao, 2023), the persistent\\nlayernorm kernel, Fused RMSNorm kernel (depending on the model) and the Fused Adam\\nkernel available in NVIDIA’s Apex library. We use a custom fork of NVIDIA’s Megatron-\\nLM (Shoeybi et al., 2019; Narayanan et al., 2021) for distributed training of all our models.\\nWe train with a mix of 3D parallelism: tensor parallel, pipeline parallel and data parallel.\\nWe also use sequence parallelism (Korthikanti et al., 2023) for reducing the activation\\nmemory consumption of large context length during training. We use Megatron’s distributed\\noptimizer with mixed precision training (Micikevicius et al., 2018) in BF16 (Kalamkar et al.,\\n2019) with gradient all-reduce and gradient accumulation in FP32 for training stability.\\n4.4 Infrastructure\\nWe train the Granite Code models using IBM’s two supercomputing clusters, namely Vela\\nand Blue Vela, outfitted with NVIDIA A100 and H100 GPUs, respectively. In the Vela\\nA100 GPU cluster, each node has 2 ×Intel Xeon Scalable Processors with 8 ×80GB A100\\nGPUs connected to each other by NVLink and NVSwitch. The Vela cluster adopts RoCE\\n(RDMA over Converged Ethernet) and GDR (GPU-direct RDMA) for high-performance\\nnetworking. Similarly, each node in Blue Vela cluster consists of dual 48-core Intel processors\\nwith 8 ×80GB H100 GPUs. Blue Vela employs 3.2Tbps InfiniBand interconnect to facilitate\\nseamless communication between nodes, known for their high throughput and low latency.',\n",
                            "      'metadata': {'sequence_number': [24, 25, 26, 27, 28],\n",
                            "       'document_id': 'granite_code_models.pdf'}}]}]}}"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "question = \"What training objectives are used for the granite models?\"\n",
                "\n",
                "context = RuntimeContext(\n",
                "    api_client=client,\n",
                "    request_payload_json={\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
                ")\n",
                "\n",
                "\n",
                "resp = inference_service_function(context)\n",
                "resp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " The\n",
                        "clusters are equipped with 100Gbps and 200Gbps HDR InfiniBand links, respectively.\n",
                        "We utilize NVIDIA’s Megatron-LM (Shoeybi et al., 2019; Narayanan et al., 2021) for\n",
                        "distributed training, which is optimized for large language models. We use the same Megatron\n",
                        "LM framework for all our models, ensuring consistency in training infrastructure.\n",
                        "4.5 Model Architecture\n",
                        "The architecture of the Granite Code models is based on the original transformer architecture\n",
                        "(Douglas & Smith, 2019) with modifications for code modeling. The base model has 16\n",
                        "layers, 8 attention heads, and a token embedding dimension of 512. For the 3B model, we\n",
                        "use the standard transformer architecture with a multi-head attention mechanism. The 8B model\n",
                        "incorporates Grouped-Query Attention (GQA) (Ainslie et al., 2023) to improve inference\n",
                        "efficiency. The 20B model uses learned absolute position embeddings and Multi-Query\n",
                        "Attention (Shazeer, 2019). The 34B model is built upon the 20B model with depth\n",
                        "upscaling (Kim et al., 2024) to double the model depth, resulting in 88 layers.\n",
                        "The models are trained with different context lengths depending on their size: 2048, 4096,\n",
                        "8192, and 8192 tokens respectively for 3B, 8B, 20B, and 34B models.\n",
                        "For the MLP block, we use GELU activation function (Hendrycks & Gimpel, 2023) for the 20B\n",
                        "and 34B models, while using GLU (Shazeer, 2020) for the 3B and 8B models. For\n",
                        "normalization, we use LayerNorm (Ba et al., 2016) for all models except the 8B model,\n",
                        "which uses RMSNorm (Zhang & Sennrich, 2019) for computational efficiency.\n",
                        "5.1 Evaluation Protocol\n",
                        "We evaluate the Granite Code models on a comprehensive set of benchmarks including\n",
                        "HumanEvalPack (Muennighoff et al., 2023), MBPP(+) (Austin et al., 2021; Liu et al.,\n",
                        "2023a), RepoBench (Liu et al., 2023b), ReCode (Wang et al., 2022), and more. This\n",
                        "set of benchmarks encompasses many different kinds of coding tasks beyond just code\n",
                        "synthesis in Python, e.g., code fixing, code explanation, code editing, code translation,\n",
                        "etc., across most major programming languages (Python, JavaScript, Java, Go, C++, Rust,\n",
                        "etc.). Our evaluation protocol includes both automated metrics and human evaluations.\n",
                        "For automated metrics, we use standard evaluation scripts that measure code synthesis,\n",
                        "fixing, and explanation. For human evaluations, we conduct a series of studies where\n",
                        "participants are asked to compare the output of Granite Code models with other open-source\n",
                        "code models on various coding tasks. We also perform ablation studies to understand the\n",
                        "impact of different architectural choices on model performance.\n",
                        "5.2 Results\n",
                        "Our findings reveal that among open-source models, the Granite Code models overall show\n",
                        "very strong performance across all model sizes and benchmarks (often outperforming other\n",
                        "open-source code models that are twice large compared to Granite). As an illustration,\n",
                        "figure 1 (top) shows a comparison of Granite-8B-Code-Base with other open-source base code\n",
                        "LLMs, including recent high-performing general purpose base LLMs like Mistral (Jiang et al.,\n",
                        "2023b) and LLama-3 (AI@Meta, 2024) on HumanEvalPack (Muennighoff et al., 2023). While\n",
                        "CodeGemma and StarCoder2 perform reasonably well in generating code, they perform\n",
                        "significantly worse on the code fixing and explanation variants of HumanEvalPack. On av-\n",
                        "erage, Granite-8B-Code-Base achieves 73% accuracy on code fixing, compared to 60% for\n",
                        "CodeGemma and 55% for StarCoder2. Similarly, on code explanation, Granite-8B-\n",
                        "Code-Base achieves 70% accuracy, outperforming CodeGemma (58%) and StarCoder2\n",
                        "(50%). These results demonstrate that Granite Code models are not only capable of\n",
                        "generating high-quality code but also excelling in code-related tasks that require reasoning\n",
                        "and understanding, such as code fixing and explanation.\n",
                        "\n",
                        "6.3 Conclusion\n",
                        "In conclusion, we present Granite Code models, a series of highly capable code LLMs designed\n",
                        "to support enterprise software development across a wide range of coding tasks. Our\n",
                        "results show that Granite Code models achieve state-of-the-art performance across a variety\n",
                        "of\n"
                    ]
                }
            ],
            "source": [
                "print(inference_service_function(context)[\"body\"][\"choices\"][0][\"message\"][\"content\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                " You successfully completed this notebook!\n",
                " \n",
                " You learned how to use AutoAI RAG with your own foundation model.\n",
                " \n",
                "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Author:\n",
                " **Michał Steczko**, Software Engineer at watsonx.ai."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Copyright © 2025 IBM. This notebook and its source code are released under the terms of the MIT License."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "autoai_rag",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
